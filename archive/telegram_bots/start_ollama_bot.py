#!/usr/bin/env python3
"""
Quick launcher for Ollama Chat Bot
"""

import os
import sys
import subprocess
from pathlib import Path

def check_dependencies():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏"""
    print("üîç Checking dependencies...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º Python –ø–∞–∫–µ—Ç—ã
    required_packages = ["httpx", "asyncio"]
    for package in required_packages:
        try:
            __import__(package)
            print(f"‚úÖ {package}")
        except ImportError:
            print(f"‚ùå {package} - install with: pip install {package}")
            return False
    
    return True

def check_services():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–∏—Å–æ–≤"""
    print("\nüîç Checking services...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º Ollama
    try:
        import httpx
        import asyncio
        
        async def check_ollama():
            try:
                async with httpx.AsyncClient(timeout=5.0) as client:
                    response = await client.get("http://localhost:11434/api/tags")
                    if response.status_code == 200:
                        models = response.json().get("models", [])
                        print(f"‚úÖ Ollama (found {len(models)} models)")
                        for model in models[:3]:  # Show first 3
                            print(f"   üì¶ {model['name']}")
                        return True
                    else:
                        print("‚ùå Ollama - server responded with error")
                        return False
            except Exception as e:
                print(f"‚ùå Ollama - not running: {e}")
                return False
        
        ollama_ok = asyncio.run(check_ollama())
    except Exception as e:
        print(f"‚ùå Ollama - check failed: {e}")
        ollama_ok = False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º API –∫–ª—é—á–∏
    grok_key = os.getenv("GROK_API_KEY")
    anthropic_key = os.getenv("ANTHROPIC_API_KEY")
    
    print(f"{'‚úÖ' if grok_key else '‚ö†Ô∏è'} Grok API key {'set' if grok_key else 'not set'}")
    print(f"{'‚úÖ' if anthropic_key else '‚ö†Ô∏è'} Anthropic API key {'set' if anthropic_key else 'not set'}")
    
    return ollama_ok

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ LLMStruct Ollama Chat Bot Launcher")
    print("=" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–∫–µ–Ω –±–æ—Ç–∞
    bot_token = os.getenv("TELEGRAM_BOT_TOKEN")
    if not bot_token:
        print("‚ùå TELEGRAM_BOT_TOKEN not set!")
        print("üîß Set it with: export TELEGRAM_BOT_TOKEN='your_token'")
        print("üì± Get token from @BotFather on Telegram")
        return 1
    
    print(f"‚úÖ Telegram bot token: ...{bot_token[-10:]}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    if not check_dependencies():
        print("\n‚ùå Dependencies missing. Install with:")
        print("pip install httpx")
        return 1
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–µ—Ä–≤–∏—Å—ã
    services_ok = check_services()
    
    if not services_ok:
        print("\n‚ö†Ô∏è Ollama not available, will use fallback models only")
        print("üí° To install Ollama: https://ollama.ai/")
        print("üí° To start Ollama: ollama serve")
    
    # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    print("\nüìÅ Creating directories...")
    Path("logs").mkdir(exist_ok=True)
    Path("data/ollama_chat").mkdir(parents=True, exist_ok=True)
    print("‚úÖ Directories ready")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞
    print("\nüöÄ Starting Ollama Chat Bot...")
    print("üõë Press Ctrl+C to stop")
    print("=" * 50)
    
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º subprocess –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞
        result = subprocess.run([
            sys.executable, "ollama_chat_bot.py"
        ], cwd=Path.cwd())
        return result.returncode
    except KeyboardInterrupt:
        print("\nüõë Bot stopped by user")
        return 0
    except Exception as e:
        print(f"\n‚ùå Error starting bot: {e}")
        return 1

if __name__ == "__main__":
    exit(main()) 