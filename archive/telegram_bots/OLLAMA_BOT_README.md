# ü¶ô LLMStruct Ollama Chat Bot

–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π Telegram –±–æ—Ç –¥–ª—è –æ–±—â–µ–Ω–∏—è —Å –û–ª–ª–∞–º–æ–π + fallback –Ω–∞ –ì—Ä–æ–∫/–ê–Ω—Ç—Ä–æ–ø–∏–∫, —Å –ø–∞–º—è—Ç—å—é, –∫–µ—à–µ–º –∏ –¥–æ—Å—Ç—É–ø–æ–º –∫ —Ñ–∞–π–ª–∞–º –ø—Ä–æ–µ–∫—Ç–∞.

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
pip install httpx

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
export TELEGRAM_BOT_TOKEN="7576808324:AAG_lyXEt-AEfGCSJ5VoOx1oUEniyjcmHBI"
export GROK_API_KEY="your_grok_key"          # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
export ANTHROPIC_API_KEY="your_claude_key"   # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
```

### 2. –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞

```bash
# –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—É—Å–∫
python start_ollama_bot.py

# –ò–ª–∏ –Ω–∞–ø—Ä—è–º—É—é
python ollama_chat_bot.py
```

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

### –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã:

1. **`OllamaChatBot`** - –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –±–æ—Ç–∞
2. **`ModelManager`** - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏
3. **`FileManager`** - –î–æ—Å—Ç—É–ø –∫ —Ñ–∞–π–ª–∞–º –ø—Ä–æ–µ–∫—Ç–∞ —á–µ—Ä–µ–∑ CLI
4. **`MemoryManager`** - –ü–∞–º—è—Ç—å –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤
5. **`CursorReporter`** - –û—Ç–ø—Ä–∞–≤–∫–∞ –æ—Ç—á–µ—Ç–æ–≤ –∏–∑ Cursor

### Fallback —Å–∏—Å—Ç–µ–º–∞:

```
ü¶ô Ollama (primary) ‚Üí üöÄ Grok ‚Üí ü§ñ Claude
```

## üì± –ö–æ–º–∞–Ω–¥—ã –±–æ—Ç–∞

### –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:
- `/start` - –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ –∏ —Å–ø—Ä–∞–≤–∫–∞
- `/help` - –ü–æ–¥—Ä–æ–±–Ω–∞—è —Å–ø—Ä–∞–≤–∫–∞
- `/memory` - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ—Å—Å–∏–∏
- `/models` - –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏

### –î–æ—Å—Ç—É–ø –∫ —Ñ–∞–π–ª–∞–º:
- `/file src/llmstruct/cli.py` - –ß–∏—Ç–∞—Ç—å —Ñ–∞–π–ª
- `/ls src/llmstruct` - –°–ø–∏—Å–æ–∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏  
- `/ls` - –ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è

### CLI –∫–æ–º–∞–Ω–¥—ã:
- `/cli status` - –°—Ç–∞—Ç—É—Å –ø—Ä–æ–µ–∫—Ç–∞
- `/cli metrics` - –ú–µ—Ç—Ä–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã
- `/cli ai_status` - –°—Ç–∞—Ç—É—Å AI —Å–∏—Å—Ç–µ–º—ã
- `/cli workflow` - –°—Ç–∞—Ç—É—Å workflow
- `/cli search context` - –ü–æ–∏—Å–∫ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π

### –û–±—ã—á–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è:
–õ—é–±–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –±–µ–∑ –∫–æ–º–∞–Ω–¥—ã –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –≤ AI –¥–ª—è –æ–±—â–µ–Ω–∏—è.

## üß† –°–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏

### –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- ‚úÖ –ö–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ (–¥–æ 10 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π)
- ‚úÖ –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 5 —Å–æ–æ–±—â–µ–Ω–∏–π
- ‚úÖ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Å–µ—Å—Å–∏–∏ (50 —Å–æ–æ–±—â–µ–Ω–∏–π ‚Üí 40)
- ‚úÖ –ü–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON
- ‚úÖ –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π

### –§–∞–π–ª—ã —Ö—Ä–∞–Ω–µ–Ω–∏—è:
```
data/ollama_chat/
‚îú‚îÄ‚îÄ sessions.json           # –°–µ—Å—Å–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
‚îî‚îÄ‚îÄ global_context.json     # –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
```

## ü§ñ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π

### Ollama (–æ—Å–Ω–æ–≤–Ω–∞—è):
```python
# URL: http://localhost:11434
# –ú–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: llama3.2:3b
# Timeout: 60 —Å–µ–∫—É–Ω–¥
```

### Grok (fallback):
```python
# API: https://api.x.ai/v1/chat/completions
# –ú–æ–¥–µ–ª—å: grok-beta
# –¢—Ä–µ–±—É–µ—Ç: GROK_API_KEY
```

### Claude (fallback):
```python
# API: https://api.anthropic.com/v1/messages  
# –ú–æ–¥–µ–ª—å: claude-3-haiku-20240307
# –¢—Ä–µ–±—É–µ—Ç: ANTHROPIC_API_KEY
```

## üìÅ –î–æ—Å—Ç—É–ø –∫ —Ñ–∞–π–ª–∞–º

### –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å:
- ‚úÖ –¢–æ–ª—å–∫–æ —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –≤–Ω—É—Ç—Ä–∏ –ø—Ä–æ–µ–∫—Ç–∞
- ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Ç–µ–π –Ω–∞ –≤—ã—Ö–æ–¥ –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –ø—Ä–æ–µ–∫—Ç–∞
- ‚úÖ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –≤—ã–≤–æ–¥–∞ (100 —Å—Ç—Ä–æ–∫ / 50 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)

### CLI –∫–æ–º–∞–Ω–¥—ã:
```python
safe_commands = {
    "status": ["python", "-m", "llmstruct.cli", "parse", "--help"],
    "query": ["python", "-m", "llmstruct.cli", "query", "--help"],
    "context": ["python", "-m", "llmstruct.cli", "context", "--help"],
    "metrics": ["python", "-m", "llmstruct.cli", "metrics", "status"],
    "ai_status": ["python", "-c", "from auto_init_ai_system import get_ai_status; print(get_ai_status())"],
    "workflow": ["python", "-c", "from auto_init_ai_system import get_workflow_status; print(get_workflow_status())"],
    "search": ["python", "-c", "from auto_init_ai_system import search_ai_capabilities; print(search_ai_capabilities('{}'))"]
}
```

## üìã –û—Ç–ø—Ä–∞–≤–∫–∞ –æ—Ç—á–µ—Ç–æ–≤ –∏–∑ Cursor

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CursorReporter:

```python
from cursor_reporter import report_started, report_completed, report_failed

# –ü—Ä–æ—Å—Ç—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
report_started("Implementing new feature", "Started work on user authentication")
report_completed("Bug fix", "Fixed memory leak in background task")
report_failed("API integration", "Connection timeout errors")

# –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏
import asyncio
from cursor_reporter import report_task_progress

async def my_task():
    await report_task_progress("Data processing", "Processed 50% of records")
```

### –§–æ—Ä–º–∞—Ç—ã –æ—Ç—á–µ—Ç–æ–≤:

```
üöÄ Cursor Task Report

üîµ Priority: Normal
üìã Task: Implementing new feature  
üìä Status: Started
üïê Time: 2025-05-30 18:00:00

Started work on user authentication

*Sent from Cursor IDE*
```

## üîß –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º—ã:
```bash
python start_ollama_bot.py
```

–í—ã–≤–æ–¥–∏—Ç:
- ‚úÖ/‚ùå –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ Python
- ‚úÖ/‚ùå Ollama –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å + –º–æ–¥–µ–ª–∏
- ‚úÖ/‚ö†Ô∏è API –∫–ª—é—á–∏ (Grok, Claude)
- ‚úÖ –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π

### –õ–æ–≥–∏:
```
logs/ollama_chat_bot.log  # –û—Å–Ω–æ–≤–Ω—ã–µ –ª–æ–≥–∏ –±–æ—Ç–∞
```

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ª–æ–≥–æ–≤:
```
2025-05-30 18:00:00 - INFO - ü§ñ OllamaChatBot initialized
2025-05-30 18:00:01 - INFO - üìö Loaded 3 chat sessions  
2025-05-30 18:00:02 - INFO - üöÄ LLMStruct Ollama Chat Bot starting...
2025-05-30 18:00:05 - INFO - üí¨ Message from –ú–∏—Ö–∞–∏–ª: –ü—Ä–∏–≤–µ—Ç...
2025-05-30 18:00:06 - INFO - ü¶ô Trying Ollama for user –ú–∏—Ö–∞–∏–ª
2025-05-30 18:00:08 - INFO - ‚úÖ Report sent: Task completed - completed
```

## üõ†Ô∏è –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama

### Linux/macOS:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve &
ollama pull llama3.2:3b
```

### Windows:
1. –°–∫–∞—á–∞—Ç—å —Å https://ollama.ai/
2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å 
3. `ollama serve`
4. `ollama pull llama3.2:3b`

### –ü—Ä–æ–≤–µ—Ä–∫–∞:
```bash
curl http://localhost:11434/api/tags
```

## üîÄ –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### 1. –û–±—ã—á–Ω—ã–π —á–∞—Ç:
```
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: –ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?
–ë–æ—Ç: ü¶ô Ollama: –ü—Ä–∏–≤–µ—Ç! –£ –º–µ–Ω—è –≤—Å—ë –æ—Ç–ª–∏—á–Ω–æ, —Å–ø–∞—Å–∏–±–æ! –ì–æ—Ç–æ–≤ –ø–æ–º–æ—á—å...
```

### 2. –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤:
```
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: /file src/llmstruct/cli.py
–ë–æ—Ç: üìÑ src/llmstruct/cli.py
```python
#!/usr/bin/env python3
...
```

### 3. CLI –∫–æ–º–∞–Ω–¥—ã:
```
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: /cli ai_status
–ë–æ—Ç: ‚úÖ ai_status
```
üß† AI System Status: Active
üìä Modules: 272 analyzed
...
```

### 4. Fallback –≤ –¥–µ–π—Å—Ç–≤–∏–∏:
```
–õ–æ–≥–∏:
INFO - ü¶ô Trying Ollama for user –ú–∏—Ö–∞–∏–ª
ERROR - Ollama request failed: Connection refused  
INFO - üöÄ Ollama failed, trying Grok
INFO - üöÄ Grok: Success!

–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: –ü—Ä–∏–≤–µ—Ç!
–ë–æ—Ç: üöÄ Grok: –ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞? –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?
```

## üìä –ú–µ—Ç—Ä–∏–∫–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã–µ –¥–∞–Ω–Ω—ã–µ:
- üì± –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Å–µ—Å—Å–∏–∏
- ü§ñ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
- ‚è∞ –í—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏  
- üìà –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ—Å—Å–∏–π
- üîÑ –ß–∞—Å—Ç–æ—Ç–∞ –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–π

### –ö–æ–º–∞–Ω–¥–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏:
```
/memory

üß† Memory Statistics

Session: ollama_-4938821563_123456789
‚Ä¢ Messages: 15
‚Ä¢ Started: 2025-05-30 17:30:00
‚Ä¢ Last activity: 2025-05-30 18:15:00  
‚Ä¢ Preferred model: ollama

Global:
‚Ä¢ Total sessions: 3
```

## ‚ö†Ô∏è Troubleshooting

### –ü—Ä–æ–±–ª–µ–º–∞: Ollama –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å
curl http://localhost:11434/api/tags

# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å
pkill ollama
ollama serve &
```

### –ü—Ä–æ–±–ª–µ–º–∞: –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã  
```
‚ùå All AI providers are currently unavailable
Please try again later.
```

**–†–µ—à–µ–Ω–∏–µ:**
1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å Ollama: `ollama serve`
2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å API –∫–ª—é—á–∏: `echo $GROK_API_KEY`
3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ

### –ü—Ä–æ–±–ª–µ–º–∞: –î–æ—Å—Ç—É–ø –∫ —Ñ–∞–π–ª–∞–º
```
‚ùå Access denied: path outside project
```

**–†–µ—à–µ–Ω–∏–µ:**
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Ç–∏ –æ—Ç –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
- –ü—Ä–∏–º–µ—Ä: `/file src/llmstruct/cli.py` –≤–º–µ—Å—Ç–æ `/file /home/user/...`

## üéØ Roadmap

### –ë–ª–∏–∂–∞–π—à–∏–µ —É–ª—É—á—à–µ–Ω–∏—è:
- [ ] –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö
- [ ] –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –∫–∞–ª–µ–Ω–¥–∞—Ä–µ–º –∑–∞–¥–∞—á
- [ ] –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–ª–∞–≥–∏–Ω–æ–≤
- [ ] –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
- [ ] –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≥–æ–ª–æ—Å–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏:
- [ ] GitHub API –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è–º–∏
- [ ] Jira/Trello –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–∞–¥–∞—á–∞–º–∏  
- [ ] Notion API –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
- [ ] Docker –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∫–æ–º–∞–Ω–¥

## üìÑ –õ–∏—Ü–µ–Ω–∑–∏—è

MIT License - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–∞–∫ —Ö–æ—Ç–∏—Ç–µ!

---

**–°–æ–∑–¥–∞–Ω–æ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ LLMStruct** üöÄ 