#!/usr/bin/env python3
"""
Enhanced LLMStruct Telegram Bot
- Smart reply detection (reply_to_message support)
- LLM chain: Ollama â†’ Grok â†’ Anthropic
- Cursor command forwarding to Cursor AI
- Concise responses by default
"""

import os
import sys
import json
import time
import asyncio
import logging
import signal
from datetime import datetime
from typing import Optional, Dict, Any

import httpx
from telegram import Update, Bot
from telegram.ext import Application, ContextTypes, CommandHandler, MessageHandler, filters

# Add project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from src.llmstruct.metrics_tracker import get_metrics_tracker, track_workflow_event, track_task_start, track_task_complete, track_token_usage
    METRICS_AVAILABLE = True
except ImportError:
    print("âš ï¸ Metrics system not available")
    METRICS_AVAILABLE = False

class EnhancedLLMStructChatBot:
    """Enhanced Telegram bot with LLM chain and smart reply detection"""
    
    def __init__(self, token: str):
        self.token = token
        self.bot = Bot(token=token)
        self.app = None
        
        # LLM chain configuration
        self.ollama_base = "http://192.168.88.50:11434"
        self.grok_api_key = os.getenv("GROK_API_KEY", "")
        self.anthropic_key = os.getenv("ANTHROPIC_API_KEY", "")
        
        # Cursor integration
        self.cursor_chat_id = None  # Will be detected from first cursor message
        self.user_chat_id = None    # Primary user chat ID
        
        # Logging setup
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
        # Message logging
        os.makedirs("logs/telegram", exist_ok=True)
        self.message_log_file = "logs/telegram/user_messages.log"
        self.cursor_log_file = "logs/telegram/cursor_commands.log"
        
        if METRICS_AVAILABLE:
            self.metrics = get_metrics_tracker()
            track_workflow_event("chat_bot_startup")
            self.logger.info(f"ğŸ“Š Metrics session: {self.metrics.session_data.get('session_id', 'unknown')}")
        else:
            self.metrics = None

    def log_user_message(self, user_info: dict, message: str, message_type: str = "text", chat_id: int = None, reply_to_message: dict = None):
        """Log user message with reply context"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        with open(self.message_log_file, "a", encoding="utf-8") as f:
            f.write(f"\n=== {timestamp} ===\n")
            f.write(f"ğŸ‘¤ USER: {user_info.get('first_name', 'Unknown')} (@{user_info.get('username', 'no_username')})\n")
            f.write(f"ğŸ“± TYPE: {message_type}\n")
            if chat_id:
                f.write(f"ğŸ†” CHAT_ID: {chat_id}\n")
            if reply_to_message:
                f.write(f"â†©ï¸ REPLY_TO: {reply_to_message.get('text', 'N/A')[:50]}...\n")
            f.write(f"ğŸ’¬ MESSAGE: {message}\n")
            f.write("=" * 60 + "\n")

    def log_cursor_command(self, message: str, user_info: dict, reply_context: str = ""):
        """Log cursor command for processing"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        with open(self.cursor_log_file, "a", encoding="utf-8") as f:
            f.write(f"\n=== CURSOR COMMAND {timestamp} ===\n")
            f.write(f"ğŸ‘¤ FROM: {user_info.get('first_name', 'Unknown')}\n")
            f.write(f"ğŸ¯ COMMAND: {message}\n")
            if reply_context:
                f.write(f"ğŸ“ CONTEXT: {reply_context}\n")
            f.write("=" * 60 + "\n")

    async def ollama_chat(self, message: str, model: str = "llama3.1") -> Optional[str]:
        """Try Ollama first"""
        try:
            url = f"{self.ollama_base}/api/generate"
            data = {
                "model": model,
                "prompt": f"ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ Ğ¸ Ğ¿Ğ¾ Ğ´ĞµĞ»Ñƒ (1-3 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼):\n{message}",
                "stream": False
            }
            
            async with httpx.AsyncClient(timeout=15.0) as client:
                response = await client.post(url, json=data)
                if response.status_code == 200:
                    result = response.json()
                    return result.get("response", "").strip()
        except Exception as e:
            self.logger.warning(f"Ollama failed: {e}")
        return None

    async def grok_chat(self, message: str) -> Optional[str]:
        """Fallback to Grok"""
        if not self.grok_api_key:
            return None
            
        try:
            url = "https://api.x.ai/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.grok_api_key}",
                "Content-Type": "application/json"
            }
            data = {
                "model": "grok-beta",
                "messages": [
                    {"role": "system", "content": "ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾. ĞœĞ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ 2-3 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ."},
                    {"role": "user", "content": message}
                ],
                "max_tokens": 150
            }
            
            async with httpx.AsyncClient(timeout=20.0) as client:
                response = await client.post(url, headers=headers, json=data)
                if response.status_code == 200:
                    result = response.json()
                    return result["choices"][0]["message"]["content"].strip()
        except Exception as e:
            self.logger.warning(f"Grok failed: {e}")
        return None

    async def anthropic_chat(self, message: str) -> Optional[str]:
        """Final fallback to Anthropic"""
        if not self.anthropic_key:
            return None
            
        try:
            url = "https://api.anthropic.com/v1/messages"
            headers = {
                "x-api-key": self.anthropic_key,
                "Content-Type": "application/json",
                "anthropic-version": "2023-06-01"
            }
            data = {
                "model": "claude-3-haiku-20240307",
                "max_tokens": 150,
                "system": "ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ Ğ¸ Ğ¿Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ñƒ. ĞœĞ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ 2-3 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ.",
                "messages": [
                    {"role": "user", "content": message}
                ]
            }
            
            async with httpx.AsyncClient(timeout=20.0) as client:
                response = await client.post(url, headers=headers, json=data)
                if response.status_code == 200:
                    result = response.json()
                    return result["content"][0]["text"].strip()
        except Exception as e:
            self.logger.warning(f"Anthropic failed: {e}")
        return None

    async def llm_chain_response(self, message: str) -> str:
        """LLM chain: Ollama â†’ Grok â†’ Anthropic"""
        
        # Try Ollama first
        response = await self.ollama_chat(message)
        if response:
            return f"ğŸ¦™ {response}"
        
        # Fallback to Grok
        response = await self.grok_chat(message)
        if response:
            return f"ğŸš€ {response}"
            
        # Final fallback to Anthropic
        response = await self.anthropic_chat(message)
        if response:
            return f"ğŸ§  {response}"
            
        # If all fail
        return "âŒ Ğ’ÑĞµ LLM Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ñ‹ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Ğ¿Ğ¾Ğ·Ğ¶Ğµ."

    def setup_handlers(self):
        """Setup bot handlers"""
        self.app = Application.builder().token(self.token).build()
        
        # Command handlers
        self.app.add_handler(CommandHandler("start", self.start_command))
        self.app.add_handler(CommandHandler("help", self.help_command))
        self.app.add_handler(CommandHandler("status", self.status_command))
        
        # Message handler
        self.app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, self.handle_message))
        
        return self.app

    async def start_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Start command handler"""
        user = update.effective_user
        chat_id = update.effective_chat.id
        
        # Set user chat ID for the primary user
        if not self.user_chat_id:
            self.user_chat_id = chat_id
        
        response = """ğŸ¤– *Enhanced LLMStruct Bot*

*LLM Chain:* ğŸ¦™ Ollama â†’ ğŸš€ Grok â†’ ğŸ§  Anthropic

*Commands:*
â€¢ ĞĞ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ â†’ LLM chain (ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹)
â€¢ `cursor <ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°>` â†’ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° Ğ² Cursor AI
â€¢ ĞÑ‚Ğ²ĞµÑ‚ Ğ½Ğ° ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ñ‚Ğ° â†’ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ cursor Ñ€ĞµĞ¶Ğ¸Ğ¼

*Features:*
âœ… Ğ£Ğ¼Ğ½Ğ°Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²
âœ… ĞšÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹  
âœ… Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ Cursor
âœ… ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ

Ğ“Ğ¾Ñ‚Ğ¾Ğ² Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ! ğŸš€"""

        await update.message.reply_text(response, parse_mode='Markdown')

    async def help_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Help command"""
        help_text = """ğŸ“š *Ğ¡Ğ¿Ñ€Ğ°Ğ²ĞºĞ° Ğ¿Ğ¾ Enhanced Bot*

*ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹:*
â€¢ ĞĞ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ â†’ LLM chain
â€¢ `cursor ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°` â†’ Cursor AI
â€¢ ĞÑ‚Ğ²ĞµÑ‚ Ğ½Ğ° ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ñ‚Ğ° â†’ Cursor AI

*LLM Chain Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚:*
1ï¸âƒ£ ğŸ¦™ Ollama (192.168.88.50) - Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾
2ï¸âƒ£ ğŸš€ Grok - ÑƒĞ¼Ğ½Ğ¾ Ğ¸ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾  
3ï¸âƒ£ ğŸ§  Anthropic - Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¹ fallback

*ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸:*
â€¢ ĞšÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ (1-3 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ)
â€¢ ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°
â€¢ Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ÑĞµÑ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹
â€¢ ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸

Ready! ğŸ¯"""

        await update.message.reply_text(help_text, parse_mode='Markdown')

    async def status_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Status command"""
        status_parts = ["ğŸ” *System Status*\n"]
        
        # Check Ollama
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{self.ollama_base}/api/tags")
                if response.status_code == 200:
                    models = response.json().get("models", [])
                    status_parts.append(f"ğŸ¦™ Ollama: âœ… ({len(models)} models)")
                else:
                    status_parts.append("ğŸ¦™ Ollama: âŒ")
        except:
            status_parts.append("ğŸ¦™ Ollama: âŒ")
        
        # Check other services
        status_parts.append(f"ğŸš€ Grok: {'âœ…' if self.grok_api_key else 'âŒ'}")
        status_parts.append(f"ğŸ§  Anthropic: {'âœ…' if self.anthropic_key else 'âŒ'}")
        
        if METRICS_AVAILABLE and self.metrics:
            status_parts.append(f"ğŸ“Š Metrics: âœ… (session: {self.metrics.session_data.get('session_id', 'unknown')[:8]})")
        else:
            status_parts.append("ğŸ“Š Metrics: âŒ")
            
        await update.message.reply_text("\n".join(status_parts), parse_mode='Markdown')

    async def handle_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Handle all text messages with smart routing"""
        user = update.effective_user
        message = update.message
        text = message.text.strip()
        chat_id = update.effective_chat.id
        
        # Track metrics
        if METRICS_AVAILABLE and self.metrics:
            task_id = f"chat_bot_message_{int(time.time())}"
            track_task_start(task_id, "chat_bot_conversation")
            track_workflow_event("bot_message")
        
        # Log message with reply context
        reply_to_message = None
        if message.reply_to_message:
            reply_to_message = {
                "text": message.reply_to_message.text,
                "from_bot": message.reply_to_message.from_user.is_bot
            }
        
        self.log_user_message(
            user_info=user.to_dict(),
            message=text,
            chat_id=chat_id,
            reply_to_message=reply_to_message
        )
        
        # Determine routing
        is_cursor_command = False
        cursor_message = text
        
        # Check for explicit cursor command
        if text.lower().startswith("cursor"):
            is_cursor_command = True
            cursor_message = text[6:].strip() if len(text) > 6 else "status"
            
        # Check for reply to bot message (auto-cursor mode)
        elif (reply_to_message and reply_to_message.get("from_bot", False)):
            is_cursor_command = True
            cursor_message = f"ĞÑ‚Ğ²ĞµÑ‚ Ğ½Ğ°: {reply_to_message['text'][:50]}...\nĞœĞ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚: {text}"
        
        try:
            if is_cursor_command:
                # Log cursor command
                reply_context = reply_to_message.get("text", "") if reply_to_message else ""
                self.log_cursor_command(cursor_message, user.to_dict(), reply_context)
                
                # Send acknowledgment
                response = f"ğŸ¯ *ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½Ğ° Ğ² Cursor AI*\n\n`{cursor_message[:100]}{'...' if len(cursor_message) > 100 else ''}`\n\nâ³ ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ..."
                await message.reply_text(response, parse_mode='Markdown')
                
                # Track tokens for cursor command
                if METRICS_AVAILABLE:
                    input_tokens = len(cursor_message.split())
                    output_tokens = len(response.split()) 
                    track_token_usage("telegram", "cursor", input_tokens, output_tokens)
                    
            else:
                # Regular LLM chain
                response = await self.llm_chain_response(text)
                await message.reply_text(response, parse_mode='Markdown')
                
                # Track tokens for LLM response
                if METRICS_AVAILABLE:
                    input_tokens = len(text.split())
                    output_tokens = len(response.split())
                    track_token_usage("telegram", "llm_chain", input_tokens, output_tokens)
            
            # Complete task tracking
            if METRICS_AVAILABLE and self.metrics:
                track_task_complete(task_id, "success")
                
        except Exception as e:
            self.logger.error(f"Error handling message: {e}")
            error_msg = "âŒ ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ"
            await message.reply_text(error_msg)
            
            if METRICS_AVAILABLE and self.metrics:
                track_task_complete(task_id, "failed")

    def run_sync(self, timeout_minutes: int = 10):
        """Run bot synchronously with timeout"""
        shutdown_requested = False
        
        def signal_handler(signum, frame):
            nonlocal shutdown_requested
            self.logger.info(f"ğŸ›‘ Received signal {signum}, shutting down...")
            shutdown_requested = True
        
        # Setup signal handlers
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        try:
            app = self.setup_handlers()
            
            self.logger.info("ğŸ¤– Starting Enhanced LLMStruct Chat Bot...")
            self.logger.info(f"â° Will run for {timeout_minutes} minutes max")
            if METRICS_AVAILABLE and self.metrics:
                self.logger.info(f"ğŸ“Š Metrics session: {self.metrics.session_data.get('session_id', 'unknown')}")
            
            # Start time for timeout
            start_time = time.time()
            timeout_seconds = timeout_minutes * 60
            
            # Create async runner with timeout
            async def run_with_timeout():
                nonlocal shutdown_requested
                application = app
                await application.initialize()
                await application.start()
                await application.updater.start_polling(drop_pending_updates=True)
                
                # Check for timeout or shutdown signal periodically
                while not shutdown_requested:
                    await asyncio.sleep(1)
                    
                    if time.time() - start_time > timeout_seconds:
                        self.logger.info(f"â° Timeout reached ({timeout_minutes} minutes)")
                        break
                
                await application.updater.stop()
                await application.stop()
                await application.shutdown()
            
            # Run the bot
            try:
                asyncio.run(run_with_timeout())
            except RuntimeError as e:
                if "This event loop is already running" in str(e):
                    self.logger.warning("âš ï¸ Event loop already running, using current loop")
                    loop = asyncio.get_event_loop()
                    task = loop.create_task(run_with_timeout())
                    loop.run_until_complete(task)
                else:
                    raise
            
        except KeyboardInterrupt:
            self.logger.info("ğŸ›‘ Bot stopped by user")
        except Exception as e:
            self.logger.error(f"âŒ Bot error: {e}")
            import traceback
            traceback.print_exc()
        finally:
            if METRICS_AVAILABLE:
                track_workflow_event("chat_bot_shutdown")
                self.logger.info("ğŸ“Š Metrics saved")
            self.logger.info("âœ… Bot shutdown complete")

def main():
    """Main entry point"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Enhanced LLMStruct Telegram Bot")
    parser.add_argument("--timeout", "-t", type=int, default=10, 
                       help="Bot timeout in minutes (default: 10)")
    parser.add_argument("--foreground", "-f", action="store_true",
                       help="Run in foreground mode")
    
    args = parser.parse_args()
    
    token = os.getenv("TELEGRAM_BOT_TOKEN")
    if not token:
        print("âŒ TELEGRAM_BOT_TOKEN not found in environment")
        return 1
    
    print(f"ğŸš€ Starting Enhanced Bot (timeout: {args.timeout}min)")
    bot = EnhancedLLMStructChatBot(token)
    bot.run_sync(timeout_minutes=args.timeout)
    return 0

if __name__ == "__main__":
    exit(main()) 