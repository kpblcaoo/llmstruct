{
  "consultation_context": {
    "document_type": "comprehensive_context",
    "version": "2.1",
    "sections": {
      "Strategic Decision Point: GPT-4.1 vs Grok vs Hybrid Approach": "",
      "üìä PROJECT STATUS": "**LLMStruct AI-Enhanced Development Environment**\n- **Scale:** 272 modules, 1857 functions, 183 classes\n- **Critical Problems:** \n  - Giant number of unsorted files, docs, scripts, tests and so on\n  - 8 different bot versions need consolidation\n  - Architectural analysis needed across all subsystems\n",
      "ü§ñ BUILT-IN AI SELF-ANALYSIS CAPABILITIES": "**LLMStruct has advanced AI systems that could assist analysis:**\n\n**1. AI Self-Awareness System (`src/llmstruct/ai_self_awareness.py`)**\n- SystemCapabilityDiscovery - analyzes all 272 modules automatically\n- Can search through 1857 functions and 183 classes intelligently\n- Real-time capability analysis and smart caching\n- Already indexed the entire project structure\n\n**2. struct.json - Complete Project Knowledge Base**\n- Contains parsed analysis of all 272 modules\n- Function signatures, dependencies, class hierarchies\n- Auto-generated and constantly updated\n- Can be used for semantic search and duplicate detection\n\n**3. Context Orchestrator (`src/llmstruct/context_orchestrator.py`)**\n- Manages different analysis modes: full, focused, minimal, session\n- Adaptive context switching based on task requirements\n- Token budget optimization for efficient analysis\n\n**4. Metrics & Analytics System**\n- Tracks analysis progress, token usage, efficiency metrics\n- Can measure analysis quality and identify bottlenecks\n- Integration with CLI for automated reporting\n\n**These systems mean we already have internal capabilities to:**\n- Parse and analyze the 272 modules automatically\n- Detect duplicates and dead code through struct.json analysis\n- Generate dependency maps and architectural insights\n- Optimize analysis workflows for maximum efficiency\n",
      "üéØ ORIGINAL EXPERT CONSENSUS": "**Unanimous Recommendation from DeepSeek Coder, Mistral, and Grok:**\n- **Primary Choice:** GPT-4.1 with 1M context window\n- **Budget:** $22 total for comprehensive architectural analysis\n- **Method:** Structured prompts with CoT protection\n- **Goal:** Complete architectural analysis with implementation roadmap\n",
      "‚úÖ SERIES 1 RESULTS (Grok-based)": "- **5 consultations completed:** $0.315 spent\n- **Remaining budget:** $21.685\n- **Key outputs:** Structured templates, 4-level analysis approach, diagram specs\n- **Status:** Strong foundation established\n",
      "ü§ñ LOCAL OLLAMA CAPABILITIES ANALYSIS": "**Hardware:** RTX 3060 Ti (8GB VRAM)\n**Available:** 233 models total, 135 compatible with hardware\n\n**Top Models by Category (from full analysis):**\n\n**Code Analysis Leaders:**\n- **starcoder2** (959.8K pulls) - Next-gen transparent code LLM, 3B/7B/15B sizes\n- **deepseek-coder-v2** (836.7K pulls) - GPT4-Turbo comparable performance, 16B/236B\n- **deepseek-coder** (737.9K pulls) - 2 trillion token training, 33B parameters\n\n**Reasoning & Analysis:**\n- **openthinker** (527.7K pulls) - DeepSeek-R1 derived, thinking capabilities, 7B/32B\n- **wizardlm2** (368.9K pulls) - Microsoft AI reasoning specialist, 7B\n- **reflection** (105.7K pulls) - Self-correcting reasoning, 70B\n\n**Embeddings for Search:**\n- Multiple embedding models with 22M-335M parameters\n- 738.1K+ pulls indicating proven reliability\n- Perfect for semantic analysis of struct.json\n\n**Vision for Diagrams:**\n- Various vision models with 902.8K+ pulls\n- Sizes from 3B to 72B parameters\n- Capable of diagram generation and analysis\n",
      "ü§î STRATEGIC DECISION NEEDED": "**Four approaches now possible:**\n\n1. **Original GPT-4.1 Plan** ($22, 1M context)\n   - Comprehensive single-model analysis\n   - Proven expert recommendation\n\n2. **Continued Grok Series** ($21.685, proven effective)\n   - Strategic consultation approach\n   - Series 1 showed excellent results\n\n3. **Hybrid: 1M Token Models + Strategic Grok + Ollama** (needs careful planning)\n   - Combine best of all worlds\n   - Strategic decisions via high-end models\n   - Technical implementation via local models\n\n4. **Self-Analysis Enhanced Approach**\n   - Leverage built-in LLMStruct AI systems\n   - Use struct.json for automated analysis\n   - Combine internal capabilities with external consultation\n",
      "‚ùì SPECIFIC QUESTION FOR EXPERT ANALYSIS": "**Given our LLMStruct project with built-in AI capabilities, which approach would be most effective for:**\n\n1. **Comprehensive architectural analysis** leveraging existing struct.json and AI systems\n2. **Critical project cleanup** using automated tools + expert guidance\n3. **Implementation-ready outputs** with diagrams and documentation\n4. **Maximum value** within budget constraints\n\n**Key considerations:**\n- We already have AI self-awareness systems that can analyze the 272 modules\n- struct.json contains complete project knowledge that could accelerate analysis\n- RTX 3060 Ti can run sophisticated local models for technical tasks\n- $21.685 budget allows for strategic high-level consultations\n\n**Should we proceed with original GPT-4.1 plan, continue proven Grok approach, implement hybrid strategy, or create novel approach combining internal AI capabilities with external expertise?**\n\n**Please recommend specific Ollama models and strategic approach considering our existing AI infrastructure.** "
    },
    "raw_content": "# üß† COMPREHENSIVE LLMStruct ANALYSIS CONTEXT v2.0\n## Strategic Decision Point: GPT-4.1 vs Grok vs Hybrid Approach\n\n### üìä PROJECT STATUS\n**LLMStruct AI-Enhanced Development Environment**\n- **Scale:** 272 modules, 1857 functions, 183 classes\n- **Critical Problems:** \n  - Giant number of unsorted files, docs, scripts, tests and so on\n  - 8 different bot versions need consolidation\n  - Architectural analysis needed across all subsystems\n\n### ü§ñ BUILT-IN AI SELF-ANALYSIS CAPABILITIES\n**LLMStruct has advanced AI systems that could assist analysis:**\n\n**1. AI Self-Awareness System (`src/llmstruct/ai_self_awareness.py`)**\n- SystemCapabilityDiscovery - analyzes all 272 modules automatically\n- Can search through 1857 functions and 183 classes intelligently\n- Real-time capability analysis and smart caching\n- Already indexed the entire project structure\n\n**2. struct.json - Complete Project Knowledge Base**\n- Contains parsed analysis of all 272 modules\n- Function signatures, dependencies, class hierarchies\n- Auto-generated and constantly updated\n- Can be used for semantic search and duplicate detection\n\n**3. Context Orchestrator (`src/llmstruct/context_orchestrator.py`)**\n- Manages different analysis modes: full, focused, minimal, session\n- Adaptive context switching based on task requirements\n- Token budget optimization for efficient analysis\n\n**4. Metrics & Analytics System**\n- Tracks analysis progress, token usage, efficiency metrics\n- Can measure analysis quality and identify bottlenecks\n- Integration with CLI for automated reporting\n\n**These systems mean we already have internal capabilities to:**\n- Parse and analyze the 272 modules automatically\n- Detect duplicates and dead code through struct.json analysis\n- Generate dependency maps and architectural insights\n- Optimize analysis workflows for maximum efficiency\n\n### üéØ ORIGINAL EXPERT CONSENSUS\n**Unanimous Recommendation from DeepSeek Coder, Mistral, and Grok:**\n- **Primary Choice:** GPT-4.1 with 1M context window\n- **Budget:** $22 total for comprehensive architectural analysis\n- **Method:** Structured prompts with CoT protection\n- **Goal:** Complete architectural analysis with implementation roadmap\n\n### ‚úÖ SERIES 1 RESULTS (Grok-based)\n- **5 consultations completed:** $0.315 spent\n- **Remaining budget:** $21.685\n- **Key outputs:** Structured templates, 4-level analysis approach, diagram specs\n- **Status:** Strong foundation established\n\n### ü§ñ LOCAL OLLAMA CAPABILITIES ANALYSIS\n**Hardware:** RTX 3060 Ti (8GB VRAM)\n**Available:** 233 models total, 135 compatible with hardware\n\n**Top Models by Category (from full analysis):**\n\n**Code Analysis Leaders:**\n- **starcoder2** (959.8K pulls) - Next-gen transparent code LLM, 3B/7B/15B sizes\n- **deepseek-coder-v2** (836.7K pulls) - GPT4-Turbo comparable performance, 16B/236B\n- **deepseek-coder** (737.9K pulls) - 2 trillion token training, 33B parameters\n\n**Reasoning & Analysis:**\n- **openthinker** (527.7K pulls) - DeepSeek-R1 derived, thinking capabilities, 7B/32B\n- **wizardlm2** (368.9K pulls) - Microsoft AI reasoning specialist, 7B\n- **reflection** (105.7K pulls) - Self-correcting reasoning, 70B\n\n**Embeddings for Search:**\n- Multiple embedding models with 22M-335M parameters\n- 738.1K+ pulls indicating proven reliability\n- Perfect for semantic analysis of struct.json\n\n**Vision for Diagrams:**\n- Various vision models with 902.8K+ pulls\n- Sizes from 3B to 72B parameters\n- Capable of diagram generation and analysis\n\n### ü§î STRATEGIC DECISION NEEDED\n**Four approaches now possible:**\n\n1. **Original GPT-4.1 Plan** ($22, 1M context)\n   - Comprehensive single-model analysis\n   - Proven expert recommendation\n\n2. **Continued Grok Series** ($21.685, proven effective)\n   - Strategic consultation approach\n   - Series 1 showed excellent results\n\n3. **Hybrid: 1M Token Models + Strategic Grok + Ollama** (needs careful planning)\n   - Combine best of all worlds\n   - Strategic decisions via high-end models\n   - Technical implementation via local models\n\n4. **Self-Analysis Enhanced Approach**\n   - Leverage built-in LLMStruct AI systems\n   - Use struct.json for automated analysis\n   - Combine internal capabilities with external consultation\n\n### ‚ùì SPECIFIC QUESTION FOR EXPERT ANALYSIS\n**Given our LLMStruct project with built-in AI capabilities, which approach would be most effective for:**\n\n1. **Comprehensive architectural analysis** leveraging existing struct.json and AI systems\n2. **Critical project cleanup** using automated tools + expert guidance\n3. **Implementation-ready outputs** with diagrams and documentation\n4. **Maximum value** within budget constraints\n\n**Key considerations:**\n- We already have AI self-awareness systems that can analyze the 272 modules\n- struct.json contains complete project knowledge that could accelerate analysis\n- RTX 3060 Ti can run sophisticated local models for technical tasks\n- $21.685 budget allows for strategic high-level consultations\n\n**Should we proceed with original GPT-4.1 plan, continue proven Grok approach, implement hybrid strategy, or create novel approach combining internal AI capabilities with external expertise?**\n\n**Please recommend specific Ollama models and strategic approach considering our existing AI infrastructure.** "
  },
  "ollama_analysis": {
    "parsing_info": {
      "total_parsed": 344,
      "valid_models": 287,
      "source_file": "tmp/ollama_list.txt",
      "parsing_algorithm": "improved_block_based",
      "timestamp": "2025-01-27"
    },
    "models_data": {
      "summary": {
        "total_models": 287,
        "rtx3060ti_compatible": 245,
        "top_categories": [
          "code_analysis",
          "reasoning",
          "vision",
          "embedding",
          "general"
        ]
      },
      "categories": {
        "code_analysis": {
          "description": "Models specifically designed for code analysis, generation, and debugging",
          "models": [
            {
              "name": "devstral",
              "desc": "Devstral: the best open source model for coding agents",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "llava",
              "desc": "üåã LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicu...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "qwen2.5-coder",
              "desc": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, co...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "codellama",
              "desc": "A large language model that can use text prompts to generate and discuss code.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "starcoder2",
              "desc": "StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes:...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "deepseek-coder-v2",
              "desc": "An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-T...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "deepseek-coder",
              "desc": "DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "codegemma",
              "desc": "CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "codestral",
              "desc": "Codestral is Mistral AI‚Äôs first-ever code model designed for code generation tasks.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "granite-code",
              "desc": "A family of open foundation models by IBM for Code Intelligence",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "starcoder",
              "desc": "StarCoder is a code generation model trained on 80+ programming languages.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "codeqwen",
              "desc": "CodeQwen1.5 is a large language model pretrained on a large amount of code data.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "codegeex4",
              "desc": "A versatile model for AI software development scenarios, including code completion.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "mistral-large",
              "desc": "Mistral Large 2 is Mistral's new flagship model that is significantly more capable in code generatio...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "stable-code",
              "desc": "Stable Code 3B is a coding model with instruct and code completion variants on par with models such ...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "deepcoder",
              "desc": "DeepCoder is a fully open-Source 14B coder model at O3-mini level, with a 1.5B version also availabl...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "wizardcoder",
              "desc": "State-of-the-art code generation model",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "sqlcoder",
              "desc": "SQLCoder is a code completion model fined-tuned on StarCoder for SQL generation tasks",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "granite3-dense",
              "desc": "The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrie...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "dolphincoder",
              "desc": "A 7B and 15B uncensored variant of the Dolphin model family that excels at coding, based on StarCode...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "phind-codellama",
              "desc": "Code generation model based on Code Llama.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "yi-coder",
              "desc": "Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding perfo...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "athene-v2",
              "desc": "Athene-V2 is a 72B parameter model which excels at code completion, mathematics, and log extraction ...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "deepseek-v2.5",
              "desc": "An upgraded version of DeekSeek-V2 that integrates the general and coding abilities of both DeepSeek...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "granite-embedding",
              "desc": "The IBM Granite Embedding 30M and 278M models models are text-only dense biencoder embedding models,...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "llama-pro",
              "desc": "An expansion of Llama 2 that specializes in integrating both general language understanding and doma...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "opencoder",
              "desc": "OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting ...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "codeup",
              "desc": "Great code generation model based on Llama2.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "falcon2",
              "desc": "Falcon2 is an 11B parameters causal decoder-only model built by TII and trained over 5T tokens.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "magicoder",
              "desc": "üé© Magicoder is a family of 7B parameter models trained on 75K synthetic instruction data using OSS-I...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "codebooga",
              "desc": "A high-performing code instruct model created by merging two existing code models.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "tulu3",
              "desc": "T√ºlu 3 is a leading instruction following model family, offering fully open-source data, code, and r...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "open-orca-platypus2",
              "desc": "Merge of the Open Orca OpenChat model and the Garage-bAInd Platypus 2 model. Designed for chat and c...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            }
          ]
        },
        "reasoning": {
          "description": "Models with enhanced reasoning and thinking capabilities",
          "models": [
            {
              "name": "deepseek-r1",
              "desc": "DeepSeek-R1 first-generation of open reasoning models with comparable performance to OpenAI-o3.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "llama3.2-vision",
              "desc": "Llama 3.2 Vision is a collection of instruction-tuned image reasoning generative models in 11B and 9...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "qwq",
              "desc": "QwQ is the reasoning model of the Qwen series.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "phi",
              "desc": "Phi-2: a 2.7B language model by Microsoft Research that demonstrates outstanding reasoning and langu...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "openthinker",
              "desc": "A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "wizardlm2",
              "desc": "State of the art large language model from Microsoft AI with improved performance on complex chat, m...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "wizard-vicuna-uncensored",
              "desc": "Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric H...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "phi4-mini",
              "desc": "Phi-4-mini brings significant enhancements in multilingual support, reasoning, and mathematics, and ...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "reflection",
              "desc": "A high-performing model trained with a new technique called Reflection-tuning that teaches a LLM to ...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "wizard-math",
              "desc": "Model focused on math and logic problems",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "granite3.2",
              "desc": "Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for thinking capabilit...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "cogito",
              "desc": "Cogito v1 Preview is a family of hybrid reasoning models by Deep Cogito that outperform the best ava...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "deepscaler",
              "desc": "A fine-tuned version of Deepseek-R1-Distilled-Qwen-1.5B that surpasses the performance of OpenAI‚Äôs o...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "internlm2",
              "desc": "InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding reasoning capa...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "granite3.3",
              "desc": "IBM Granite 2B and 8B models are 128K context length language models that have been fine-tuned for i...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "wizardlm",
              "desc": "General use model based on Llama 2.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "orca2",
              "desc": "Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models. The mo...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "wizardlm-uncensored",
              "desc": "Uncensored version of Wizard LM model",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "phi4-reasoning",
              "desc": "Phi 4 reasoning and reasoning plus are 14-billion parameter open-weight reasoning models that rival ...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "smallthinker",
              "desc": "A new small reasoning model fine-tuned from the Qwen 2.5 3B Instruct model.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "exaone-deep",
              "desc": "EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benc...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "mathstral",
              "desc": "MathŒ£tral: a 7B model designed for math reasoning and scientific discovery by Mistral AI.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "r1-1776",
              "desc": "A version of the DeepSeek-R1 model that has been post trained to provide unbiased, accurate, and fac...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "marco-o1",
              "desc": "An open large reasoning model for real-world solutions by the Alibaba International Digital Commerce...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "wizard-vicuna",
              "desc": "Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "phi4-mini-reasoning",
              "desc": "Phi 4 mini reasoning is a lightweight open model that balances efficiency with advanced reasoning ab...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            }
          ]
        },
        "vision": {
          "description": "Multimodal models capable of processing images and diagrams",
          "models": [
            {
              "name": "llama4",
              "desc": "Meta's latest collection of multimodal models.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "qwen2.5vl",
              "desc": "Flagship vision-language model of Qwen and also a significant leap from the previous Qwen2-VL.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "minicpm-v",
              "desc": "A series of multimodal LLMs (MLLMs) designed for vision-language understanding.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "llava-llama3",
              "desc": "A LLaVA model fine-tuned from Llama 3 Instruct with better scores in several benchmarks.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "moondream",
              "desc": "moondream2 is a small vision language model designed to run efficiently on edge devices.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "bakllava",
              "desc": "BakLLaVA is a multimodal model consisting of the Mistral 7B base model augmented with the LLaVA arch...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "mistral-small3.1",
              "desc": "Building upon Mistral Small 3, Mistral Small 3.1 (2503) adds state-of-the-art vision understanding a...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "llava-phi3",
              "desc": "A new small LLaVA model fine-tuned from Phi 3 Mini.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "granite3.2-vision",
              "desc": "A compact and efficient vision-language model, specifically designed for visual document understandi...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            }
          ]
        },
        "embedding": {
          "description": "Specialized models for text embeddings and semantic search",
          "models": [
            {
              "name": "nomic-embed-text",
              "desc": "A high-performing open embedding model with a large token context window.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "mxbai-embed-large",
              "desc": "State-of-the-art large embedding model from mixedbread.ai",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "snowflake-arctic-embed",
              "desc": "A suite of text embedding models by Snowflake, optimized for performance.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "all-minilm",
              "desc": "Embedding models on very large sentence level datasets.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "bge-large",
              "desc": "Embedding model from BAAI mapping texts to vectors.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "snowflake-arctic-embed2",
              "desc": "Snowflake's frontier embedding model. Arctic Embed 2.0 adds multilingual support without sacrificing...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "paraphrase-multilingual",
              "desc": "Sentence-transformers model that can be used for tasks like clustering or semantic search.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            }
          ]
        },
        "general": {
          "description": "General-purpose language models for various tasks",
          "models": [
            {
              "name": "gemma3",
              "desc": "The current, most capable model that runs on a single GPU.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "qwen3",
              "desc": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive sui...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "llama3.3",
              "desc": "New state of the art 70B model. Llama 3.3 70B offers similar performance compared to the Llama 3.1 4...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "phi4",
              "desc": "Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "14b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "2.4M",
              "updated": "4 months ago"
            },
            {
              "name": "llama3.2",
              "desc": "Meta's Llama 3.2 goes small with 1B and 3B models.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "llama3.1",
              "desc": "Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "mistral",
              "desc": "The 7B model released by Mistral AI, updated to version 0.3.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "qwen2.5",
              "desc": "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillio...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "llama3",
              "desc": "Meta Llama 3: The most capable openly available LLM to date",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "8b",
              "desc": "",
              "caps": [],
              "sizes": [
                "70b"
              ],
              "pulls": "8.2M",
              "updated": "1 year ago"
            },
            {
              "name": "gemma2",
              "desc": "Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "2b",
              "desc": "",
              "caps": [],
              "sizes": [
                "9b",
                "27b"
              ],
              "pulls": "4.9M",
              "updated": "10 months ago"
            },
            {
              "name": "qwen",
              "desc": "Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "0.5b",
              "desc": "",
              "caps": [],
              "sizes": [
                "1.8b",
                "4b",
                "7b",
                "14b",
                "32b",
                "72b",
                "110b"
              ],
              "pulls": "4.7M",
              "updated": "1 year ago"
            },
            {
              "name": "gemma",
              "desc": "Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to ...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "2b",
              "desc": "",
              "caps": [],
              "sizes": [
                "7b"
              ],
              "pulls": "4.6M",
              "updated": "1 year ago"
            },
            {
              "name": "qwen2",
              "desc": "Qwen2 is a new series of large language models from Alibaba group",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "llama2",
              "desc": "Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "13b",
                "70b"
              ],
              "pulls": "3.5M",
              "updated": "1 year ago"
            },
            {
              "name": "phi3",
              "desc": "Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsof...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "3.8b",
              "desc": "",
              "caps": [],
              "sizes": [
                "14b"
              ],
              "pulls": "3.3M",
              "updated": "10 months ago"
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "13b",
                "34b",
                "70b"
              ],
              "pulls": "2.1M",
              "updated": "10 months ago"
            },
            {
              "name": "tinyllama",
              "desc": "The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "1.1b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "1.9M",
              "updated": "1 year ago"
            },
            {
              "name": "mistral-nemo",
              "desc": "A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVI...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "deepseek-v3",
              "desc": "A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for e...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "671b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "1.4M",
              "updated": "4 months ago"
            },
            {
              "name": "dolphin3",
              "desc": "Dolphin 3.0 Llama 3.1 8B üê¨ is the next generation of the Dolphin series of instruct-tuned models des...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "8b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "1.3M",
              "updated": "4 months ago"
            },
            {
              "name": "olmo2",
              "desc": "OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "13b"
              ],
              "pulls": "1.2M",
              "updated": "4 months ago"
            },
            {
              "name": "bge-m3",
              "desc": "BGE-M3 is a new model from BAAI distinguished for its versatility in Multi-Functionality, Multi-Ling...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "llama2-uncensored",
              "desc": "Uncensored Llama 2 model by George Sung and Jarrad Hope.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "70b"
              ],
              "pulls": "1.2M",
              "updated": "1 year ago"
            },
            {
              "name": "mixtral",
              "desc": "A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter ...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "3b",
              "desc": "",
              "caps": [],
              "sizes": [
                "7b",
                "15b"
              ],
              "pulls": "959.8K",
              "updated": "8 months ago"
            },
            {
              "name": "mistral-small",
              "desc": "Mistral Small 3 sets a new benchmark in the ‚Äúsmall‚Äù Large Language Models category below 70B.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "smollm2",
              "desc": "SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B paramet...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "16b",
              "desc": "",
              "caps": [],
              "sizes": [
                "236b"
              ],
              "pulls": "836.7K",
              "updated": "8 months ago"
            },
            {
              "name": "1.3b",
              "desc": "",
              "caps": [],
              "sizes": [
                "6.7b",
                "33b"
              ],
              "pulls": "737.9K",
              "updated": "1 year ago"
            },
            {
              "name": "2b",
              "desc": "",
              "caps": [],
              "sizes": [
                "7b"
              ],
              "pulls": "614.2K",
              "updated": "10 months ago"
            },
            {
              "name": "dolphin-mixtral",
              "desc": "Uncensored, 8x7b and 8x22b fine-tuned models based on the Mixtral mixture of experts models that exc...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "8x7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "8x22b"
              ],
              "pulls": "576.5K",
              "updated": "5 months ago"
            },
            {
              "name": "2.7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "544K",
              "updated": "1 year ago"
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "32b"
              ],
              "pulls": "527.7K",
              "updated": "1 month ago"
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "8x22b"
              ],
              "pulls": "368.9K",
              "updated": "1 year ago"
            },
            {
              "name": "dolphin-mistral",
              "desc": "The uncensored Dolphin model based on Mistral that excels at coding tasks. Updated to version 2.8.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "348.7K",
              "updated": "1 year ago"
            },
            {
              "name": "orca-mini",
              "desc": "A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level ha...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "3b",
              "desc": "",
              "caps": [],
              "sizes": [
                "7b",
                "13b",
                "70b"
              ],
              "pulls": "341.6K",
              "updated": "1 year ago"
            },
            {
              "name": "dolphin-llama3",
              "desc": "Dolphin 2.9 is a new model with 8B and 70B sizes by Eric Hartford based on Llama 3 that has a variet...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "8b",
              "desc": "",
              "caps": [],
              "sizes": [
                "70b"
              ],
              "pulls": "325.1K",
              "updated": "1 year ago"
            },
            {
              "name": "command-r",
              "desc": "Command R is a Large Language Model optimized for conversational interaction and long context tasks.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "22b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "294.6K",
              "updated": "8 months ago"
            },
            {
              "name": "hermes3",
              "desc": "Hermes 3 is the latest version of the flagship Hermes series of LLMs by Nous Research",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "phi3.5",
              "desc": "A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger ...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "3.8b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "280.2K",
              "updated": "8 months ago"
            },
            {
              "name": "yi",
              "desc": "Yi 1.5 is a high-performing, bilingual language model.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "6b",
              "desc": "",
              "caps": [],
              "sizes": [
                "9b",
                "34b"
              ],
              "pulls": "279.7K",
              "updated": "1 year ago"
            },
            {
              "name": "smollm",
              "desc": "ü™ê A family of small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality datas...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "135m",
              "desc": "360m",
              "caps": [],
              "sizes": [
                "1.7b"
              ],
              "pulls": "269.9K",
              "updated": "9 months ago"
            },
            {
              "name": "zephyr",
              "desc": "Zephyr is a series of fine-tuned versions of the Mistral and Mixtral models that are trained to act ...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "141b"
              ],
              "pulls": "267.9K",
              "updated": "1 year ago"
            },
            {
              "name": "3b",
              "desc": "",
              "caps": [],
              "sizes": [
                "8b",
                "20b",
                "34b"
              ],
              "pulls": "211K",
              "updated": "8 months ago"
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "13b",
                "30b"
              ],
              "pulls": "204.1K",
              "updated": "1 year ago"
            },
            {
              "name": "1b",
              "desc": "",
              "caps": [],
              "sizes": [
                "3b",
                "7b",
                "15b"
              ],
              "pulls": "198.9K",
              "updated": "1 year ago"
            },
            {
              "name": "vicuna",
              "desc": "General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "13b",
                "33b"
              ],
              "pulls": "182.6K",
              "updated": "1 year ago"
            },
            {
              "name": "mistral-openorca",
              "desc": "Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "171K",
              "updated": "1 year ago"
            },
            {
              "name": "openchat",
              "desc": "A family of open-source models trained on a wide variety of data, surpassing ChatGPT on various benc...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "164.5K",
              "updated": "1 year ago"
            },
            {
              "name": "deepseek-v2",
              "desc": "A strong, economical, and efficient Mixture-of-Experts language model.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "16b",
              "desc": "",
              "caps": [],
              "sizes": [
                "236b"
              ],
              "pulls": "156.6K",
              "updated": "11 months ago"
            },
            {
              "name": "llama2-chinese",
              "desc": "Llama 2 based model fine tuned to improve Chinese dialogue ability.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "13b"
              ],
              "pulls": "153.7K",
              "updated": "1 year ago"
            },
            {
              "name": "openhermes",
              "desc": "OpenHermes 2.5 is a 7B model fine-tuned by Teknium on Mistral with fully open datasets.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "151K",
              "desc": "Pulls",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": "1 year ago"
            },
            {
              "name": "deepseek-llm",
              "desc": "An advanced language model crafted with 2 trillion bilingual tokens.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "67b"
              ],
              "pulls": "150.6K",
              "updated": "1 year ago"
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "146.6K",
              "updated": "11 months ago"
            },
            {
              "name": "9b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "145.1K",
              "updated": "10 months ago"
            },
            {
              "name": "aya",
              "desc": "Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "8b",
              "desc": "",
              "caps": [],
              "sizes": [
                "35b"
              ],
              "pulls": "144.2K",
              "updated": "1 year ago"
            },
            {
              "name": "glm4",
              "desc": "A strong multi-lingual general language model with competitive performance to Llama 3.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "9b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "132.9K",
              "updated": "10 months ago"
            },
            {
              "name": "3b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "132.1K",
              "updated": "1 year ago"
            },
            {
              "name": "tinydolphin",
              "desc": "An experimental 1.1B parameter model trained on the new Dolphin 2.8 dataset by Eric Hartford and bas...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "1.1b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "129.3K",
              "updated": "1 year ago"
            },
            {
              "name": "1.5b",
              "desc": "",
              "caps": [],
              "sizes": [
                "14b"
              ],
              "pulls": "128.8K",
              "updated": "1 month ago"
            },
            {
              "name": "nous-hermes2",
              "desc": "The powerful family of models by Nous Research that excels at scientific discussion and coding tasks...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "10.7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "34b"
              ],
              "pulls": "128.6K",
              "updated": "1 year ago"
            },
            {
              "name": "qwen2-math",
              "desc": "Qwen2 Math is a series of specialized math language models built upon the Qwen2 LLMs, which signific...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "1.5b",
              "desc": "",
              "caps": [],
              "sizes": [
                "7b",
                "72b"
              ],
              "pulls": "128.6K",
              "updated": "9 months ago"
            },
            {
              "name": "command-r-plus",
              "desc": "Command R+ is a powerful, scalable large language model purpose-built to excel at real-world enterpr...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "33b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "125.2K",
              "updated": "1 year ago"
            },
            {
              "name": "stablelm2",
              "desc": "Stable LM 2 is a state-of-the-art 1.6B and 12B parameter language model trained on multilingual data...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "1.6b",
              "desc": "",
              "caps": [],
              "sizes": [
                "12b"
              ],
              "pulls": "113.8K",
              "updated": "1 year ago"
            },
            {
              "name": "neural-chat",
              "desc": "A fine-tuned model based on Mistral with good coverage of domain and language.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "113.4K",
              "updated": "1 year ago"
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "15b"
              ],
              "pulls": "106K",
              "updated": "1 year ago"
            },
            {
              "name": "llama3-chatqa",
              "desc": "A model from NVIDIA based on Llama 3 that excels at conversational question answering (QA) and retri...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "8b",
              "desc": "",
              "caps": [],
              "sizes": [
                "70b"
              ],
              "pulls": "105.7K",
              "updated": "1 year ago"
            },
            {
              "name": "70b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "105.7K",
              "updated": "8 months ago"
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "13b",
                "70b"
              ],
              "pulls": "104.7K",
              "updated": "1 year ago"
            },
            {
              "name": "llama3-gradient",
              "desc": "This model extends LLama-3 8B's context length from 8k to over 1m tokens.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "8b",
              "desc": "",
              "caps": [],
              "sizes": [
                "70b"
              ],
              "pulls": "102K",
              "updated": "1 year ago"
            },
            {
              "name": "granite3.1-dense",
              "desc": "The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "samantha-mistral",
              "desc": "A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistra...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "94.1K",
              "updated": "1 year ago"
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "15b"
              ],
              "pulls": "89.5K",
              "updated": "1 year ago"
            },
            {
              "name": "nous-hermes",
              "desc": "General use models based on Llama and Llama 2 from Nous Research.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "13b"
              ],
              "pulls": "87.4K",
              "updated": "1 year ago"
            },
            {
              "name": "xwinlm",
              "desc": "Conversational model based on Llama 2 that performs competitively on various benchmarks.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "13b"
              ],
              "pulls": "86.5K",
              "updated": "1 year ago"
            },
            {
              "name": "starling-lm",
              "desc": "Starling is a large language model trained by reinforcement learning from AI feedback focused on imp...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "85.7K",
              "updated": "1 year ago"
            },
            {
              "name": "34b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "85.1K",
              "updated": "1 year ago"
            },
            {
              "name": "1.5b",
              "desc": "",
              "caps": [],
              "sizes": [
                "9b"
              ],
              "pulls": "83.5K",
              "updated": "8 months ago"
            },
            {
              "name": "nemotron-mini",
              "desc": "A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function ca...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "solar",
              "desc": "A compact, yet powerful 10.7B large language model designed for single-turn conversation.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "10.7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "82.3K",
              "updated": "1 year ago"
            },
            {
              "name": "yarn-llama2",
              "desc": "An extension of Llama 2 that supports a context of up to 128k tokens.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "13b"
              ],
              "pulls": "81K",
              "updated": "1 year ago"
            },
            {
              "name": "1.5b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "80.9K",
              "updated": "3 months ago"
            },
            {
              "name": "1m",
              "desc": "",
              "caps": [],
              "sizes": [
                "1.8b",
                "7b",
                "20b"
              ],
              "pulls": "80.3K",
              "updated": "9 months ago"
            },
            {
              "name": "77.1K",
              "desc": "Pulls",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": "1 year ago"
            },
            {
              "name": "exaone3.5",
              "desc": "EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ran...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "2.4b",
              "desc": "",
              "caps": [],
              "sizes": [
                "7.8b",
                "32b"
              ],
              "pulls": "75.3K",
              "updated": "5 months ago"
            },
            {
              "name": "dolphin-phi",
              "desc": "2.7B uncensored Dolphin model by Eric Hartford, based on the Phi language model by Microsoft Researc...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "2.7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "74.9K",
              "updated": "1 year ago"
            },
            {
              "name": "falcon",
              "desc": "A large language model built by the Technology Innovation Institute (TII) for use in summarization, ...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "40b",
                "180b"
              ],
              "pulls": "74.8K",
              "updated": "1 year ago"
            },
            {
              "name": "nemotron",
              "desc": "Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfu...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "llama3-groq-tool-use",
              "desc": "A series of models from Groq that represent a significant advancement in open-source AI capabilities...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "13b"
              ],
              "pulls": "66.6K",
              "updated": "1 year ago"
            },
            {
              "name": "13b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "66.4K",
              "updated": "1 year ago"
            },
            {
              "name": "aya-expanse",
              "desc": "Cohere For AI's language models trained to perform well across 23 different languages.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "stable-beluga",
              "desc": "Llama 2 based model fine tuned on an Orca-style dataset. Originally called Free Willy.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "13b",
                "70b"
              ],
              "pulls": "60.7K",
              "updated": "1 year ago"
            },
            {
              "name": "14b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "60.1K",
              "updated": "4 weeks ago"
            },
            {
              "name": "nous-hermes2-mixtral",
              "desc": "The Nous Hermes 2 model from Nous Research, now trained over Mixtral.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "8x7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "59.6K",
              "updated": "5 months ago"
            },
            {
              "name": "3b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "58.7K",
              "updated": "5 months ago"
            },
            {
              "name": "falcon3",
              "desc": "A family of efficient AI models under 10B parameters performant in science, math, and coding through...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "1b",
              "desc": "",
              "caps": [],
              "sizes": [
                "3b",
                "7b",
                "10b"
              ],
              "pulls": "54.7K",
              "updated": "5 months ago"
            },
            {
              "name": "meditron",
              "desc": "Open-source medical large language model adapted from Llama 2 to the medical domain.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [
                "70b"
              ],
              "pulls": "54.6K",
              "updated": "1 year ago"
            },
            {
              "name": "236b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "54K",
              "updated": "8 months ago"
            },
            {
              "name": "medllama2",
              "desc": "Fine-tuned Llama 2 model to answer medical questions based on an open source medical dataset.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "52.6K",
              "updated": "1 year ago"
            },
            {
              "name": "granite3-moe",
              "desc": "The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM desi...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "47.7K",
              "desc": "Pulls",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": "1 year ago"
            },
            {
              "name": "1.5b",
              "desc": "",
              "caps": [],
              "sizes": [
                "8b"
              ],
              "pulls": "46.7K",
              "updated": "6 months ago"
            },
            {
              "name": "yarn-mistral",
              "desc": "An extension of Mistral to support context windows of 64K or 128K.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "46.6K",
              "updated": "1 year ago"
            },
            {
              "name": "granite3.1-moe",
              "desc": "The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM d...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "2.4b",
              "desc": "",
              "caps": [],
              "sizes": [
                "7.8b",
                "32b"
              ],
              "pulls": "44.5K",
              "updated": "2 months ago"
            },
            {
              "name": "nexusraven",
              "desc": "Nexus Raven is a 13B instruction tuned model for function calling tasks.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "13b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "43.3K",
              "updated": "1 year ago"
            },
            {
              "name": "shieldgemma",
              "desc": "ShieldGemma is set of instruction tuned models for evaluating the safety of text prompt input and te...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "2b",
              "desc": "",
              "caps": [],
              "sizes": [
                "9b",
                "27b"
              ],
              "pulls": "41.5K",
              "updated": "7 months ago"
            },
            {
              "name": "13b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "41.4K",
              "updated": "1 year ago"
            },
            {
              "name": "everythinglm",
              "desc": "Uncensored Llama2 based model with support for a 16K context window.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "13b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "41.1K",
              "updated": "1 year ago"
            },
            {
              "name": "llama-guard3",
              "desc": "Llama Guard 3 is a series of models fine-tuned for content safety classification of LLM inputs and r...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "1b",
              "desc": "",
              "caps": [],
              "sizes": [
                "8b"
              ],
              "pulls": "40.5K",
              "updated": "7 months ago"
            },
            {
              "name": "reader-lm",
              "desc": "A series of models that convert HTML content to Markdown content, which is useful for content conver...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "0.5b",
              "desc": "",
              "caps": [],
              "sizes": [
                "1.5b"
              ],
              "pulls": "39.9K",
              "updated": "8 months ago"
            },
            {
              "name": "stablelm-zephyr",
              "desc": "A lightweight chat model allowing accurate, and responsive output without requiring high-end hardwar...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "3b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "39.2K",
              "updated": "1 year ago"
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "37.2K",
              "updated": "10 months ago"
            },
            {
              "name": "solar-pro",
              "desc": "Solar Pro Preview: an advanced large language model (LLM) with 22 billion parameters designed to fit...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "22b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "36.7K",
              "updated": "8 months ago"
            },
            {
              "name": "70b",
              "desc": "",
              "caps": [],
              "sizes": [
                "671b"
              ],
              "pulls": "36.5K",
              "updated": "3 months ago"
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "36.4K",
              "updated": "5 months ago"
            },
            {
              "name": "duckdb-nsql",
              "desc": "7B parameter text-to-SQL model made by MotherDuck and Numbers Station.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "35K",
              "updated": "1 year ago"
            },
            {
              "name": "11b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "34.9K",
              "updated": "1 year ago"
            },
            {
              "name": "command-r7b",
              "desc": "The smallest model in Cohere's R series delivers top-tier speed, efficiency, and quality to build po...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "33.8K",
              "updated": "1 year ago"
            },
            {
              "name": "mistrallite",
              "desc": "MistralLite is a fine-tuned model based on Mistral with enhanced capabilities of processing long con...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "33.4K",
              "updated": "1 year ago"
            },
            {
              "name": "34b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "33K",
              "updated": "1 year ago"
            },
            {
              "name": "13b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "30.9K",
              "updated": "1 year ago"
            },
            {
              "name": "nuextract",
              "desc": "A 3.8B model fine-tuned on a private high-quality synthetic dataset for information extraction, base...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "3.8b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "29.2K",
              "updated": "10 months ago"
            },
            {
              "name": "bespoke-minicheck",
              "desc": "A state-of-the-art fact-checking model developed by Bespoke Labs.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "27.2K",
              "updated": "8 months ago"
            },
            {
              "name": "8b",
              "desc": "",
              "caps": [],
              "sizes": [
                "70b"
              ],
              "pulls": "26.3K",
              "updated": "5 months ago"
            },
            {
              "name": "megadolphin",
              "desc": "MegaDolphin-2.2-120b is a transformation of Dolphin-2.2-70b created by interleaving the model with i...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "120b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "26.2K",
              "updated": "1 year ago"
            },
            {
              "name": "notux",
              "desc": "A top-performing mixture of experts model, fine-tuned with high-quality data.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "8x7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "25.4K",
              "updated": "1 year ago"
            },
            {
              "name": "13b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "25K",
              "updated": "1 year ago"
            },
            {
              "name": "notus",
              "desc": "A 7B chat model fine-tuned with high-quality data and based on Zephyr.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "7b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "24.7K",
              "updated": "1 year ago"
            },
            {
              "name": "goliath",
              "desc": "A language model created by combining two fine-tuned Llama 2 70B models into one.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "23.7K",
              "desc": "Pulls",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": "1 year ago"
            },
            {
              "name": "firefunction-v2",
              "desc": "An open weights function calling model based on Llama 3, competitive with GPT-4o function calling ca...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "dbrx",
              "desc": "DBRX is an open, general-purpose LLM created by Databricks.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "132b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "19.3K",
              "updated": "1 year ago"
            },
            {
              "name": "granite3-guardian",
              "desc": "The IBM Granite Guardian 3.0 2B and 8B models are designed to detect risks in prompts and/or respons...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "2b",
              "desc": "",
              "caps": [],
              "sizes": [
                "8b"
              ],
              "pulls": "19.1K",
              "updated": "6 months ago"
            },
            {
              "name": "3.8b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "18.7K",
              "updated": "4 weeks ago"
            },
            {
              "name": "alfred",
              "desc": "A robust conversational model designed to be used for both chat and instruct use cases.",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "40b",
              "desc": "",
              "caps": [],
              "sizes": [],
              "pulls": "17K",
              "updated": "1 year ago"
            },
            {
              "name": "command-a",
              "desc": "111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "sailor2",
              "desc": "Sailor2 are multilingual language models made for South-East Asia. Available in 1B, 8B, and 20B para...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            },
            {
              "name": "1b",
              "desc": "",
              "caps": [],
              "sizes": [
                "8b",
                "20b"
              ],
              "pulls": "12K",
              "updated": "5 months ago"
            },
            {
              "name": "command-r7b-arabic",
              "desc": "A new state-of-the-art version of the lightweight Command R7B model that excels in advanced Arabic l...",
              "caps": [],
              "sizes": [],
              "pulls": "",
              "updated": ""
            }
          ]
        }
      },
      "by_hardware_compatibility": {
        "rtx3060ti_8gb": [
          {
            "name": "deepseek-r1",
            "desc": "DeepSeek-R1 first-generation of open reasoning models with comparable performance to OpenAI-o3.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "gemma3",
            "desc": "The current, most capable model that runs on a single GPU.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "qwen3",
            "desc": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive sui...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "devstral",
            "desc": "Devstral: the best open source model for coding agents",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "llama4",
            "desc": "Meta's latest collection of multimodal models.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "qwen2.5vl",
            "desc": "Flagship vision-language model of Qwen and also a significant leap from the previous Qwen2-VL.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "llama3.3",
            "desc": "New state of the art 70B model. Llama 3.3 70B offers similar performance compared to the Llama 3.1 4...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "phi4",
            "desc": "Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "14b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "2.4M",
            "updated": "4 months ago"
          },
          {
            "name": "llama3.2",
            "desc": "Meta's Llama 3.2 goes small with 1B and 3B models.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "llama3.1",
            "desc": "Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "nomic-embed-text",
            "desc": "A high-performing open embedding model with a large token context window.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "mistral",
            "desc": "The 7B model released by Mistral AI, updated to version 0.3.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "qwen2.5",
            "desc": "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillio...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "llama3",
            "desc": "Meta Llama 3: The most capable openly available LLM to date",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "llava",
            "desc": "üåã LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicu...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "qwen2.5-coder",
            "desc": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, co...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "gemma2",
            "desc": "Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "qwen",
            "desc": "Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "gemma",
            "desc": "Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to ...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "2b",
            "desc": "",
            "caps": [],
            "sizes": [
              "7b"
            ],
            "pulls": "4.6M",
            "updated": "1 year ago"
          },
          {
            "name": "qwen2",
            "desc": "Qwen2 is a new series of large language models from Alibaba group",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "llama2",
            "desc": "Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "mxbai-embed-large",
            "desc": "State-of-the-art large embedding model from mixedbread.ai",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "phi3",
            "desc": "Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsof...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "llama3.2-vision",
            "desc": "Llama 3.2 Vision is a collection of instruction-tuned image reasoning generative models in 11B and 9...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "codellama",
            "desc": "A large language model that can use text prompts to generate and discuss code.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "tinyllama",
            "desc": "The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "1.1b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "1.9M",
            "updated": "1 year ago"
          },
          {
            "name": "mistral-nemo",
            "desc": "A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVI...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "minicpm-v",
            "desc": "A series of multimodal LLMs (MLLMs) designed for vision-language understanding.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "qwq",
            "desc": "QwQ is the reasoning model of the Qwen series.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "deepseek-v3",
            "desc": "A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for e...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "671b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "1.4M",
            "updated": "4 months ago"
          },
          {
            "name": "dolphin3",
            "desc": "Dolphin 3.0 Llama 3.1 8B üê¨ is the next generation of the Dolphin series of instruct-tuned models des...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "8b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "1.3M",
            "updated": "4 months ago"
          },
          {
            "name": "olmo2",
            "desc": "OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "bge-m3",
            "desc": "BGE-M3 is a new model from BAAI distinguished for its versatility in Multi-Functionality, Multi-Ling...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "llama2-uncensored",
            "desc": "Uncensored Llama 2 model by George Sung and Jarrad Hope.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "mixtral",
            "desc": "A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter ...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "starcoder2",
            "desc": "StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes:...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "llava-llama3",
            "desc": "A LLaVA model fine-tuned from Llama 3 Instruct with better scores in several benchmarks.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "mistral-small",
            "desc": "Mistral Small 3 sets a new benchmark in the ‚Äúsmall‚Äù Large Language Models category below 70B.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "smollm2",
            "desc": "SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B paramet...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "deepseek-coder-v2",
            "desc": "An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-T...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "snowflake-arctic-embed",
            "desc": "A suite of text embedding models by Snowflake, optimized for performance.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "deepseek-coder",
            "desc": "DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "codegemma",
            "desc": "CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "2b",
            "desc": "",
            "caps": [],
            "sizes": [
              "7b"
            ],
            "pulls": "614.2K",
            "updated": "10 months ago"
          },
          {
            "name": "dolphin-mixtral",
            "desc": "Uncensored, 8x7b and 8x22b fine-tuned models based on the Mixtral mixture of experts models that exc...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "phi",
            "desc": "Phi-2: a 2.7B language model by Microsoft Research that demonstrates outstanding reasoning and langu...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "2.7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "544K",
            "updated": "1 year ago"
          },
          {
            "name": "all-minilm",
            "desc": "Embedding models on very large sentence level datasets.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "openthinker",
            "desc": "A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "wizardlm2",
            "desc": "State of the art large language model from Microsoft AI with improved performance on complex chat, m...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "dolphin-mistral",
            "desc": "The uncensored Dolphin model based on Mistral that excels at coding tasks. Updated to version 2.8.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "348.7K",
            "updated": "1 year ago"
          },
          {
            "name": "orca-mini",
            "desc": "A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level ha...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "dolphin-llama3",
            "desc": "Dolphin 2.9 is a new model with 8B and 70B sizes by Eric Hartford based on Llama 3 that has a variet...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "command-r",
            "desc": "Command R is a Large Language Model optimized for conversational interaction and long context tasks.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "codestral",
            "desc": "Codestral is Mistral AI‚Äôs first-ever code model designed for code generation tasks.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "22b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "294.6K",
            "updated": "8 months ago"
          },
          {
            "name": "hermes3",
            "desc": "Hermes 3 is the latest version of the flagship Hermes series of LLMs by Nous Research",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "phi3.5",
            "desc": "A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger ...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "3.8b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "280.2K",
            "updated": "8 months ago"
          },
          {
            "name": "yi",
            "desc": "Yi 1.5 is a high-performing, bilingual language model.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "smollm",
            "desc": "ü™ê A family of small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality datas...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "135m",
            "desc": "360m",
            "caps": [],
            "sizes": [
              "1.7b"
            ],
            "pulls": "269.9K",
            "updated": "9 months ago"
          },
          {
            "name": "zephyr",
            "desc": "Zephyr is a series of fine-tuned versions of the Mistral and Mixtral models that are trained to act ...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "granite-code",
            "desc": "A family of open foundation models by IBM for Code Intelligence",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "wizard-vicuna-uncensored",
            "desc": "Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric H...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "starcoder",
            "desc": "StarCoder is a code generation model trained on 80+ programming languages.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "moondream",
            "desc": "moondream2 is a small vision language model designed to run efficiently on edge devices.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "vicuna",
            "desc": "General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "mistral-openorca",
            "desc": "Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "171K",
            "updated": "1 year ago"
          },
          {
            "name": "openchat",
            "desc": "A family of open-source models trained on a wide variety of data, surpassing ChatGPT on various benc...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "164.5K",
            "updated": "1 year ago"
          },
          {
            "name": "phi4-mini",
            "desc": "Phi-4-mini brings significant enhancements in multilingual support, reasoning, and mathematics, and ...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "deepseek-v2",
            "desc": "A strong, economical, and efficient Mixture-of-Experts language model.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "llama2-chinese",
            "desc": "Llama 2 based model fine tuned to improve Chinese dialogue ability.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "openhermes",
            "desc": "OpenHermes 2.5 is a 7B model fine-tuned by Teknium on Mistral with fully open datasets.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "151K",
            "desc": "Pulls",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": "1 year ago"
          },
          {
            "name": "deepseek-llm",
            "desc": "An advanced language model crafted with 2 trillion bilingual tokens.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "codeqwen",
            "desc": "CodeQwen1.5 is a large language model pretrained on a large amount of code data.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "146.6K",
            "updated": "11 months ago"
          },
          {
            "name": "codegeex4",
            "desc": "A versatile model for AI software development scenarios, including code completion.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "9b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "145.1K",
            "updated": "10 months ago"
          },
          {
            "name": "aya",
            "desc": "Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "mistral-large",
            "desc": "Mistral Large 2 is Mistral's new flagship model that is significantly more capable in code generatio...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "glm4",
            "desc": "A strong multi-lingual general language model with competitive performance to Llama 3.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "9b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "132.9K",
            "updated": "10 months ago"
          },
          {
            "name": "stable-code",
            "desc": "Stable Code 3B is a coding model with instruct and code completion variants on par with models such ...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "3b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "132.1K",
            "updated": "1 year ago"
          },
          {
            "name": "tinydolphin",
            "desc": "An experimental 1.1B parameter model trained on the new Dolphin 2.8 dataset by Eric Hartford and bas...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "1.1b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "129.3K",
            "updated": "1 year ago"
          },
          {
            "name": "deepcoder",
            "desc": "DeepCoder is a fully open-Source 14B coder model at O3-mini level, with a 1.5B version also availabl...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "nous-hermes2",
            "desc": "The powerful family of models by Nous Research that excels at scientific discussion and coding tasks...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "qwen2-math",
            "desc": "Qwen2 Math is a series of specialized math language models built upon the Qwen2 LLMs, which signific...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "command-r-plus",
            "desc": "Command R+ is a powerful, scalable large language model purpose-built to excel at real-world enterpr...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "wizardcoder",
            "desc": "State-of-the-art code generation model",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "33b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "125.2K",
            "updated": "1 year ago"
          },
          {
            "name": "bakllava",
            "desc": "BakLLaVA is a multimodal model consisting of the Mistral 7B base model augmented with the LLaVA arch...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "mistral-small3.1",
            "desc": "Building upon Mistral Small 3, Mistral Small 3.1 (2503) adds state-of-the-art vision understanding a...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "stablelm2",
            "desc": "Stable LM 2 is a state-of-the-art 1.6B and 12B parameter language model trained on multilingual data...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "neural-chat",
            "desc": "A fine-tuned model based on Mistral with good coverage of domain and language.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "113.4K",
            "updated": "1 year ago"
          },
          {
            "name": "sqlcoder",
            "desc": "SQLCoder is a code completion model fined-tuned on StarCoder for SQL generation tasks",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "llama3-chatqa",
            "desc": "A model from NVIDIA based on Llama 3 that excels at conversational question answering (QA) and retri...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "reflection",
            "desc": "A high-performing model trained with a new technique called Reflection-tuning that teaches a LLM to ...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "70b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "105.7K",
            "updated": "8 months ago"
          },
          {
            "name": "wizard-math",
            "desc": "Model focused on math and logic problems",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "bge-large",
            "desc": "Embedding model from BAAI mapping texts to vectors.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "granite3.2",
            "desc": "Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for thinking capabilit...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "llama3-gradient",
            "desc": "This model extends LLama-3 8B's context length from 8k to over 1m tokens.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "granite3-dense",
            "desc": "The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrie...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "granite3.1-dense",
            "desc": "The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "samantha-mistral",
            "desc": "A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistra...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "94.1K",
            "updated": "1 year ago"
          },
          {
            "name": "cogito",
            "desc": "Cogito v1 Preview is a family of hybrid reasoning models by Deep Cogito that outperform the best ava...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "llava-phi3",
            "desc": "A new small LLaVA model fine-tuned from Phi 3 Mini.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "dolphincoder",
            "desc": "A 7B and 15B uncensored variant of the Dolphin model family that excels at coding, based on StarCode...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "nous-hermes",
            "desc": "General use models based on Llama and Llama 2 from Nous Research.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "xwinlm",
            "desc": "Conversational model based on Llama 2 that performs competitively on various benchmarks.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "starling-lm",
            "desc": "Starling is a large language model trained by reinforcement learning from AI feedback focused on imp...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "85.7K",
            "updated": "1 year ago"
          },
          {
            "name": "phind-codellama",
            "desc": "Code generation model based on Code Llama.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "34b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "85.1K",
            "updated": "1 year ago"
          },
          {
            "name": "granite3.2-vision",
            "desc": "A compact and efficient vision-language model, specifically designed for visual document understandi...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "snowflake-arctic-embed2",
            "desc": "Snowflake's frontier embedding model. Arctic Embed 2.0 adds multilingual support without sacrificing...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "yi-coder",
            "desc": "Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding perfo...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "nemotron-mini",
            "desc": "A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function ca...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "solar",
            "desc": "A compact, yet powerful 10.7B large language model designed for single-turn conversation.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "10.7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "82.3K",
            "updated": "1 year ago"
          },
          {
            "name": "athene-v2",
            "desc": "Athene-V2 is a 72B parameter model which excels at code completion, mathematics, and log extraction ...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "yarn-llama2",
            "desc": "An extension of Llama 2 that supports a context of up to 128k tokens.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "deepscaler",
            "desc": "A fine-tuned version of Deepseek-R1-Distilled-Qwen-1.5B that surpasses the performance of OpenAI‚Äôs o...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "1.5b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "80.9K",
            "updated": "3 months ago"
          },
          {
            "name": "internlm2",
            "desc": "InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding reasoning capa...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "granite3.3",
            "desc": "IBM Granite 2B and 8B models are 128K context length language models that have been fine-tuned for i...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "wizardlm",
            "desc": "General use model based on Llama 2.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "77.1K",
            "desc": "Pulls",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": "1 year ago"
          },
          {
            "name": "exaone3.5",
            "desc": "EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ran...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "dolphin-phi",
            "desc": "2.7B uncensored Dolphin model by Eric Hartford, based on the Phi language model by Microsoft Researc...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "2.7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "74.9K",
            "updated": "1 year ago"
          },
          {
            "name": "falcon",
            "desc": "A large language model built by the Technology Innovation Institute (TII) for use in summarization, ...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "nemotron",
            "desc": "Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfu...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "llama3-groq-tool-use",
            "desc": "A series of models from Groq that represent a significant advancement in open-source AI capabilities...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "orca2",
            "desc": "Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models. The mo...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "wizardlm-uncensored",
            "desc": "Uncensored version of Wizard LM model",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "13b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "66.4K",
            "updated": "1 year ago"
          },
          {
            "name": "aya-expanse",
            "desc": "Cohere For AI's language models trained to perform well across 23 different languages.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "paraphrase-multilingual",
            "desc": "Sentence-transformers model that can be used for tasks like clustering or semantic search.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "stable-beluga",
            "desc": "Llama 2 based model fine tuned on an Orca-style dataset. Originally called Free Willy.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "phi4-reasoning",
            "desc": "Phi 4 reasoning and reasoning plus are 14-billion parameter open-weight reasoning models that rival ...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "14b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "60.1K",
            "updated": "4 weeks ago"
          },
          {
            "name": "nous-hermes2-mixtral",
            "desc": "The Nous Hermes 2 model from Nous Research, now trained over Mixtral.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "8x7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "59.6K",
            "updated": "5 months ago"
          },
          {
            "name": "smallthinker",
            "desc": "A new small reasoning model fine-tuned from the Qwen 2.5 3B Instruct model.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "3b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "58.7K",
            "updated": "5 months ago"
          },
          {
            "name": "falcon3",
            "desc": "A family of efficient AI models under 10B parameters performant in science, math, and coding through...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "meditron",
            "desc": "Open-source medical large language model adapted from Llama 2 to the medical domain.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "deepseek-v2.5",
            "desc": "An upgraded version of DeekSeek-V2 that integrates the general and coding abilities of both DeepSeek...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "236b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "54K",
            "updated": "8 months ago"
          },
          {
            "name": "medllama2",
            "desc": "Fine-tuned Llama 2 model to answer medical questions based on an open source medical dataset.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "52.6K",
            "updated": "1 year ago"
          },
          {
            "name": "granite-embedding",
            "desc": "The IBM Granite Embedding 30M and 278M models models are text-only dense biencoder embedding models,...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "granite3-moe",
            "desc": "The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM desi...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "llama-pro",
            "desc": "An expansion of Llama 2 that specializes in integrating both general language understanding and doma...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "47.7K",
            "desc": "Pulls",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": "1 year ago"
          },
          {
            "name": "opencoder",
            "desc": "OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting ...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "yarn-mistral",
            "desc": "An extension of Mistral to support context windows of 64K or 128K.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "46.6K",
            "updated": "1 year ago"
          },
          {
            "name": "granite3.1-moe",
            "desc": "The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM d...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "exaone-deep",
            "desc": "EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benc...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "nexusraven",
            "desc": "Nexus Raven is a 13B instruction tuned model for function calling tasks.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "13b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "43.3K",
            "updated": "1 year ago"
          },
          {
            "name": "shieldgemma",
            "desc": "ShieldGemma is set of instruction tuned models for evaluating the safety of text prompt input and te...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "codeup",
            "desc": "Great code generation model based on Llama2.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "13b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "41.4K",
            "updated": "1 year ago"
          },
          {
            "name": "everythinglm",
            "desc": "Uncensored Llama2 based model with support for a 16K context window.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "13b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "41.1K",
            "updated": "1 year ago"
          },
          {
            "name": "llama-guard3",
            "desc": "Llama Guard 3 is a series of models fine-tuned for content safety classification of LLM inputs and r...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "reader-lm",
            "desc": "A series of models that convert HTML content to Markdown content, which is useful for content conver...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "0.5b",
            "desc": "",
            "caps": [],
            "sizes": [
              "1.5b"
            ],
            "pulls": "39.9K",
            "updated": "8 months ago"
          },
          {
            "name": "stablelm-zephyr",
            "desc": "A lightweight chat model allowing accurate, and responsive output without requiring high-end hardwar...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "3b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "39.2K",
            "updated": "1 year ago"
          },
          {
            "name": "mathstral",
            "desc": "MathŒ£tral: a 7B model designed for math reasoning and scientific discovery by Mistral AI.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "37.2K",
            "updated": "10 months ago"
          },
          {
            "name": "solar-pro",
            "desc": "Solar Pro Preview: an advanced large language model (LLM) with 22 billion parameters designed to fit...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "22b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "36.7K",
            "updated": "8 months ago"
          },
          {
            "name": "r1-1776",
            "desc": "A version of the DeepSeek-R1 model that has been post trained to provide unbiased, accurate, and fac...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "marco-o1",
            "desc": "An open large reasoning model for real-world solutions by the Alibaba International Digital Commerce...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "36.4K",
            "updated": "5 months ago"
          },
          {
            "name": "duckdb-nsql",
            "desc": "7B parameter text-to-SQL model made by MotherDuck and Numbers Station.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "35K",
            "updated": "1 year ago"
          },
          {
            "name": "falcon2",
            "desc": "Falcon2 is an 11B parameters causal decoder-only model built by TII and trained over 5T tokens.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "11b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "34.9K",
            "updated": "1 year ago"
          },
          {
            "name": "command-r7b",
            "desc": "The smallest model in Cohere's R series delivers top-tier speed, efficiency, and quality to build po...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "magicoder",
            "desc": "üé© Magicoder is a family of 7B parameter models trained on 75K synthetic instruction data using OSS-I...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "33.8K",
            "updated": "1 year ago"
          },
          {
            "name": "mistrallite",
            "desc": "MistralLite is a fine-tuned model based on Mistral with enhanced capabilities of processing long con...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "33.4K",
            "updated": "1 year ago"
          },
          {
            "name": "codebooga",
            "desc": "A high-performing code instruct model created by merging two existing code models.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "34b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "33K",
            "updated": "1 year ago"
          },
          {
            "name": "wizard-vicuna",
            "desc": "Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "13b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "30.9K",
            "updated": "1 year ago"
          },
          {
            "name": "nuextract",
            "desc": "A 3.8B model fine-tuned on a private high-quality synthetic dataset for information extraction, base...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "3.8b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "29.2K",
            "updated": "10 months ago"
          },
          {
            "name": "bespoke-minicheck",
            "desc": "A state-of-the-art fact-checking model developed by Bespoke Labs.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "27.2K",
            "updated": "8 months ago"
          },
          {
            "name": "tulu3",
            "desc": "T√ºlu 3 is a leading instruction following model family, offering fully open-source data, code, and r...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "megadolphin",
            "desc": "MegaDolphin-2.2-120b is a transformation of Dolphin-2.2-70b created by interleaving the model with i...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "120b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "26.2K",
            "updated": "1 year ago"
          },
          {
            "name": "notux",
            "desc": "A top-performing mixture of experts model, fine-tuned with high-quality data.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "8x7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "25.4K",
            "updated": "1 year ago"
          },
          {
            "name": "open-orca-platypus2",
            "desc": "Merge of the Open Orca OpenChat model and the Garage-bAInd Platypus 2 model. Designed for chat and c...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "13b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "25K",
            "updated": "1 year ago"
          },
          {
            "name": "notus",
            "desc": "A 7B chat model fine-tuned with high-quality data and based on Zephyr.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "24.7K",
            "updated": "1 year ago"
          },
          {
            "name": "goliath",
            "desc": "A language model created by combining two fine-tuned Llama 2 70B models into one.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "23.7K",
            "desc": "Pulls",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": "1 year ago"
          },
          {
            "name": "firefunction-v2",
            "desc": "An open weights function calling model based on Llama 3, competitive with GPT-4o function calling ca...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "dbrx",
            "desc": "DBRX is an open, general-purpose LLM created by Databricks.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "132b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "19.3K",
            "updated": "1 year ago"
          },
          {
            "name": "granite3-guardian",
            "desc": "The IBM Granite Guardian 3.0 2B and 8B models are designed to detect risks in prompts and/or respons...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "phi4-mini-reasoning",
            "desc": "Phi 4 mini reasoning is a lightweight open model that balances efficiency with advanced reasoning ab...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "3.8b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "18.7K",
            "updated": "4 weeks ago"
          },
          {
            "name": "alfred",
            "desc": "A robust conversational model designed to be used for both chat and instruct use cases.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "40b",
            "desc": "",
            "caps": [],
            "sizes": [],
            "pulls": "17K",
            "updated": "1 year ago"
          },
          {
            "name": "command-a",
            "desc": "111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "sailor2",
            "desc": "Sailor2 are multilingual language models made for South-East Asia. Available in 1B, 8B, and 20B para...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "command-r7b-arabic",
            "desc": "A new state-of-the-art version of the lightweight Command R7B model that excels in advanced Arabic l...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          }
        ],
        "rtx3060ti_8gb_quantized": [
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "13b"
            ],
            "pulls": "1.2M",
            "updated": "4 months ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "13b"
            ],
            "pulls": "153.7K",
            "updated": "1 year ago"
          },
          {
            "name": "1.6b",
            "desc": "",
            "caps": [],
            "sizes": [
              "12b"
            ],
            "pulls": "113.8K",
            "updated": "1 year ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "13b"
            ],
            "pulls": "87.4K",
            "updated": "1 year ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "13b"
            ],
            "pulls": "86.5K",
            "updated": "1 year ago"
          },
          {
            "name": "1.5b",
            "desc": "",
            "caps": [],
            "sizes": [
              "9b"
            ],
            "pulls": "83.5K",
            "updated": "8 months ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "13b"
            ],
            "pulls": "81K",
            "updated": "1 year ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "13b"
            ],
            "pulls": "66.6K",
            "updated": "1 year ago"
          },
          {
            "name": "1b",
            "desc": "",
            "caps": [],
            "sizes": [
              "3b",
              "7b",
              "10b"
            ],
            "pulls": "54.7K",
            "updated": "5 months ago"
          },
          {
            "name": "1.5b",
            "desc": "",
            "caps": [],
            "sizes": [
              "8b"
            ],
            "pulls": "46.7K",
            "updated": "6 months ago"
          },
          {
            "name": "1b",
            "desc": "",
            "caps": [],
            "sizes": [
              "8b"
            ],
            "pulls": "40.5K",
            "updated": "7 months ago"
          },
          {
            "name": "2b",
            "desc": "",
            "caps": [],
            "sizes": [
              "8b"
            ],
            "pulls": "19.1K",
            "updated": "6 months ago"
          }
        ],
        "requires_more_vram": [
          {
            "name": "8b",
            "desc": "",
            "caps": [],
            "sizes": [
              "70b"
            ],
            "pulls": "8.2M",
            "updated": "1 year ago"
          },
          {
            "name": "2b",
            "desc": "",
            "caps": [],
            "sizes": [
              "9b",
              "27b"
            ],
            "pulls": "4.9M",
            "updated": "10 months ago"
          },
          {
            "name": "0.5b",
            "desc": "",
            "caps": [],
            "sizes": [
              "1.8b",
              "4b",
              "7b",
              "14b",
              "32b",
              "72b",
              "110b"
            ],
            "pulls": "4.7M",
            "updated": "1 year ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "13b",
              "70b"
            ],
            "pulls": "3.5M",
            "updated": "1 year ago"
          },
          {
            "name": "3.8b",
            "desc": "",
            "caps": [],
            "sizes": [
              "14b"
            ],
            "pulls": "3.3M",
            "updated": "10 months ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "13b",
              "34b",
              "70b"
            ],
            "pulls": "2.1M",
            "updated": "10 months ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "70b"
            ],
            "pulls": "1.2M",
            "updated": "1 year ago"
          },
          {
            "name": "3b",
            "desc": "",
            "caps": [],
            "sizes": [
              "7b",
              "15b"
            ],
            "pulls": "959.8K",
            "updated": "8 months ago"
          },
          {
            "name": "16b",
            "desc": "",
            "caps": [],
            "sizes": [
              "236b"
            ],
            "pulls": "836.7K",
            "updated": "8 months ago"
          },
          {
            "name": "1.3b",
            "desc": "",
            "caps": [],
            "sizes": [
              "6.7b",
              "33b"
            ],
            "pulls": "737.9K",
            "updated": "1 year ago"
          },
          {
            "name": "8x7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "8x22b"
            ],
            "pulls": "576.5K",
            "updated": "5 months ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "32b"
            ],
            "pulls": "527.7K",
            "updated": "1 month ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "8x22b"
            ],
            "pulls": "368.9K",
            "updated": "1 year ago"
          },
          {
            "name": "3b",
            "desc": "",
            "caps": [],
            "sizes": [
              "7b",
              "13b",
              "70b"
            ],
            "pulls": "341.6K",
            "updated": "1 year ago"
          },
          {
            "name": "8b",
            "desc": "",
            "caps": [],
            "sizes": [
              "70b"
            ],
            "pulls": "325.1K",
            "updated": "1 year ago"
          },
          {
            "name": "6b",
            "desc": "",
            "caps": [],
            "sizes": [
              "9b",
              "34b"
            ],
            "pulls": "279.7K",
            "updated": "1 year ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "141b"
            ],
            "pulls": "267.9K",
            "updated": "1 year ago"
          },
          {
            "name": "3b",
            "desc": "",
            "caps": [],
            "sizes": [
              "8b",
              "20b",
              "34b"
            ],
            "pulls": "211K",
            "updated": "8 months ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "13b",
              "30b"
            ],
            "pulls": "204.1K",
            "updated": "1 year ago"
          },
          {
            "name": "1b",
            "desc": "",
            "caps": [],
            "sizes": [
              "3b",
              "7b",
              "15b"
            ],
            "pulls": "198.9K",
            "updated": "1 year ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "13b",
              "33b"
            ],
            "pulls": "182.6K",
            "updated": "1 year ago"
          },
          {
            "name": "16b",
            "desc": "",
            "caps": [],
            "sizes": [
              "236b"
            ],
            "pulls": "156.6K",
            "updated": "11 months ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "67b"
            ],
            "pulls": "150.6K",
            "updated": "1 year ago"
          },
          {
            "name": "8b",
            "desc": "",
            "caps": [],
            "sizes": [
              "35b"
            ],
            "pulls": "144.2K",
            "updated": "1 year ago"
          },
          {
            "name": "1.5b",
            "desc": "",
            "caps": [],
            "sizes": [
              "14b"
            ],
            "pulls": "128.8K",
            "updated": "1 month ago"
          },
          {
            "name": "10.7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "34b"
            ],
            "pulls": "128.6K",
            "updated": "1 year ago"
          },
          {
            "name": "1.5b",
            "desc": "",
            "caps": [],
            "sizes": [
              "7b",
              "72b"
            ],
            "pulls": "128.6K",
            "updated": "9 months ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "15b"
            ],
            "pulls": "106K",
            "updated": "1 year ago"
          },
          {
            "name": "8b",
            "desc": "",
            "caps": [],
            "sizes": [
              "70b"
            ],
            "pulls": "105.7K",
            "updated": "1 year ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "13b",
              "70b"
            ],
            "pulls": "104.7K",
            "updated": "1 year ago"
          },
          {
            "name": "8b",
            "desc": "",
            "caps": [],
            "sizes": [
              "70b"
            ],
            "pulls": "102K",
            "updated": "1 year ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "15b"
            ],
            "pulls": "89.5K",
            "updated": "1 year ago"
          },
          {
            "name": "1m",
            "desc": "",
            "caps": [],
            "sizes": [
              "1.8b",
              "7b",
              "20b"
            ],
            "pulls": "80.3K",
            "updated": "9 months ago"
          },
          {
            "name": "2.4b",
            "desc": "",
            "caps": [],
            "sizes": [
              "7.8b",
              "32b"
            ],
            "pulls": "75.3K",
            "updated": "5 months ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "40b",
              "180b"
            ],
            "pulls": "74.8K",
            "updated": "1 year ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "13b",
              "70b"
            ],
            "pulls": "60.7K",
            "updated": "1 year ago"
          },
          {
            "name": "7b",
            "desc": "",
            "caps": [],
            "sizes": [
              "70b"
            ],
            "pulls": "54.6K",
            "updated": "1 year ago"
          },
          {
            "name": "2.4b",
            "desc": "",
            "caps": [],
            "sizes": [
              "7.8b",
              "32b"
            ],
            "pulls": "44.5K",
            "updated": "2 months ago"
          },
          {
            "name": "2b",
            "desc": "",
            "caps": [],
            "sizes": [
              "9b",
              "27b"
            ],
            "pulls": "41.5K",
            "updated": "7 months ago"
          },
          {
            "name": "70b",
            "desc": "",
            "caps": [],
            "sizes": [
              "671b"
            ],
            "pulls": "36.5K",
            "updated": "3 months ago"
          },
          {
            "name": "8b",
            "desc": "",
            "caps": [],
            "sizes": [
              "70b"
            ],
            "pulls": "26.3K",
            "updated": "5 months ago"
          },
          {
            "name": "1b",
            "desc": "",
            "caps": [],
            "sizes": [
              "8b",
              "20b"
            ],
            "pulls": "12K",
            "updated": "5 months ago"
          }
        ]
      },
      "top_models_by_popularity": [
        {
          "name": "8b",
          "description": "",
          "capabilities": [],
          "sizes": [
            "70b"
          ],
          "pulls": "8.2M",
          "tags": "68",
          "updated": "1 year ago"
        },
        {
          "name": "2b",
          "description": "",
          "capabilities": [],
          "sizes": [
            "9b",
            "27b"
          ],
          "pulls": "4.9M",
          "tags": "94",
          "updated": "10 months ago"
        },
        {
          "name": "0.5b",
          "description": "",
          "capabilities": [],
          "sizes": [
            "1.8b",
            "4b",
            "7b",
            "14b",
            "32b",
            "72b",
            "110b"
          ],
          "pulls": "4.7M",
          "tags": "379",
          "updated": "1 year ago"
        },
        {
          "name": "2b",
          "description": "",
          "capabilities": [],
          "sizes": [
            "7b"
          ],
          "pulls": "4.6M",
          "tags": "102",
          "updated": "1 year ago"
        },
        {
          "name": "7b",
          "description": "",
          "capabilities": [],
          "sizes": [
            "13b",
            "70b"
          ],
          "pulls": "3.5M",
          "tags": "102",
          "updated": "1 year ago"
        },
        {
          "name": "3.8b",
          "description": "",
          "capabilities": [],
          "sizes": [
            "14b"
          ],
          "pulls": "3.3M",
          "tags": "72",
          "updated": "10 months ago"
        },
        {
          "name": "14b",
          "description": "",
          "capabilities": [],
          "sizes": [],
          "pulls": "2.4M",
          "tags": "5",
          "updated": "4 months ago"
        },
        {
          "name": "7b",
          "description": "",
          "capabilities": [],
          "sizes": [
            "13b",
            "34b",
            "70b"
          ],
          "pulls": "2.1M",
          "tags": "199",
          "updated": "10 months ago"
        },
        {
          "name": "1.1b",
          "description": "",
          "capabilities": [],
          "sizes": [],
          "pulls": "1.9M",
          "tags": "36",
          "updated": "1 year ago"
        },
        {
          "name": "671b",
          "description": "",
          "capabilities": [],
          "sizes": [],
          "pulls": "1.4M",
          "tags": "5",
          "updated": "4 months ago"
        },
        {
          "name": "8b",
          "description": "",
          "capabilities": [],
          "sizes": [],
          "pulls": "1.3M",
          "tags": "5",
          "updated": "4 months ago"
        },
        {
          "name": "7b",
          "description": "",
          "capabilities": [],
          "sizes": [
            "13b"
          ],
          "pulls": "1.2M",
          "tags": "9",
          "updated": "4 months ago"
        },
        {
          "name": "7b",
          "description": "",
          "capabilities": [],
          "sizes": [
            "70b"
          ],
          "pulls": "1.2M",
          "tags": "34",
          "updated": "1 year ago"
        },
        {
          "name": "3b",
          "description": "",
          "capabilities": [],
          "sizes": [
            "7b",
            "15b"
          ],
          "pulls": "959.8K",
          "tags": "67",
          "updated": "8 months ago"
        },
        {
          "name": "16b",
          "description": "",
          "capabilities": [],
          "sizes": [
            "236b"
          ],
          "pulls": "836.7K",
          "tags": "64",
          "updated": "8 months ago"
        }
      ],
      "recommended_for_llmstruct": {
        "primary_code_analysis": [
          {
            "name": "devstral",
            "desc": "Devstral: the best open source model for coding agents",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "llava",
            "desc": "üåã LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicu...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "qwen2.5-coder",
            "desc": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, co...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "codellama",
            "desc": "A large language model that can use text prompts to generate and discuss code.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "starcoder2",
            "desc": "StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes:...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          }
        ],
        "secondary_reasoning": [
          {
            "name": "deepseek-r1",
            "desc": "DeepSeek-R1 first-generation of open reasoning models with comparable performance to OpenAI-o3.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "llama3.2-vision",
            "desc": "Llama 3.2 Vision is a collection of instruction-tuned image reasoning generative models in 11B and 9...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "qwq",
            "desc": "QwQ is the reasoning model of the Qwen series.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "phi",
            "desc": "Phi-2: a 2.7B language model by Microsoft Research that demonstrates outstanding reasoning and langu...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "openthinker",
            "desc": "A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-...",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          }
        ],
        "embedding_search": [
          {
            "name": "nomic-embed-text",
            "desc": "A high-performing open embedding model with a large token context window.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "mxbai-embed-large",
            "desc": "State-of-the-art large embedding model from mixedbread.ai",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "snowflake-arctic-embed",
            "desc": "A suite of text embedding models by Snowflake, optimized for performance.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "all-minilm",
            "desc": "Embedding models on very large sentence level datasets.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "bge-large",
            "desc": "Embedding model from BAAI mapping texts to vectors.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          }
        ],
        "vision_diagrams": [
          {
            "name": "llama4",
            "desc": "Meta's latest collection of multimodal models.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "qwen2.5vl",
            "desc": "Flagship vision-language model of Qwen and also a significant leap from the previous Qwen2-VL.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "minicpm-v",
            "desc": "A series of multimodal LLMs (MLLMs) designed for vision-language understanding.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "llava-llama3",
            "desc": "A LLaVA model fine-tuned from Llama 3 Instruct with better scores in several benchmarks.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          },
          {
            "name": "moondream",
            "desc": "moondream2 is a small vision language model designed to run efficiently on edge devices.",
            "caps": [],
            "sizes": [],
            "pulls": "",
            "updated": ""
          }
        ]
      }
    },
    "quick_stats": {
      "rtx3060ti_compatible": 245,
      "code_analysis_models": 33,
      "reasoning_models": 26,
      "vision_models": 9,
      "embedding_models": 7
    }
  },
  "project_context": {
    "name": "LLMStruct",
    "scale": "272 modules, 1857 functions, 183 classes",
    "hardware": "RTX 3060 Ti (8GB VRAM)",
    "budget_remaining": "$21.685",
    "goal": "Architectural analysis with Ollama model recommendations"
  },
  "grok_consultation_request": {
    "primary_question": "Which specific Ollama models should we use for LLMStruct architectural analysis on RTX 3060 Ti?",
    "strategy_options": [
      "Original GPT-4.1 plan ($22, 1M context)",
      "Continued Grok series ($21.685, proven effective)",
      "Hybrid approach: Strategic Grok + Local Ollama implementation",
      "Self-analysis enhanced with LLMStruct AI + Ollama support"
    ],
    "specific_needs": [
      "Code analysis for 272 modules",
      "Architectural documentation generation",
      "Bot consolidation strategy (8 versions)",
      "Implementation roadmap with diagrams"
    ],
    "constraints": [
      "RTX 3060 Ti 8GB VRAM limit",
      "Budget: $21.685 remaining",
      "Need production-ready outputs",
      "Leverage existing LLMStruct AI capabilities"
    ]
  }
}