{
 "context": {
  "🧠 COMPREHENSIVE LLMStruct ANALYSIS CONTEXT v2.0": "",
  "Strategic Decision Point: GPT-4.1 vs Grok vs Hybrid Approach": "",
  "📊 PROJECT STATUS": "**LLMStruct AI-Enhanced Development Environment**\n- **Scale:** 272 modules, 1857 functions, 183 classes\n- **Critical Problems:** \n  - Giant number of unsorted files, docs, scripts, tests and so on\n  - 8 different bot versions need consolidation\n  - Architectural analysis needed across all subsystems",
  "🤖 BUILT-IN AI SELF-ANALYSIS CAPABILITIES": "**LLMStruct has advanced AI systems that could assist analysis:**\n\n**1. AI Self-Awareness System (`src/llmstruct/ai_self_awareness.py`)**\n- SystemCapabilityDiscovery - analyzes all 272 modules automatically\n- Can search through 1857 functions and 183 classes intelligently\n- Real-time capability analysis and smart caching\n- Already indexed the entire project structure\n\n**2. struct.json - Complete Project Knowledge Base**\n- Contains parsed analysis of all 272 modules\n- Function signatures, dependencies, class hierarchies\n- Auto-generated and constantly updated\n- Can be used for semantic search and duplicate detection\n\n**3. Context Orchestrator (`src/llmstruct/context_orchestrator.py`)**\n- Manages different analysis modes: full, focused, minimal, session\n- Adaptive context switching based on task requirements\n- Token budget optimization for efficient analysis\n\n**4. Metrics & Analytics System**\n- Tracks analysis progress, token usage, efficiency metrics\n- Can measure analysis quality and identify bottlenecks\n- Integration with CLI for automated reporting\n\n**These systems mean we already have internal capabilities to:**\n- Parse and analyze the 272 modules automatically\n- Detect duplicates and dead code through struct.json analysis\n- Generate dependency maps and architectural insights\n- Optimize analysis workflows for maximum efficiency",
  "🎯 ORIGINAL EXPERT CONSENSUS": "**Unanimous Recommendation from DeepSeek Coder, Mistral, and Grok:**\n- **Primary Choice:** GPT-4.1 with 1M context window\n- **Budget:** $22 total for comprehensive architectural analysis\n- **Method:** Structured prompts with CoT protection\n- **Goal:** Complete architectural analysis with implementation roadmap",
  "✅ SERIES 1 RESULTS (Grok-based)": "- **5 consultations completed:** $0.315 spent\n- **Remaining budget:** $21.685\n- **Key outputs:** Structured templates, 4-level analysis approach, diagram specs\n- **Status:** Strong foundation established",
  "🤖 LOCAL OLLAMA CAPABILITIES ANALYSIS": "**Hardware:** RTX 3060 Ti (8GB VRAM)\n**Available:** 233 models total, 135 compatible with hardware\n\n**Top Models by Category (from full analysis):**\n\n**Code Analysis Leaders:**\n- **starcoder2** (959.8K pulls) - Next-gen transparent code LLM, 3B/7B/15B sizes\n- **deepseek-coder-v2** (836.7K pulls) - GPT4-Turbo comparable performance, 16B/236B\n- **deepseek-coder** (737.9K pulls) - 2 trillion token training, 33B parameters\n\n**Reasoning & Analysis:**\n- **openthinker** (527.7K pulls) - DeepSeek-R1 derived, thinking capabilities, 7B/32B\n- **wizardlm2** (368.9K pulls) - Microsoft AI reasoning specialist, 7B\n- **reflection** (105.7K pulls) - Self-correcting reasoning, 70B\n\n**Embeddings for Search:**\n- Multiple embedding models with 22M-335M parameters\n- 738.1K+ pulls indicating proven reliability\n- Perfect for semantic analysis of struct.json\n\n**Vision for Diagrams:**\n- Various vision models with 902.8K+ pulls\n- Sizes from 3B to 72B parameters\n- Capable of diagram generation and analysis",
  "🤔 STRATEGIC DECISION NEEDED": "**Four approaches now possible:**\n\n1. **Original GPT-4.1 Plan** ($22, 1M context)\n   - Comprehensive single-model analysis\n   - Proven expert recommendation\n\n2. **Continued Grok Series** ($21.685, proven effective)\n   - Strategic consultation approach\n   - Series 1 showed excellent results\n\n3. **Hybrid: 1M Token Models + Strategic Grok + Ollama** (needs careful planning)\n   - Combine best of all worlds\n   - Strategic decisions via high-end models\n   - Technical implementation via local models\n\n4. **Self-Analysis Enhanced Approach**\n   - Leverage built-in LLMStruct AI systems\n   - Use struct.json for automated analysis\n   - Combine internal capabilities with external consultation",
  "❓ SPECIFIC QUESTION FOR EXPERT ANALYSIS": "**Given our LLMStruct project with built-in AI capabilities, which approach would be most effective for:**\n\n1. **Comprehensive architectural analysis** leveraging existing struct.json and AI systems\n2. **Critical project cleanup** using automated tools + expert guidance\n3. **Implementation-ready outputs** with diagrams and documentation\n4. **Maximum value** within budget constraints\n\n**Key considerations:**\n- We already have AI self-awareness systems that can analyze the 272 modules\n- struct.json contains complete project knowledge that could accelerate analysis\n- RTX 3060 Ti can run sophisticated local models for technical tasks\n- $21.685 budget allows for strategic high-level consultations\n\n**Should we proceed with original GPT-4.1 plan, continue proven Grok approach, implement hybrid strategy, or create novel approach combining internal AI capabilities with external expertise?**\n\n**Please recommend specific Ollama models and strategic approach considering our existing AI infrastructure.**",
  
  "📏 CONTEXT SIZE REQUIREMENTS FOR LLM ANALYSIS": "**Critical consideration: Different models have different context windows that affect analysis approach:**\n\n**Large Context Models (1M+ tokens):**\n- **GPT-4.1**: 1M context - can analyze entire struct.json + full documentation\n- **Gemini 2.5 Pro**: 1M context - cost-effective for comprehensive analysis\n- **Approach**: Single comprehensive prompt with full project context\n\n**Medium Context Models (200K tokens):**\n- **Claude 4 Sonnet**: 200K context - requires chunked analysis\n- **Claude 4 Opus**: 200K context - premium quality but needs segmentation\n- **Approach**: Multi-stage analysis with context orchestration\n\n**Local Ollama Models (8K-128K):**\n- **deepseek-coder-v2**: ~32K context - focused module analysis\n- **starcoder2**: ~16K context - code-specific tasks\n- **qwen3**: ~32K context - general analysis and reasoning\n- **Approach**: Micro-analysis with aggregation through struct.json\n\n**Context Strategy Impact:**\n- **Current request**: ~11K tokens (fits in all models)\n- **struct.json**: ~120K tokens (medium+ models only)\n- **Full project docs**: ~500K+ tokens (large context only)\n- **Combined analysis**: 1M+ tokens (GPT-4.1/Gemini optimal)",
  
  "🏗️ TARGET PROJECT STRUCTURE": "**Professional modular architecture optimized for LLM-assisted development:**\n\n```\nllmstruct/\n├── .github/workflows/          # CI/CD automation\n├── src/llmstruct/             # Core package\n│   ├── cli/                   # Command-line interface\n│   │   ├── commands/          # Modular CLI commands\n│   │   └── config.py          # Configuration management\n│   ├── core/                  # Core orchestration\n│   │   ├── orchestrator.py    # Main workflow orchestrator\n│   │   ├── context.py         # Context management\n│   │   └── cache.py           # Intelligent caching\n│   ├── parsers/               # Universal parsing system\n│   │   ├── universal_converter.py\n│   │   ├── python_parser.py\n│   │   ├── go_analyzer.py\n│   │   └── javascript_parser.py\n│   ├── generators/            # Code/docs generation\n│   ├── validators/            # Schema/structure validation\n│   └── integrations/          # IDE integrations\n│       ├── copilot.py\n│       ├── vscode.py\n│       └── cursor.py\n├── tests/                     # Comprehensive testing\n│   ├── unit/integration/fixtures/\n├── docs/                      # Multi-tier documentation\n│   ├── api/user-guide/development/\n├── config/                    # Environment configurations\n├── scripts/                   # Utility scripts\n├── deployment/                # Docker/K8s deployment\n├── examples/                  # Usage examples\n├── requirements/              # Dependency management\n├── workspace/                 # 🆕 LLM session workspace\n│   ├── current_session/       # Active development\n│   ├── experiments/           # LLM experiments\n│   ├── drafts/               # Work-in-progress\n│   └── archive/              # Completed sessions\n└── tmp/                      # Temporary files\n```\n\n**Key Design Principles:**\n- **LLM-First Design**: Every component designed for LLM interaction\n- **Modular JSON**: All configurations and data in modular JSON format\n- **Session Workspace**: Dedicated space for LLM collaboration sessions\n- **struct.json Integration**: Central knowledge base for all tools\n- **Dogfooding Ready**: Self-analysis capabilities built-in",
  
  "🔧 MODULAR ARCHITECTURE PLANNING": "**Comprehensive modular system designed for LLM-human collaboration:**\n\n**1. CLI Modular System:**\n```bash\n# Analysis commands with struct.json integration\nllmstruct analyze <target> --type complexity|dependencies|patterns|performance\nllmstruct review <target> --focus security|performance|maintainability\nllmstruct dogfood  # Self-analysis using internal AI systems\nllmstruct context --show  # Current context management\n\n# Workflow automation\nllmstruct queue process  # Automated command queues\nllmstruct session create|switch|archive  # Session management\n```\n\n**2. API Modular Endpoints:**\n```python\n# Core analysis APIs\nPOST /api/v1/cli/execute     # Execute CLI commands via API\nPOST /api/v1/chat/ollama     # Local Ollama integration\nGET  /api/v1/metrics         # System metrics and analytics\nPOST /api/v1/struct/analyze  # struct.json analysis\nPOST /api/v1/session/create  # Session management\n```\n\n**3. JSON Modular System:**\n- **struct.json**: Core project knowledge (cacheable, searchable)\n- **config/*.json**: Environment-specific configurations\n- **sessions/*.json**: Session state and history\n- **metrics/*.json**: Performance and usage analytics\n- **cache/*.json**: Intelligent caching for faster analysis\n\n**4. LLM Integration Points:**\n- **Context Orchestrator**: Manages LLM context windows efficiently\n- **AI Self-Awareness**: Uses SystemCapabilityDiscovery for real-time analysis\n- **Workflow Sessions**: Dedicated workspace for LLM collaboration\n- **Telegram Integration**: Two-way communication with full logging\n- **Metrics Tracking**: Token usage, cost analysis, efficiency metrics",
  
  "📚 EXISTING ASSETS & CAPABILITIES": "**Rich ecosystem of tools and documentation already available:**\n\n**Core Documentation:**\n- **MODULE_ANALYSIS_SYSTEM_GUIDE.md**: Comprehensive system analysis guide\n- **struct.json**: 1.2MB knowledge base (272 modules analyzed)\n- **Auto-caching system**: Intelligent struct.json caching for performance\n\n**Powerful Scripts Ecosystem:**\n```bash\n# Structure analysis and conversion\nauto_update_struct.py           # Auto-update struct.json\ncreate_comprehensive_index.py   # Architecture indexing\ncreate_tasks_index.py           # Task/dependency indexing\nepic_roadmap_manager.py         # Epic/session management\n\n# Universal conversion system\nsrc/llmstruct/parsers/universal_converter.py  # Multi-language parser\npython_parser.py, go_analyzer.py, javascript_parser.py\n\n# Validation and export\nexport_to_github_projects.py    # GitHub Projects integration\nvalidate_schemas.py, validate_json.py  # Structure validation\n```\n\n**AI Self-Awareness System (272 modules analyzed):**\n- **SystemCapabilityDiscovery**: Real-time capability analysis\n- **1857 functions** searchable through AI system\n- **183 classes** available for intelligent lookup\n- **Smart caching**: Optimized performance for large-scale analysis\n\n**Workflow Capabilities:**\n- **WorkflowOrchestrator**: Automated workflow management\n- **Documentation workflows**: Auto-generation of guides and schemas\n- **Code review workflows**: Automated analysis and validation\n- **Refactoring analysis**: Intelligent code improvement suggestions\n\n**Integration Ecosystem:**\n- **Ollama Integration**: Local models (192.168.88.50:11434)\n- **VS Code/Cursor**: Full IDE integration with Copilot Manager\n- **Telegram Bots**: Multiple bot versions with logging\n- **API Services**: FastAPI with metrics and health checks",
  
  "🎯 PROJECT RESTRUCTURE REQUIREMENTS": "**Critical needs for transforming 'Mad Scientist' chaos into professional structure:**\n\n**Current Problems:**\n- **Root directory chaos**: Useful files mixed with experimental debris\n- **8 different bot versions**: Need consolidation and standardization\n- **No session management**: LLM work sessions create persistent clutter\n- **Missing modular organization**: CLI, API, docs scattered across project\n\n**Solution: Professional Structure with LLM-First Design:**\n\n**1. Session Workspace System:**\n```\nworkspace/\n├── current_session/     # Active LLM collaboration\n│   ├── experiments/     # Current tests and iterations\n│   ├── drafts/         # Work-in-progress files\n│   └── outputs/        # Generated code/docs\n├── archive/            # Completed sessions\n└── templates/          # Session templates\n```\n\n**2. Modular Configuration:**\n- **Environment-specific configs**: dev.toml, prod.toml, test.toml\n- **Modular JSON systems**: Extensible, cacheable, LLM-friendly\n- **Plugin architecture**: Easy extension for new languages/tools\n\n**3. Professional Development Workflow:**\n- **Pre-commit hooks**: Automated validation and formatting\n- **CI/CD integration**: GitHub Actions for testing and deployment\n- **Documentation automation**: Auto-generated from struct.json\n- **Metrics integration**: Track LLM usage, efficiency, and costs\n\n**4. LLM Optimization Features:**\n- **Context-aware file organization**: Optimize for different LLM context sizes\n- **Intelligent caching**: Reduce repeated analysis overhead\n- **Session state management**: Preserve LLM collaboration context\n- **Token budget tracking**: Monitor and optimize LLM usage costs",
  
  "🚀 IMPLEMENTATION STRATEGY": "**Phased approach leveraging existing capabilities:**\n\n**Phase 1: Immediate Cleanup (Local AI + Scripts)**\n- Use existing `auto_update_struct.py` to refresh project analysis\n- Deploy workspace organization using `epic_roadmap_manager.py`\n- Leverage AI Self-Awareness system for duplicate detection\n- Implement session workspace structure\n\n**Phase 2: Strategic Analysis (Budget Allocation)**\n- **Option A**: GPT-4.1 comprehensive analysis ($15-20)\n- **Option B**: Continue Grok series approach ($5-10) + local implementation\n- **Option C**: Hybrid approach with 1M context models for strategy + Ollama for implementation\n\n**Phase 3: Professional Implementation**\n- Modular architecture deployment using existing parsers\n- CLI enhancement with new command structure\n- API standardization with metrics integration\n- Documentation automation leveraging struct.json\n\n**Recommended Approach:**\n1. **Immediate**: Use internal AI systems for initial cleanup and organization\n2. **Strategic**: Single high-quality consultation with 1M context model for architecture decisions\n3. **Implementation**: Local Ollama models for detailed coding and testing\n4. **Validation**: Use existing validation scripts and metrics system\n\n**Budget Optimization:**\n- **Self-analysis phase**: $0 (use internal capabilities)\n- **Strategic consultation**: $8-12 (1M context model)\n- **Implementation support**: $5-8 (targeted consultations)\n- **Reserve**: $3-5 for refinements and validation"
 },
 "ollama_models": {
  "total_count": 173,
  "models": [
   {
    "name": "deepseek-r1",
    "desc": "DeepSeek-R1 first-generation of open reasoning models with comparable performance to OpenAI-o3.",
    "caps": [
     "thinking"
    ],
    "sizes": [
     "1.5b",
     "7b",
     "8b",
     "14b",
     "32b",
     "70b",
     "671b"
    ],
    "pulls": "46.3M",
    "updated": "22 hours ago"
   },
   {
    "name": "gemma3",
    "desc": "The current, most capable model that runs on a single GPU.",
    "caps": [
     "vision"
    ],
    "sizes": [
     "1b",
     "4b",
     "12b",
     "27b"
    ],
    "pulls": "5.4M",
    "updated": "1 month ago"
   },
   {
    "name": "qwen3",
    "desc": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive sui...",
    "caps": [
     "tools",
     "thinking"
    ],
    "sizes": [
     "0.6b",
     "1.7b",
     "4b",
     "8b",
     "14b",
     "30b",
     "32b",
     "235b"
    ],
    "pulls": "1.8M",
    "updated": "yesterday"
   },
   {
    "name": "devstral",
    "desc": "Devstral: the best open source model for coding agents",
    "caps": [
     "tools"
    ],
    "sizes": [
     "24b"
    ],
    "pulls": "86.6K",
    "updated": "1 week ago"
   },
   {
    "name": "llama4",
    "desc": "Meta's latest collection of multimodal models.",
    "caps": [
     "vision",
     "tools"
    ],
    "sizes": [
     "16x17b",
     "128x17b"
    ],
    "pulls": "367K",
    "updated": "2 days ago"
   },
   {
    "name": "qwen2.5vl",
    "desc": "Flagship vision-language model of Qwen and also a significant leap from the previous Qwen2-VL.",
    "caps": [
     "vision"
    ],
    "sizes": [
     "3b",
     "7b",
     "32b",
     "72b"
    ],
    "pulls": "228.2K",
    "updated": "1 week ago"
   },
   {
    "name": "llama3.3",
    "desc": "New state of the art 70B model. Llama 3.3 70B offers similar performance compared to the Llama 3.1 4...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "70b"
    ],
    "pulls": "1.9M",
    "updated": "5 months ago"
   },
   {
    "name": "phi4",
    "desc": "Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.",
    "caps": [],
    "sizes": [
     "14b"
    ],
    "pulls": "2.4M",
    "updated": "4 months ago"
   },
   {
    "name": "llama3.2",
    "desc": "Meta's Llama 3.2 goes small with 1B and 3B models.",
    "caps": [
     "tools"
    ],
    "sizes": [
     "1b",
     "3b"
    ],
    "pulls": "18M",
    "updated": "8 months ago"
   },
   {
    "name": "llama3.1",
    "desc": "Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.",
    "caps": [
     "tools"
    ],
    "sizes": [
     "8b",
     "70b",
     "405b"
    ],
    "pulls": "93.9M",
    "updated": "6 months ago"
   },
   {
    "name": "nomic-embed-text",
    "desc": "A high-performing open embedding model with a large token context window.",
    "caps": [
     "embedding"
    ],
    "sizes": [],
    "pulls": "27M",
    "updated": "1 year ago"
   },
   {
    "name": "mistral",
    "desc": "The 7B model released by Mistral AI, updated to version 0.3.",
    "caps": [
     "tools"
    ],
    "sizes": [
     "7b"
    ],
    "pulls": "14.4M",
    "updated": "10 months ago"
   },
   {
    "name": "qwen2.5",
    "desc": "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillio...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "0.5b",
     "1.5b",
     "3b",
     "7b",
     "14b",
     "32b",
     "72b"
    ],
    "pulls": "9.1M",
    "updated": "8 months ago"
   },
   {
    "name": "llama3",
    "desc": "Meta Llama 3: The most capable openly available LLM to date",
    "caps": [],
    "sizes": [
     "8b",
     "70b"
    ],
    "pulls": "8.2M",
    "updated": "1 year ago"
   },
   {
    "name": "llava",
    "desc": "🌋 LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicu...",
    "caps": [
     "vision"
    ],
    "sizes": [
     "7b",
     "13b",
     "34b"
    ],
    "pulls": "6M",
    "updated": "1 year ago"
   },
   {
    "name": "qwen2.5-coder",
    "desc": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, co...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "0.5b",
     "1.5b",
     "3b",
     "7b",
     "14b",
     "32b"
    ],
    "pulls": "5.3M",
    "updated": "2 days ago"
   },
   {
    "name": "gemma2",
    "desc": "Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.",
    "caps": [],
    "sizes": [
     "2b",
     "9b",
     "27b"
    ],
    "pulls": "4.9M",
    "updated": "10 months ago"
   },
   {
    "name": "qwen",
    "desc": "Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters",
    "caps": [],
    "sizes": [
     "0.5b",
     "1.8b",
     "4b",
     "7b",
     "14b",
     "32b",
     "72b",
     "110b"
    ],
    "pulls": "4.7M",
    "updated": "1 year ago"
   },
   {
    "name": "gemma",
    "desc": "Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to ...",
    "caps": [],
    "sizes": [
     "2b",
     "7b"
    ],
    "pulls": "4.6M",
    "updated": "1 year ago"
   },
   {
    "name": "qwen2",
    "desc": "Qwen2 is a new series of large language models from Alibaba group",
    "caps": [
     "tools"
    ],
    "sizes": [
     "0.5b",
     "1.5b",
     "7b",
     "72b"
    ],
    "pulls": "4.2M",
    "updated": "8 months ago"
   },
   {
    "name": "llama2",
    "desc": "Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.",
    "caps": [],
    "sizes": [
     "7b",
     "13b",
     "70b"
    ],
    "pulls": "3.5M",
    "updated": "1 year ago"
   },
   {
    "name": "mxbai-embed-large",
    "desc": "State-of-the-art large embedding model from mixedbread.ai",
    "caps": [
     "embedding"
    ],
    "sizes": [
     "335m"
    ],
    "pulls": "3.4M",
    "updated": "1 year ago"
   },
   {
    "name": "phi3",
    "desc": "Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsof...",
    "caps": [],
    "sizes": [
     "3.8b",
     "14b"
    ],
    "pulls": "3.3M",
    "updated": "10 months ago"
   },
   {
    "name": "llama3.2-vision",
    "desc": "Llama 3.2 Vision is a collection of instruction-tuned image reasoning generative models in 11B and 9...",
    "caps": [
     "vision"
    ],
    "sizes": [
     "11b",
     "90b"
    ],
    "pulls": "2.1M",
    "updated": "1 week ago"
   },
   {
    "name": "codellama",
    "desc": "A large language model that can use text prompts to generate and discuss code.",
    "caps": [],
    "sizes": [
     "7b",
     "13b",
     "34b",
     "70b"
    ],
    "pulls": "2.1M",
    "updated": "10 months ago"
   },
   {
    "name": "tinyllama",
    "desc": "The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.",
    "caps": [],
    "sizes": [
     "1.1b"
    ],
    "pulls": "1.9M",
    "updated": "1 year ago"
   },
   {
    "name": "mistral-nemo",
    "desc": "A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVI...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "12b"
    ],
    "pulls": "1.8M",
    "updated": "9 months ago"
   },
   {
    "name": "minicpm-v",
    "desc": "A series of multimodal LLMs (MLLMs) designed for vision-language understanding.",
    "caps": [
     "vision"
    ],
    "sizes": [
     "8b"
    ],
    "pulls": "1.6M",
    "updated": "6 months ago"
   },
   {
    "name": "qwq",
    "desc": "QwQ is the reasoning model of the Qwen series.",
    "caps": [
     "tools"
    ],
    "sizes": [
     "32b"
    ],
    "pulls": "1.5M",
    "updated": "2 months ago"
   },
   {
    "name": "deepseek-v3",
    "desc": "A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for e...",
    "caps": [],
    "sizes": [
     "671b"
    ],
    "pulls": "1.4M",
    "updated": "4 months ago"
   },
   {
    "name": "dolphin3",
    "desc": "Dolphin 3.0 Llama 3.1 8B 🐬 is the next generation of the Dolphin series of instruct-tuned models des...",
    "caps": [],
    "sizes": [
     "8b"
    ],
    "pulls": "1.3M",
    "updated": "4 months ago"
   },
   {
    "name": "olmo2",
    "desc": "OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with...",
    "caps": [],
    "sizes": [
     "7b",
     "13b"
    ],
    "pulls": "1.2M",
    "updated": "4 months ago"
   },
   {
    "name": "bge-m3",
    "desc": "BGE-M3 is a new model from BAAI distinguished for its versatility in Multi-Functionality, Multi-Ling...",
    "caps": [
     "embedding"
    ],
    "sizes": [
     "567m"
    ],
    "pulls": "1.2M",
    "updated": "9 months ago"
   },
   {
    "name": "llama2-uncensored",
    "desc": "Uncensored Llama 2 model by George Sung and Jarrad Hope.",
    "caps": [],
    "sizes": [
     "7b",
     "70b"
    ],
    "pulls": "1.2M",
    "updated": "1 year ago"
   },
   {
    "name": "mixtral",
    "desc": "A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter ...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "8x7b",
     "8x22b"
    ],
    "pulls": "1M",
    "updated": "5 months ago"
   },
   {
    "name": "starcoder2",
    "desc": "StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes:...",
    "caps": [],
    "sizes": [
     "3b",
     "7b",
     "15b"
    ],
    "pulls": "959.8K",
    "updated": "8 months ago"
   },
   {
    "name": "llava-llama3",
    "desc": "A LLaVA model fine-tuned from Llama 3 Instruct with better scores in several benchmarks.",
    "caps": [
     "vision"
    ],
    "sizes": [
     "8b"
    ],
    "pulls": "902.8K",
    "updated": "1 year ago"
   },
   {
    "name": "mistral-small",
    "desc": "Mistral Small 3 sets a new benchmark in the 'small' Large Language Models category below 70B.",
    "caps": [
     "tools"
    ],
    "sizes": [
     "22b",
     "24b"
    ],
    "pulls": "856.2K",
    "updated": "4 months ago"
   },
   {
    "name": "smollm2",
    "desc": "SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B paramet...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "135m",
     "360m",
     "1.7b"
    ],
    "pulls": "837.6K",
    "updated": "7 months ago"
   },
   {
    "name": "deepseek-coder-v2",
    "desc": "An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-T...",
    "caps": [],
    "sizes": [
     "16b",
     "236b"
    ],
    "pulls": "836.7K",
    "updated": "8 months ago"
   },
   {
    "name": "snowflake-arctic-embed",
    "desc": "A suite of text embedding models by Snowflake, optimized for performance.",
    "caps": [
     "embedding"
    ],
    "sizes": [
     "22m",
     "33m",
     "110m",
     "137m",
     "335m"
    ],
    "pulls": "738.1K",
    "updated": "1 year ago"
   },
   {
    "name": "deepseek-coder",
    "desc": "DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.",
    "caps": [],
    "sizes": [
     "1.3b",
     "6.7b",
     "33b"
    ],
    "pulls": "737.9K",
    "updated": "1 year ago"
   },
   {
    "name": "codegemma",
    "desc": "CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks...",
    "caps": [],
    "sizes": [
     "2b",
     "7b"
    ],
    "pulls": "614.2K",
    "updated": "10 months ago"
   },
   {
    "name": "dolphin-mixtral",
    "desc": "Uncensored, 8x7b and 8x22b fine-tuned models based on the Mixtral mixture of experts models that exc...",
    "caps": [],
    "sizes": [
     "8x7b",
     "8x22b"
    ],
    "pulls": "576.5K",
    "updated": "5 months ago"
   },
   {
    "name": "phi",
    "desc": "Phi-2: a 2.7B language model by Microsoft Research that demonstrates outstanding reasoning and langu...",
    "caps": [],
    "sizes": [
     "2.7b"
    ],
    "pulls": "544K",
    "updated": "1 year ago"
   },
   {
    "name": "Embedding models on very large sentence level datasets.",
    "desc": "",
    "caps": [
     "embedding"
    ],
    "sizes": [
     "22m",
     "33m"
    ],
    "pulls": "529.1K",
    "updated": "1 year ago"
   },
   {
    "name": "openthinker",
    "desc": "A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-...",
    "caps": [],
    "sizes": [
     "7b",
     "32b"
    ],
    "pulls": "527.7K",
    "updated": "1 month ago"
   },
   {
    "name": "wizardlm2",
    "desc": "State of the art large language model from Microsoft AI with improved performance on complex chat, m...",
    "caps": [],
    "sizes": [
     "7b",
     "8x22b"
    ],
    "pulls": "368.9K",
    "updated": "1 year ago"
   },
   {
    "name": "dolphin-mistral",
    "desc": "The uncensored Dolphin model based on Mistral that excels at coding tasks. Updated to version 2.8.",
    "caps": [],
    "sizes": [
     "7b"
    ],
    "pulls": "348.7K",
    "updated": "1 year ago"
   },
   {
    "name": "orca-mini",
    "desc": "A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level ha...",
    "caps": [],
    "sizes": [
     "3b",
     "7b",
     "13b",
     "70b"
    ],
    "pulls": "341.6K",
    "updated": "1 year ago"
   },
   {
    "name": "dolphin-llama3",
    "desc": "Dolphin 2.9 is a new model with 8B and 70B sizes by Eric Hartford based on Llama 3 that has a variet...",
    "caps": [],
    "sizes": [
     "8b",
     "70b"
    ],
    "pulls": "325.1K",
    "updated": "1 year ago"
   },
   {
    "name": "command-r",
    "desc": "Command R is a Large Language Model optimized for conversational interaction and long context tasks.",
    "caps": [
     "tools"
    ],
    "sizes": [
     "35b"
    ],
    "pulls": "296.1K",
    "updated": "9 months ago"
   },
   {
    "name": "codestral",
    "desc": "Codestral is Mistral AI's first-ever code model designed for code generation tasks.",
    "caps": [],
    "sizes": [
     "22b"
    ],
    "pulls": "294.6K",
    "updated": "8 months ago"
   },
   {
    "name": "hermes3",
    "desc": "Hermes 3 is the latest version of the flagship Hermes series of LLMs by Nous Research",
    "caps": [
     "tools"
    ],
    "sizes": [
     "3b",
     "8b",
     "70b",
     "405b"
    ],
    "pulls": "285K",
    "updated": "5 months ago"
   },
   {
    "name": "phi3.5",
    "desc": "A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger ...",
    "caps": [],
    "sizes": [
     "3.8b"
    ],
    "pulls": "280.2K",
    "updated": "8 months ago"
   },
   {
    "name": "Yi 1.5 is a high-performing, bilingual language model.",
    "desc": "",
    "caps": [],
    "sizes": [
     "6b",
     "9b",
     "34b"
    ],
    "pulls": "279.7K",
    "updated": "1 year ago"
   },
   {
    "name": "🪐 A family of small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset.",
    "desc": "",
    "caps": [],
    "sizes": [
     "135m",
     "360m",
     "1.7b"
    ],
    "pulls": "269.9K",
    "updated": "9 months ago"
   },
   {
    "name": "zephyr",
    "desc": "Zephyr is a series of fine-tuned versions of the Mistral and Mixtral models that are trained to act ...",
    "caps": [],
    "sizes": [
     "7b",
     "141b"
    ],
    "pulls": "267.9K",
    "updated": "1 year ago"
   },
   {
    "name": "granite-code",
    "desc": "A family of open foundation models by IBM for Code Intelligence",
    "caps": [],
    "sizes": [
     "3b",
     "8b",
     "20b",
     "34b"
    ],
    "pulls": "211K",
    "updated": "8 months ago"
   },
   {
    "name": "wizard-vicuna-uncensored",
    "desc": "Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric H...",
    "caps": [],
    "sizes": [
     "7b",
     "13b",
     "30b"
    ],
    "pulls": "204.1K",
    "updated": "1 year ago"
   },
   {
    "name": "starcoder",
    "desc": "StarCoder is a code generation model trained on 80+ programming languages.",
    "caps": [],
    "sizes": [
     "1b",
     "3b",
     "7b",
     "15b"
    ],
    "pulls": "198.9K",
    "updated": "1 year ago"
   },
   {
    "name": "moondream2 is a small vision language model designed to run efficiently on edge devices.",
    "desc": "",
    "caps": [
     "vision"
    ],
    "sizes": [
     "1.8b"
    ],
    "pulls": "195.7K",
    "updated": "1 year ago"
   },
   {
    "name": "vicuna",
    "desc": "General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.",
    "caps": [],
    "sizes": [
     "7b",
     "13b",
     "33b"
    ],
    "pulls": "182.6K",
    "updated": "1 year ago"
   },
   {
    "name": "mistral-openorca",
    "desc": "Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the...",
    "caps": [],
    "sizes": [
     "7b"
    ],
    "pulls": "171K",
    "updated": "1 year ago"
   },
   {
    "name": "openchat",
    "desc": "A family of open-source models trained on a wide variety of data, surpassing ChatGPT on various benc...",
    "caps": [],
    "sizes": [
     "7b"
    ],
    "pulls": "164.5K",
    "updated": "1 year ago"
   },
   {
    "name": "phi4-mini",
    "desc": "Phi-4-mini brings significant enhancements in multilingual support, reasoning, and mathematics, and ...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "3.8b"
    ],
    "pulls": "160.8K",
    "updated": "3 months ago"
   },
   {
    "name": "deepseek-v2",
    "desc": "A strong, economical, and efficient Mixture-of-Experts language model.",
    "caps": [],
    "sizes": [
     "16b",
     "236b"
    ],
    "pulls": "156.6K",
    "updated": "11 months ago"
   },
   {
    "name": "llama2-chinese",
    "desc": "Llama 2 based model fine tuned to improve Chinese dialogue ability.",
    "caps": [],
    "sizes": [
     "7b",
     "13b"
    ],
    "pulls": "153.7K",
    "updated": "1 year ago"
   },
   {
    "name": "openhermes",
    "desc": "OpenHermes 2.5 is a 7B model fine-tuned by Teknium on Mistral with fully open datasets.",
    "caps": [],
    "sizes": [],
    "pulls": "151K",
    "updated": "1 year ago"
   },
   {
    "name": "An advanced language model crafted with 2 trillion bilingual tokens.",
    "desc": "",
    "caps": [],
    "sizes": [
     "7b",
     "67b"
    ],
    "pulls": "150.6K",
    "updated": "1 year ago"
   },
   {
    "name": "codeqwen",
    "desc": "CodeQwen1.5 is a large language model pretrained on a large amount of code data.",
    "caps": [],
    "sizes": [
     "7b"
    ],
    "pulls": "146.6K",
    "updated": "11 months ago"
   },
   {
    "name": "codegeex4",
    "desc": "A versatile model for AI software development scenarios, including code completion.",
    "caps": [],
    "sizes": [
     "9b"
    ],
    "pulls": "145.1K",
    "updated": "10 months ago"
   },
   {
    "name": "aya",
    "desc": "Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23...",
    "caps": [],
    "sizes": [
     "8b",
     "35b"
    ],
    "pulls": "144.2K",
    "updated": "1 year ago"
   },
   {
    "name": "mistral-large",
    "desc": "Mistral Large 2 is Mistral's new flagship model that is significantly more capable in code generatio...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "123b"
    ],
    "pulls": "136.7K",
    "updated": "6 months ago"
   },
   {
    "name": "glm4",
    "desc": "A strong multi-lingual general language model with competitive performance to Llama 3.",
    "caps": [],
    "sizes": [
     "9b"
    ],
    "pulls": "132.9K",
    "updated": "10 months ago"
   },
   {
    "name": "stable-code",
    "desc": "Stable Code 3B is a coding model with instruct and code completion variants on par with models such ...",
    "caps": [],
    "sizes": [
     "3b"
    ],
    "pulls": "132.1K",
    "updated": "1 year ago"
   },
   {
    "name": "tinydolphin",
    "desc": "An experimental 1.1B parameter model trained on the new Dolphin 2.8 dataset by Eric Hartford and bas...",
    "caps": [],
    "sizes": [
     "1.1b"
    ],
    "pulls": "129.3K",
    "updated": "1 year ago"
   },
   {
    "name": "deepcoder",
    "desc": "DeepCoder is a fully open-Source 14B coder model at O3-mini level, with a 1.5B version also availabl...",
    "caps": [],
    "sizes": [
     "1.5b",
     "14b"
    ],
    "pulls": "128.8K",
    "updated": "1 month ago"
   },
   {
    "name": "nous-hermes2",
    "desc": "The powerful family of models by Nous Research that excels at scientific discussion and coding tasks...",
    "caps": [],
    "sizes": [
     "10.7b",
     "34b"
    ],
    "pulls": "128.6K",
    "updated": "1 year ago"
   },
   {
    "name": "qwen2-math",
    "desc": "Qwen2 Math is a series of specialized math language models built upon the Qwen2 LLMs, which signific...",
    "caps": [],
    "sizes": [
     "1.5b",
     "7b",
     "72b"
    ],
    "pulls": "128.6K",
    "updated": "9 months ago"
   },
   {
    "name": "command-r-plus",
    "desc": "Command R+ is a powerful, scalable large language model purpose-built to excel at real-world enterpr...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "104b"
    ],
    "pulls": "126.8K",
    "updated": "9 months ago"
   },
   {
    "name": "wizardcoder",
    "desc": "State-of-the-art code generation model",
    "caps": [],
    "sizes": [
     "33b"
    ],
    "pulls": "125.2K",
    "updated": "1 year ago"
   },
   {
    "name": "bakllava",
    "desc": "BakLLaVA is a multimodal model consisting of the Mistral 7B base model augmented with the LLaVA arch...",
    "caps": [
     "vision"
    ],
    "sizes": [
     "7b"
    ],
    "pulls": "118.5K",
    "updated": "1 year ago"
   },
   {
    "name": "mistral-small3.1",
    "desc": "Building upon Mistral Small 3, Mistral Small 3.1 (2503) adds state-of-the-art vision understanding a...",
    "caps": [
     "vision",
     "tools"
    ],
    "sizes": [
     "24b"
    ],
    "pulls": "116.3K",
    "updated": "1 month ago"
   },
   {
    "name": "stablelm2",
    "desc": "Stable LM 2 is a state-of-the-art 1.6B and 12B parameter language model trained on multilingual data...",
    "caps": [],
    "sizes": [
     "1.6b",
     "12b"
    ],
    "pulls": "113.8K",
    "updated": "1 year ago"
   },
   {
    "name": "neural-chat",
    "desc": "A fine-tuned model based on Mistral with good coverage of domain and language.",
    "caps": [],
    "sizes": [
     "7b"
    ],
    "pulls": "113.4K",
    "updated": "1 year ago"
   },
   {
    "name": "sqlcoder",
    "desc": "SQLCoder is a code completion model fined-tuned on StarCoder for SQL generation tasks",
    "caps": [],
    "sizes": [
     "7b",
     "15b"
    ],
    "pulls": "106K",
    "updated": "1 year ago"
   },
   {
    "name": "llama3-chatqa",
    "desc": "A model from NVIDIA based on Llama 3 that excels at conversational question answering (QA) and retri...",
    "caps": [],
    "sizes": [
     "8b",
     "70b"
    ],
    "pulls": "105.7K",
    "updated": "1 year ago"
   },
   {
    "name": "reflection",
    "desc": "A high-performing model trained with a new technique called Reflection-tuning that teaches a LLM to ...",
    "caps": [],
    "sizes": [
     "70b"
    ],
    "pulls": "105.7K",
    "updated": "8 months ago"
   },
   {
    "name": "wizard-math",
    "desc": "Model focused on math and logic problems",
    "caps": [],
    "sizes": [
     "7b",
     "13b",
     "70b"
    ],
    "pulls": "104.7K",
    "updated": "1 year ago"
   },
   {
    "name": "bge-large",
    "desc": "Embedding model from BAAI mapping texts to vectors.",
    "caps": [
     "embedding"
    ],
    "sizes": [
     "335m"
    ],
    "pulls": "104.5K",
    "updated": "9 months ago"
   },
   {
    "name": "granite3.2",
    "desc": "Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for thinking capabilit...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "2b",
     "8b"
    ],
    "pulls": "104K",
    "updated": "3 months ago"
   },
   {
    "name": "llama3-gradient",
    "desc": "This model extends LLama-3 8B's context length from 8k to over 1m tokens.",
    "caps": [],
    "sizes": [
     "8b",
     "70b"
    ],
    "pulls": "102K",
    "updated": "1 year ago"
   },
   {
    "name": "granite3-dense",
    "desc": "The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrie...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "2b",
     "8b"
    ],
    "pulls": "98.7K",
    "updated": "6 months ago"
   },
   {
    "name": "granite3.1-dense",
    "desc": "The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "2b",
     "8b"
    ],
    "pulls": "96.2K",
    "updated": "4 months ago"
   },
   {
    "name": "samantha-mistral",
    "desc": "A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistra...",
    "caps": [],
    "sizes": [
     "7b"
    ],
    "pulls": "94.1K",
    "updated": "1 year ago"
   },
   {
    "name": "cogito",
    "desc": "Cogito v1 Preview is a family of hybrid reasoning models by Deep Cogito that outperform the best ava...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "3b",
     "8b",
     "14b",
     "32b",
     "70b"
    ],
    "pulls": "93.7K",
    "updated": "1 month ago"
   },
   {
    "name": "llava-phi3",
    "desc": "A new small LLaVA model fine-tuned from Phi 3 Mini.",
    "caps": [
     "vision"
    ],
    "sizes": [
     "3.8b"
    ],
    "pulls": "92.2K",
    "updated": "1 year ago"
   },
   {
    "name": "dolphincoder",
    "desc": "A 7B and 15B uncensored variant of the Dolphin model family that excels at coding, based on StarCode...",
    "caps": [],
    "sizes": [
     "7b",
     "15b"
    ],
    "pulls": "89.5K",
    "updated": "1 year ago"
   },
   {
    "name": "nous-hermes",
    "desc": "General use models based on Llama and Llama 2 from Nous Research.",
    "caps": [],
    "sizes": [
     "7b",
     "13b"
    ],
    "pulls": "87.4K",
    "updated": "1 year ago"
   },
   {
    "name": "Conversational model based on Llama 2 that performs competitively on various benchmarks.",
    "desc": "",
    "caps": [],
    "sizes": [
     "7b",
     "13b"
    ],
    "pulls": "86.5K",
    "updated": "1 year ago"
   },
   {
    "name": "Starling is a large language model trained by reinforcement learning from AI feedback focused on improving chatbot helpfulness.",
    "desc": "",
    "caps": [],
    "sizes": [
     "7b"
    ],
    "pulls": "85.7K",
    "updated": "1 year ago"
   },
   {
    "name": "phind-codellama",
    "desc": "Code generation model based on Code Llama.",
    "caps": [],
    "sizes": [
     "34b"
    ],
    "pulls": "85.1K",
    "updated": "1 year ago"
   },
   {
    "name": "granite3.2-vision",
    "desc": "A compact and efficient vision-language model, specifically designed for visual document understandi...",
    "caps": [
     "vision",
     "tools"
    ],
    "sizes": [
     "2b"
    ],
    "pulls": "85K",
    "updated": "3 months ago"
   },
   {
    "name": "snowflake-arctic-embed2",
    "desc": "Snowflake's frontier embedding model. Arctic Embed 2.0 adds multilingual support without sacrificing...",
    "caps": [
     "embedding"
    ],
    "sizes": [
     "568m"
    ],
    "pulls": "84.7K",
    "updated": "5 months ago"
   },
   {
    "name": "yi-coder",
    "desc": "Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding perfo...",
    "caps": [],
    "sizes": [
     "1.5b",
     "9b"
    ],
    "pulls": "83.5K",
    "updated": "8 months ago"
   },
   {
    "name": "nemotron-mini",
    "desc": "A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function ca...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "4b"
    ],
    "pulls": "82.4K",
    "updated": "8 months ago"
   },
   {
    "name": "solar",
    "desc": "A compact, yet powerful 10.7B large language model designed for single-turn conversation.",
    "caps": [],
    "sizes": [
     "10.7b"
    ],
    "pulls": "82.3K",
    "updated": "1 year ago"
   },
   {
    "name": "athene-v2",
    "desc": "Athene-V2 is a 72B parameter model which excels at code completion, mathematics, and log extraction ...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "72b"
    ],
    "pulls": "82.1K",
    "updated": "6 months ago"
   },
   {
    "name": "yarn-llama2",
    "desc": "An extension of Llama 2 that supports a context of up to 128k tokens.",
    "caps": [],
    "sizes": [
     "7b",
     "13b"
    ],
    "pulls": "81K",
    "updated": "1 year ago"
   },
   {
    "name": "deepscaler",
    "desc": "A fine-tuned version of Deepseek-R1-Distilled-Qwen-1.5B that surpasses the performance of OpenAI's o...",
    "caps": [],
    "sizes": [
     "1.5b"
    ],
    "pulls": "80.9K",
    "updated": "3 months ago"
   },
   {
    "name": "internlm2",
    "desc": "InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding reasoning capa...",
    "caps": [],
    "sizes": [
     "1m",
     "1.8b",
     "7b",
     "20b"
    ],
    "pulls": "80.3K",
    "updated": "9 months ago"
   },
   {
    "name": "granite3.3",
    "desc": "IBM Granite 2B and 8B models are 128K context length language models that have been fine-tuned for i...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "2b",
     "8b"
    ],
    "pulls": "77.4K",
    "updated": "1 month ago"
   },
   {
    "name": "General use model based on Llama 2.",
    "desc": "",
    "caps": [],
    "sizes": [],
    "pulls": "77.1K",
    "updated": "1 year ago"
   },
   {
    "name": "exaone3.5",
    "desc": "EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ran...",
    "caps": [],
    "sizes": [
     "2.4b",
     "7.8b",
     "32b"
    ],
    "pulls": "75.3K",
    "updated": "5 months ago"
   },
   {
    "name": "dolphin-phi",
    "desc": "2.7B uncensored Dolphin model by Eric Hartford, based on the Phi language model by Microsoft Researc...",
    "caps": [],
    "sizes": [
     "2.7b"
    ],
    "pulls": "74.9K",
    "updated": "1 year ago"
   },
   {
    "name": "falcon",
    "desc": "A large language model built by the Technology Innovation Institute (TII) for use in summarization, ...",
    "caps": [],
    "sizes": [
     "7b",
     "40b",
     "180b"
    ],
    "pulls": "74.8K",
    "updated": "1 year ago"
   },
   {
    "name": "nemotron",
    "desc": "Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfu...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "70b"
    ],
    "pulls": "73.3K",
    "updated": "7 months ago"
   },
   {
    "name": "llama3-groq-tool-use",
    "desc": "A series of models from Groq that represent a significant advancement in open-source AI capabilities...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "8b",
     "70b"
    ],
    "pulls": "67.4K",
    "updated": "10 months ago"
   },
   {
    "name": "orca2",
    "desc": "Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models. The mo...",
    "caps": [],
    "sizes": [
     "7b",
     "13b"
    ],
    "pulls": "66.6K",
    "updated": "1 year ago"
   },
   {
    "name": "wizardlm-uncensored",
    "desc": "Uncensored version of Wizard LM model",
    "caps": [],
    "sizes": [
     "13b"
    ],
    "pulls": "66.4K",
    "updated": "1 year ago"
   },
   {
    "name": "aya-expanse",
    "desc": "Cohere For AI's language models trained to perform well across 23 different languages.",
    "caps": [
     "tools"
    ],
    "sizes": [
     "8b",
     "32b"
    ],
    "pulls": "63.5K",
    "updated": "7 months ago"
   },
   {
    "name": "paraphrase-multilingual",
    "desc": "Sentence-transformers model that can be used for tasks like clustering or semantic search.",
    "caps": [
     "embedding"
    ],
    "sizes": [
     "278m"
    ],
    "pulls": "61.6K",
    "updated": "9 months ago"
   },
   {
    "name": "stable-beluga",
    "desc": "Llama 2 based model fine tuned on an Orca-style dataset. Originally called Free Willy.",
    "caps": [],
    "sizes": [
     "7b",
     "13b",
     "70b"
    ],
    "pulls": "60.7K",
    "updated": "1 year ago"
   },
   {
    "name": "phi4-reasoning",
    "desc": "Phi 4 reasoning and reasoning plus are 14-billion parameter open-weight reasoning models that rival ...",
    "caps": [],
    "sizes": [
     "14b"
    ],
    "pulls": "60.1K",
    "updated": "4 weeks ago"
   },
   {
    "name": "nous-hermes2-mixtral",
    "desc": "The Nous Hermes 2 model from Nous Research, now trained over Mixtral.",
    "caps": [],
    "sizes": [
     "8x7b"
    ],
    "pulls": "59.6K",
    "updated": "5 months ago"
   },
   {
    "name": "smallthinker",
    "desc": "A new small reasoning model fine-tuned from the Qwen 2.5 3B Instruct model.",
    "caps": [],
    "sizes": [
     "3b"
    ],
    "pulls": "58.7K",
    "updated": "5 months ago"
   },
   {
    "name": "falcon3",
    "desc": "A family of efficient AI models under 10B parameters performant in science, math, and coding through...",
    "caps": [],
    "sizes": [
     "1b",
     "3b",
     "7b",
     "10b"
    ],
    "pulls": "54.7K",
    "updated": "5 months ago"
   },
   {
    "name": "meditron",
    "desc": "Open-source medical large language model adapted from Llama 2 to the medical domain.",
    "caps": [],
    "sizes": [
     "7b",
     "70b"
    ],
    "pulls": "54.6K",
    "updated": "1 year ago"
   },
   {
    "name": "deepseek-v2.5",
    "desc": "An upgraded version of DeekSeek-V2 that integrates the general and coding abilities of both DeepSeek...",
    "caps": [],
    "sizes": [
     "236b"
    ],
    "pulls": "54K",
    "updated": "8 months ago"
   },
   {
    "name": "medllama2",
    "desc": "Fine-tuned Llama 2 model to answer medical questions based on an open source medical dataset.",
    "caps": [],
    "sizes": [
     "7b"
    ],
    "pulls": "52.6K",
    "updated": "1 year ago"
   },
   {
    "name": "granite-embedding",
    "desc": "The IBM Granite Embedding 30M and 278M models models are text-only dense biencoder embedding models,...",
    "caps": [
     "embedding"
    ],
    "sizes": [
     "30m",
     "278m"
    ],
    "pulls": "51.2K",
    "updated": "5 months ago"
   },
   {
    "name": "granite3-moe",
    "desc": "The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM desi...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "1b",
     "3b"
    ],
    "pulls": "50.1K",
    "updated": "6 months ago"
   },
   {
    "name": "llama-pro",
    "desc": "An expansion of Llama 2 that specializes in integrating both general language understanding and doma...",
    "caps": [],
    "sizes": [],
    "pulls": "47.7K",
    "updated": "1 year ago"
   },
   {
    "name": "opencoder",
    "desc": "OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting ...",
    "caps": [],
    "sizes": [
     "1.5b",
     "8b"
    ],
    "pulls": "46.7K",
    "updated": "6 months ago"
   },
   {
    "name": "yarn-mistral",
    "desc": "An extension of Mistral to support context windows of 64K or 128K.",
    "caps": [],
    "sizes": [
     "7b"
    ],
    "pulls": "46.6K",
    "updated": "1 year ago"
   },
   {
    "name": "granite3.1-moe",
    "desc": "The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM d...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "1b",
     "3b"
    ],
    "pulls": "44.6K",
    "updated": "4 months ago"
   },
   {
    "name": "exaone-deep",
    "desc": "EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benc...",
    "caps": [],
    "sizes": [
     "2.4b",
     "7.8b",
     "32b"
    ],
    "pulls": "44.5K",
    "updated": "2 months ago"
   },
   {
    "name": "nexusraven",
    "desc": "Nexus Raven is a 13B instruction tuned model for function calling tasks.",
    "caps": [],
    "sizes": [
     "13b"
    ],
    "pulls": "43.3K",
    "updated": "1 year ago"
   },
   {
    "name": "shieldgemma",
    "desc": "ShieldGemma is set of instruction tuned models for evaluating the safety of text prompt input and te...",
    "caps": [],
    "sizes": [
     "2b",
     "9b",
     "27b"
    ],
    "pulls": "41.5K",
    "updated": "7 months ago"
   },
   {
    "name": "codeup",
    "desc": "Great code generation model based on Llama2.",
    "caps": [],
    "sizes": [
     "13b"
    ],
    "pulls": "41.4K",
    "updated": "1 year ago"
   },
   {
    "name": "Uncensored Llama2 based model with support for a 16K context window.",
    "desc": "",
    "caps": [],
    "sizes": [
     "13b"
    ],
    "pulls": "41.1K",
    "updated": "1 year ago"
   },
   {
    "name": "llama-guard3",
    "desc": "Llama Guard 3 is a series of models fine-tuned for content safety classification of LLM inputs and r...",
    "caps": [],
    "sizes": [
     "1b",
     "8b"
    ],
    "pulls": "40.5K",
    "updated": "7 months ago"
   },
   {
    "name": "A series of models that convert HTML content to Markdown content, which is useful for content conversion tasks.",
    "desc": "",
    "caps": [],
    "sizes": [
     "0.5b",
     "1.5b"
    ],
    "pulls": "39.9K",
    "updated": "8 months ago"
   },
   {
    "name": "stablelm-zephyr",
    "desc": "A lightweight chat model allowing accurate, and responsive output without requiring high-end hardwar...",
    "caps": [],
    "sizes": [
     "3b"
    ],
    "pulls": "39.2K",
    "updated": "1 year ago"
   },
   {
    "name": "mathstral",
    "desc": "MathΣtral: a 7B model designed for math reasoning and scientific discovery by Mistral AI.",
    "caps": [],
    "sizes": [
     "7b"
    ],
    "pulls": "37.2K",
    "updated": "10 months ago"
   },
   {
    "name": "solar-pro",
    "desc": "Solar Pro Preview: an advanced large language model (LLM) with 22 billion parameters designed to fit...",
    "caps": [],
    "sizes": [
     "22b"
    ],
    "pulls": "36.7K",
    "updated": "8 months ago"
   },
   {
    "name": "r1-1776",
    "desc": "A version of the DeepSeek-R1 model that has been post trained to provide unbiased, accurate, and fac...",
    "caps": [],
    "sizes": [
     "70b",
     "671b"
    ],
    "pulls": "36.5K",
    "updated": "3 months ago"
   },
   {
    "name": "marco-o1",
    "desc": "An open large reasoning model for real-world solutions by the Alibaba International Digital Commerce...",
    "caps": [],
    "sizes": [
     "7b"
    ],
    "pulls": "36.4K",
    "updated": "5 months ago"
   },
   {
    "name": "duckdb-nsql",
    "desc": "7B parameter text-to-SQL model made by MotherDuck and Numbers Station.",
    "caps": [],
    "sizes": [
     "7b"
    ],
    "pulls": "35K",
    "updated": "1 year ago"
   },
   {
    "name": "falcon2",
    "desc": "Falcon2 is an 11B parameters causal decoder-only model built by TII and trained over 5T tokens.",
    "caps": [],
    "sizes": [
     "11b"
    ],
    "pulls": "34.9K",
    "updated": "1 year ago"
   },
   {
    "name": "The smallest model in Cohere's R series delivers top-tier speed, efficiency, and quality to build powerful AI applications on commodity GPUs and edge devices.",
    "desc": "",
    "caps": [
     "tools"
    ],
    "sizes": [
     "7b"
    ],
    "pulls": "34.9K",
    "updated": "4 months ago"
   },
   {
    "name": "magicoder",
    "desc": "🎩 Magicoder is a family of 7B parameter models trained on 75K synthetic instruction data using OSS-I...",
    "caps": [],
    "sizes": [
     "7b"
    ],
    "pulls": "33.8K",
    "updated": "1 year ago"
   },
   {
    "name": "mistrallite",
    "desc": "MistralLite is a fine-tuned model based on Mistral with enhanced capabilities of processing long con...",
    "caps": [],
    "sizes": [
     "7b"
    ],
    "pulls": "33.4K",
    "updated": "1 year ago"
   },
   {
    "name": "codebooga",
    "desc": "A high-performing code instruct model created by merging two existing code models.",
    "caps": [],
    "sizes": [
     "34b"
    ],
    "pulls": "33K",
    "updated": "1 year ago"
   },
   {
    "name": "wizard-vicuna",
    "desc": "Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj.",
    "caps": [],
    "sizes": [
     "13b"
    ],
    "pulls": "30.9K",
    "updated": "1 year ago"
   },
   {
    "name": "nuextract",
    "desc": "A 3.8B model fine-tuned on a private high-quality synthetic dataset for information extraction, base...",
    "caps": [],
    "sizes": [
     "3.8b"
    ],
    "pulls": "29.2K",
    "updated": "10 months ago"
   },
   {
    "name": "bespoke-minicheck",
    "desc": "A state-of-the-art fact-checking model developed by Bespoke Labs.",
    "caps": [],
    "sizes": [
     "7b"
    ],
    "pulls": "27.2K",
    "updated": "8 months ago"
   },
   {
    "name": "tulu3",
    "desc": "Tülu 3 is a leading instruction following model family, offering fully open-source data, code, and r...",
    "caps": [],
    "sizes": [
     "8b",
     "70b"
    ],
    "pulls": "26.3K",
    "updated": "5 months ago"
   },
   {
    "name": "megadolphin",
    "desc": "MegaDolphin-2.2-120b is a transformation of Dolphin-2.2-70b created by interleaving the model with i...",
    "caps": [],
    "sizes": [
     "120b"
    ],
    "pulls": "26.2K",
    "updated": "1 year ago"
   },
   {
    "name": "notux",
    "desc": "A top-performing mixture of experts model, fine-tuned with high-quality data.",
    "caps": [],
    "sizes": [
     "8x7b"
    ],
    "pulls": "25.4K",
    "updated": "1 year ago"
   },
   {
    "name": "open-orca-platypus2",
    "desc": "Merge of the Open Orca OpenChat model and the Garage-bAInd Platypus 2 model. Designed for chat and c...",
    "caps": [],
    "sizes": [
     "13b"
    ],
    "pulls": "25K",
    "updated": "1 year ago"
   },
   {
    "name": "notus",
    "desc": "A 7B chat model fine-tuned with high-quality data and based on Zephyr.",
    "caps": [],
    "sizes": [
     "7b"
    ],
    "pulls": "24.7K",
    "updated": "1 year ago"
   },
   {
    "name": "goliath",
    "desc": "A language model created by combining two fine-tuned Llama 2 70B models into one.",
    "caps": [],
    "sizes": [],
    "pulls": "23.7K",
    "updated": "1 year ago"
   },
   {
    "name": "firefunction-v2",
    "desc": "An open weights function calling model based on Llama 3, competitive with GPT-4o function calling ca...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "70b"
    ],
    "pulls": "21.4K",
    "updated": "10 months ago"
   },
   {
    "name": "dbrx",
    "desc": "DBRX is an open, general-purpose LLM created by Databricks.",
    "caps": [],
    "sizes": [
     "132b"
    ],
    "pulls": "19.3K",
    "updated": "1 year ago"
   },
   {
    "name": "granite3-guardian",
    "desc": "The IBM Granite Guardian 3.0 2B and 8B models are designed to detect risks in prompts and/or respons...",
    "caps": [],
    "sizes": [
     "2b",
     "8b"
    ],
    "pulls": "19.1K",
    "updated": "6 months ago"
   },
   {
    "name": "phi4-mini-reasoning",
    "desc": "Phi 4 mini reasoning is a lightweight open model that balances efficiency with advanced reasoning ab...",
    "caps": [],
    "sizes": [
     "3.8b"
    ],
    "pulls": "18.7K",
    "updated": "4 weeks ago"
   },
   {
    "name": "alfred",
    "desc": "A robust conversational model designed to be used for both chat and instruct use cases.",
    "caps": [],
    "sizes": [
     "40b"
    ],
    "pulls": "17K",
    "updated": "1 year ago"
   },
   {
    "name": "command-a",
    "desc": "111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "111b"
    ],
    "pulls": "15.4K",
    "updated": "2 months ago"
   },
   {
    "name": "sailor2",
    "desc": "Sailor2 are multilingual language models made for South-East Asia. Available in 1B, 8B, and 20B para...",
    "caps": [],
    "sizes": [
     "1b",
     "8b",
     "20b"
    ],
    "pulls": "12K",
    "updated": "5 months ago"
   },
   {
    "name": "command-r7b-arabic",
    "desc": "A new state-of-the-art version of the lightweight Command R7B model that excels in advanced Arabic l...",
    "caps": [
     "tools"
    ],
    "sizes": [
     "7b"
    ],
    "pulls": "",
    "updated": ""
   },
   {
    "name": "6,321",
    "desc": "Pulls",
    "caps": [],
    "sizes": [],
    "pulls": "",
    "updated": "3 months ago"
   }
  ]
 },
 "project": {
  "name": "LLMStruct",
  "scale": "272 modules, 1857 functions, 183 classes",
  "hardware": "RTX 3060 Ti (8GB VRAM)",
  "budget": "$21.685 remaining"
 },
 "question": "Which Ollama models should we use for LLMStruct architectural analysis on RTX 3060 Ti 8GB? Consider: original GPT-4.1 plan vs Grok series vs hybrid approach vs enhanced self-analysis with local models."
}