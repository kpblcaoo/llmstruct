{
  "total_models": 233,
  "categories": {
    "vision": 19,
    "code": 33,
    "embedding": 14,
    "reasoning": 16,
    "general": 114,
    "small_efficient": 20,
    "large_powerful": 17
  },
  "rtx3060ti_compatible": 135,
  "top_models_by_category": {
    "vision": [
      {
        "name": "vision",
        "description": "902.8K",
        "capabilities": [],
        "sizes": [
          "8b",
          "4"
        ],
        "pulls": "902.8K",
        "tags": 4,
        "updated": "1 year ago"
      },
      {
        "name": "vision",
        "description": "228.2K",
        "capabilities": [],
        "sizes": [
          "3b",
          "7b",
          "32b",
          "72b",
          "17"
        ],
        "pulls": "228.2K",
        "tags": 17,
        "updated": "1 week ago"
      },
      {
        "name": "vision",
        "description": "1.8b",
        "capabilities": [],
        "sizes": [
          "18"
        ],
        "pulls": "195.7K",
        "tags": 18,
        "updated": "1 year ago"
      }
    ],
    "code": [
      {
        "name": "starcoder2",
        "description": "StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes: 3B, 7B and 15B parameters.",
        "capabilities": [],
        "sizes": [
          "3b",
          "7b",
          "15b",
          "67"
        ],
        "pulls": "959.8K",
        "tags": 67,
        "updated": "8 months ago"
      },
      {
        "name": "deepseek-coder-v2",
        "description": "An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.",
        "capabilities": [],
        "sizes": [
          "16b",
          "236b",
          "64"
        ],
        "pulls": "836.7K",
        "tags": 64,
        "updated": "8 months ago"
      },
      {
        "name": "deepseek-coder",
        "description": "DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.",
        "capabilities": [],
        "sizes": [
          "33b",
          "102"
        ],
        "pulls": "737.9K",
        "tags": 102,
        "updated": "1 year ago"
      }
    ],
    "embedding": [
      {
        "name": "embedding",
        "description": "22m",
        "capabilities": [],
        "sizes": [
          "22m",
          "33m",
          "110m",
          "137m",
          "335m",
          "16"
        ],
        "pulls": "738.1K",
        "tags": 16,
        "updated": "1 year ago"
      },
      {
        "name": "embedding",
        "description": "22m",
        "capabilities": [],
        "sizes": [
          "22m",
          "33m",
          "10"
        ],
        "pulls": "529.1K",
        "tags": 10,
        "updated": "1 year ago"
      },
      {
        "name": "embedding",
        "description": "335m",
        "capabilities": [],
        "sizes": [
          "335m",
          "3"
        ],
        "pulls": "104.5K",
        "tags": 3,
        "updated": "9 months ago"
      }
    ],
    "reasoning": [
      {
        "name": "openthinker",
        "description": "A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-R1.",
        "capabilities": [],
        "sizes": [
          "7b",
          "32b",
          "15"
        ],
        "pulls": "527.7K",
        "tags": 15,
        "updated": "1 month ago"
      },
      {
        "name": "wizardlm2",
        "description": "State of the art large language model from Microsoft AI with improved performance on complex chat, multilingual, reasoning and agent use cases.",
        "capabilities": [],
        "sizes": [
          "7b",
          "22"
        ],
        "pulls": "368.9K",
        "tags": 22,
        "updated": "1 year ago"
      },
      {
        "name": "reflection",
        "description": "A high-performing model trained with a new technique called Reflection-tuning that teaches a LLM to detect mistakes in its reasoning and correct course.",
        "capabilities": [],
        "sizes": [
          "70b",
          "17"
        ],
        "pulls": "105.7K",
        "tags": 17,
        "updated": "8 months ago"
      }
    ],
    "general": [
      {
        "name": "tools",
        "description": "856.2K",
        "capabilities": [],
        "sizes": [
          "22b",
          "24b",
          "21"
        ],
        "pulls": "856.2K",
        "tags": 21,
        "updated": "4 months ago"
      },
      {
        "name": "tools",
        "description": "135m",
        "capabilities": [],
        "sizes": [
          "135m",
          "360m",
          "49"
        ],
        "pulls": "837.6K",
        "tags": 49,
        "updated": "7 months ago"
      },
      {
        "name": "dolphin-mistral",
        "description": "348.7K",
        "capabilities": [],
        "sizes": [
          "7b",
          "120"
        ],
        "pulls": "348.7K",
        "tags": 120,
        "updated": ""
      }
    ],
    "small_efficient": [
      {
        "name": "orca-mini",
        "description": "A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.",
        "capabilities": [],
        "sizes": [
          "3b",
          "7b",
          "13b",
          "70b",
          "119"
        ],
        "pulls": "341.6K",
        "tags": 119,
        "updated": "1 year ago"
      },
      {
        "name": "tools",
        "description": "98.7K",
        "capabilities": [],
        "sizes": [
          "2b",
          "8b",
          "33"
        ],
        "pulls": "98.7K",
        "tags": 33,
        "updated": "6 months ago"
      },
      {
        "name": "tools",
        "description": "96.2K",
        "capabilities": [],
        "sizes": [
          "2b",
          "8b",
          "33"
        ],
        "pulls": "96.2K",
        "tags": 33,
        "updated": "4 months ago"
      }
    ],
    "large_powerful": [
      {
        "name": "llama3-chatqa",
        "description": "A model from NVIDIA based on Llama 3 that excels at conversational question answering (QA) and retrieval-augmented generation (RAG).",
        "capabilities": [],
        "sizes": [
          "8b",
          "70b",
          "35"
        ],
        "pulls": "105.7K",
        "tags": 35,
        "updated": "1 year ago"
      },
      {
        "name": "wizard-math",
        "description": "Model focused on math and logic problems",
        "capabilities": [],
        "sizes": [
          "7b",
          "13b",
          "70b",
          "64"
        ],
        "pulls": "104.7K",
        "tags": 64,
        "updated": "1 year ago"
      },
      {
        "name": "tools",
        "description": "93.9M",
        "capabilities": [],
        "sizes": [
          "8b",
          "70b",
          "405b",
          "93"
        ],
        "pulls": "93.9M",
        "tags": 93,
        "updated": "6 months ago"
      }
    ]
  },
  "recommended_for_llmstruct": [
    {
      "name": "qwen2.5-coder",
      "description": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.",
      "capabilities": [],
      "sizes": [],
      "pulls": 0,
      "tags": 0,
      "updated": "",
      "max_size_b": 0,
      "rtx3060ti_compatibility": "excellent",
      "recommended_for": "code_analysis_cleanup"
    },
    {
      "name": "stable-code",
      "description": "Stable Code 3B is a coding model with instruct and code completion variants on par with models such as Code Llama 7B that are 2.5x larger.",
      "capabilities": [],
      "sizes": [
        "3b",
        "36"
      ],
      "pulls": "132.1K",
      "tags": 36,
      "updated": "1 year ago",
      "max_size_b": 3.0,
      "rtx3060ti_compatibility": "excellent",
      "recommended_for": "code_analysis_cleanup"
    },
    {
      "name": "codegemma",
      "description": "CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks like fill-in-the-middle code completion, code generation, natural language understanding, mathematical reasoning, and instruction following.",
      "capabilities": [],
      "sizes": [
        "2b",
        "7b",
        "85"
      ],
      "pulls": "614.2K",
      "tags": 85,
      "updated": "10 months ago",
      "max_size_b": 7.0,
      "rtx3060ti_compatibility": "good",
      "recommended_for": "code_analysis_cleanup"
    }
  ]
}