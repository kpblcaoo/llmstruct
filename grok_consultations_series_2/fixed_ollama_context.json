{
  "consultation_context": {
    "document_type": "comprehensive_context",
    "version": "2.2_fixed",
    "sections": {
      "Strategic Decision Point: GPT-4.1 vs Grok vs Hybrid Approach": "",
      "üìä PROJECT STATUS": "**LLMStruct AI-Enhanced Development Environment**\n- **Scale:** 272 modules, 1857 functions, 183 classes\n- **Critical Problems:** \n  - Giant number of unsorted files, docs, scripts, tests and so on\n  - 8 different bot versions need consolidation\n  - Architectural analysis needed across all subsystems\n",
      "ü§ñ BUILT-IN AI SELF-ANALYSIS CAPABILITIES": "**LLMStruct has advanced AI systems that could assist analysis:**\n\n**1. AI Self-Awareness System (`src/llmstruct/ai_self_awareness.py`)**\n- SystemCapabilityDiscovery - analyzes all 272 modules automatically\n- Can search through 1857 functions and 183 classes intelligently\n- Real-time capability analysis and smart caching\n- Already indexed the entire project structure\n\n**2. struct.json - Complete Project Knowledge Base**\n- Contains parsed analysis of all 272 modules\n- Function signatures, dependencies, class hierarchies\n- Auto-generated and constantly updated\n- Can be used for semantic search and duplicate detection\n\n**3. Context Orchestrator (`src/llmstruct/context_orchestrator.py`)**\n- Manages different analysis modes: full, focused, minimal, session\n- Adaptive context switching based on task requirements\n- Token budget optimization for efficient analysis\n\n**4. Metrics & Analytics System**\n- Tracks analysis progress, token usage, efficiency metrics\n- Can measure analysis quality and identify bottlenecks\n- Integration with CLI for automated reporting\n\n**These systems mean we already have internal capabilities to:**\n- Parse and analyze the 272 modules automatically\n- Detect duplicates and dead code through struct.json analysis\n- Generate dependency maps and architectural insights\n- Optimize analysis workflows for maximum efficiency\n",
      "üéØ ORIGINAL EXPERT CONSENSUS": "**Unanimous Recommendation from DeepSeek Coder, Mistral, and Grok:**\n- **Primary Choice:** GPT-4.1 with 1M context window\n- **Budget:** $22 total for comprehensive architectural analysis\n- **Method:** Structured prompts with CoT protection\n- **Goal:** Complete architectural analysis with implementation roadmap\n",
      "‚úÖ SERIES 1 RESULTS (Grok-based)": "- **5 consultations completed:** $0.315 spent\n- **Remaining budget:** $21.685\n- **Key outputs:** Structured templates, 4-level analysis approach, diagram specs\n- **Status:** Strong foundation established\n",
      "ü§ñ LOCAL OLLAMA CAPABILITIES ANALYSIS": "**Hardware:** RTX 3060 Ti (8GB VRAM)\n**Available:** 233 models total, 135 compatible with hardware\n\n**Top Models by Category (from full analysis):**\n\n**Code Analysis Leaders:**\n- **starcoder2** (959.8K pulls) - Next-gen transparent code LLM, 3B/7B/15B sizes\n- **deepseek-coder-v2** (836.7K pulls) - GPT4-Turbo comparable performance, 16B/236B\n- **deepseek-coder** (737.9K pulls) - 2 trillion token training, 33B parameters\n\n**Reasoning & Analysis:**\n- **openthinker** (527.7K pulls) - DeepSeek-R1 derived, thinking capabilities, 7B/32B\n- **wizardlm2** (368.9K pulls) - Microsoft AI reasoning specialist, 7B\n- **reflection** (105.7K pulls) - Self-correcting reasoning, 70B\n\n**Embeddings for Search:**\n- Multiple embedding models with 22M-335M parameters\n- 738.1K+ pulls indicating proven reliability\n- Perfect for semantic analysis of struct.json\n\n**Vision for Diagrams:**\n- Various vision models with 902.8K+ pulls\n- Sizes from 3B to 72B parameters\n- Capable of diagram generation and analysis\n",
      "ü§î STRATEGIC DECISION NEEDED": "**Four approaches now possible:**\n\n1. **Original GPT-4.1 Plan** ($22, 1M context)\n   - Comprehensive single-model analysis\n   - Proven expert recommendation\n\n2. **Continued Grok Series** ($21.685, proven effective)\n   - Strategic consultation approach\n   - Series 1 showed excellent results\n\n3. **Hybrid: 1M Token Models + Strategic Grok + Ollama** (needs careful planning)\n   - Combine best of all worlds\n   - Strategic decisions via high-end models\n   - Technical implementation via local models\n\n4. **Self-Analysis Enhanced Approach**\n   - Leverage built-in LLMStruct AI systems\n   - Use struct.json for automated analysis\n   - Combine internal capabilities with external consultation\n",
      "‚ùì SPECIFIC QUESTION FOR EXPERT ANALYSIS": "**Given our LLMStruct project with built-in AI capabilities, which approach would be most effective for:**\n\n1. **Comprehensive architectural analysis** leveraging existing struct.json and AI systems\n2. **Critical project cleanup** using automated tools + expert guidance\n3. **Implementation-ready outputs** with diagrams and documentation\n4. **Maximum value** within budget constraints\n\n**Key considerations:**\n- We already have AI self-awareness systems that can analyze the 272 modules\n- struct.json contains complete project knowledge that could accelerate analysis\n- RTX 3060 Ti can run sophisticated local models for technical tasks\n- $21.685 budget allows for strategic high-level consultations\n\n**Should we proceed with original GPT-4.1 plan, continue proven Grok approach, implement hybrid strategy, or create novel approach combining internal AI capabilities with external expertise?**\n\n**Please recommend specific Ollama models and strategic approach considering our existing AI infrastructure.** "
    },
    "raw_content": "# üß† COMPREHENSIVE LLMStruct ANALYSIS CONTEXT v2.0\n## Strategic Decision Point: GPT-4.1 vs Grok vs Hybrid Approach\n\n### üìä PROJECT STATUS\n**LLMStruct AI-Enhanced Development Environment**\n- **Scale:** 272 modules, 1857 functions, 183 classes\n- **Critical Problems:** \n  - Giant number of unsorted files, docs, scripts, tests and so on\n  - 8 different bot versions need consolidation\n  - Architectural analysis needed across all subsystems\n\n### ü§ñ BUILT-IN AI SELF-ANALYSIS CAPABILITIES\n**LLMStruct has advanced AI systems that could assist analysis:**\n\n**1. AI Self-Awareness System (`src/llmstruct/ai_self_awareness.py`)**\n- SystemCapabilityDiscovery - analyzes all 272 modules automatically\n- Can search through 1857 functions and 183 classes intelligently\n- Real-time capability analysis and smart caching\n- Already indexed the entire project structure\n\n**2. struct.json - Complete Project Knowledge Base**\n- Contains parsed analysis of all 272 modules\n- Function signatures, dependencies, class hierarchies\n- Auto-generated and constantly updated\n- Can be used for semantic search and duplicate detection\n\n**3. Context Orchestrator (`src/llmstruct/context_orchestrator.py`)**\n- Manages different analysis modes: full, focused, minimal, session\n- Adaptive context switching based on task requirements\n- Token budget optimization for efficient analysis\n\n**4. Metrics & Analytics System**\n- Tracks analysis progress, token usage, efficiency metrics\n- Can measure analysis quality and identify bottlenecks\n- Integration with CLI for automated reporting\n\n**These systems mean we already have internal capabilities to:**\n- Parse and analyze the 272 modules automatically\n- Detect duplicates and dead code through struct.json analysis\n- Generate dependency maps and architectural insights\n- Optimize analysis workflows for maximum efficiency\n\n### üéØ ORIGINAL EXPERT CONSENSUS\n**Unanimous Recommendation from DeepSeek Coder, Mistral, and Grok:**\n- **Primary Choice:** GPT-4.1 with 1M context window\n- **Budget:** $22 total for comprehensive architectural analysis\n- **Method:** Structured prompts with CoT protection\n- **Goal:** Complete architectural analysis with implementation roadmap\n\n### ‚úÖ SERIES 1 RESULTS (Grok-based)\n- **5 consultations completed:** $0.315 spent\n- **Remaining budget:** $21.685\n- **Key outputs:** Structured templates, 4-level analysis approach, diagram specs\n- **Status:** Strong foundation established\n\n### ü§ñ LOCAL OLLAMA CAPABILITIES ANALYSIS\n**Hardware:** RTX 3060 Ti (8GB VRAM)\n**Available:** 233 models total, 135 compatible with hardware\n\n**Top Models by Category (from full analysis):**\n\n**Code Analysis Leaders:**\n- **starcoder2** (959.8K pulls) - Next-gen transparent code LLM, 3B/7B/15B sizes\n- **deepseek-coder-v2** (836.7K pulls) - GPT4-Turbo comparable performance, 16B/236B\n- **deepseek-coder** (737.9K pulls) - 2 trillion token training, 33B parameters\n\n**Reasoning & Analysis:**\n- **openthinker** (527.7K pulls) - DeepSeek-R1 derived, thinking capabilities, 7B/32B\n- **wizardlm2** (368.9K pulls) - Microsoft AI reasoning specialist, 7B\n- **reflection** (105.7K pulls) - Self-correcting reasoning, 70B\n\n**Embeddings for Search:**\n- Multiple embedding models with 22M-335M parameters\n- 738.1K+ pulls indicating proven reliability\n- Perfect for semantic analysis of struct.json\n\n**Vision for Diagrams:**\n- Various vision models with 902.8K+ pulls\n- Sizes from 3B to 72B parameters\n- Capable of diagram generation and analysis\n\n### ü§î STRATEGIC DECISION NEEDED\n**Four approaches now possible:**\n\n1. **Original GPT-4.1 Plan** ($22, 1M context)\n   - Comprehensive single-model analysis\n   - Proven expert recommendation\n\n2. **Continued Grok Series** ($21.685, proven effective)\n   - Strategic consultation approach\n   - Series 1 showed excellent results\n\n3. **Hybrid: 1M Token Models + Strategic Grok + Ollama** (needs careful planning)\n   - Combine best of all worlds\n   - Strategic decisions via high-end models\n   - Technical implementation via local models\n\n4. **Self-Analysis Enhanced Approach**\n   - Leverage built-in LLMStruct AI systems\n   - Use struct.json for automated analysis\n   - Combine internal capabilities with external consultation\n\n### ‚ùì SPECIFIC QUESTION FOR EXPERT ANALYSIS\n**Given our LLMStruct project with built-in AI capabilities, which approach would be most effective for:**\n\n1. **Comprehensive architectural analysis** leveraging existing struct.json and AI systems\n2. **Critical project cleanup** using automated tools + expert guidance\n3. **Implementation-ready outputs** with diagrams and documentation\n4. **Maximum value** within budget constraints\n\n**Key considerations:**\n- We already have AI self-awareness systems that can analyze the 272 modules\n- struct.json contains complete project knowledge that could accelerate analysis\n- RTX 3060 Ti can run sophisticated local models for technical tasks\n- $21.685 budget allows for strategic high-level consultations\n\n**Should we proceed with original GPT-4.1 plan, continue proven Grok approach, implement hybrid strategy, or create novel approach combining internal AI capabilities with external expertise?**\n\n**Please recommend specific Ollama models and strategic approach considering our existing AI infrastructure.** "
  },
  "ollama_analysis": {
    "parsing_info": {
      "total_parsed": 234,
      "source_file": "tmp/ollama_list.txt",
      "parsing_algorithm": "fixed_structure_aware",
      "timestamp": "2025-01-27_fixed",
      "validation": "no_size_names_as_models"
    },
    "models_data": {
      "summary": {
        "total_valid_models": 234,
        "rtx3060ti_native": 198,
        "rtx3060ti_quantized": 7,
        "requires_more_vram": 29
      },
      "categories": {
        "code_specialists": {
          "description": "Models specifically designed for code analysis, generation, and debugging",
          "models": [
            {
              "name": "devstral",
              "description": "Devstral: the best open source model for coding agents",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llava",
              "description": "üåã LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpo...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "qwen2.5-coder",
              "description": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and co...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "codellama",
              "description": "A large language model that can use text prompts to generate and discuss code.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "dolphin3",
              "description": "Dolphin 3.0 Llama 3.1 8B üê¨ is the next generation of the Dolphin series of instruct-tuned models designed to be the ulti...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "starcoder2",
              "description": "StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes: 3B, 7B and 15B para...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "deepseek-coder-v2",
              "description": "An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specifi...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "deepseek-coder",
              "description": "DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "codegemma",
              "description": "CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks like fill-in-the-mi...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "dolphin-mixtral",
              "description": "Uncensored, 8x7b and 8x22b fine-tuned models based on the Mixtral mixture of experts models that excels at coding tasks....",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "dolphin-mistral",
              "description": "The uncensored Dolphin model based on Mistral that excels at coding tasks. Updated to version 2.8.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "dolphin-llama3",
              "description": "Dolphin 2.9 is a new model with 8B and 70B sizes by Eric Hartford based on Llama 3 that has a variety of instruction, co...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "codestral",
              "description": "Codestral is Mistral AI‚Äôs first-ever code model designed for code generation tasks.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "granite-code",
              "description": "A family of open foundation models by IBM for Code Intelligence",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "starcoder",
              "description": "StarCoder is a code generation model trained on 80+ programming languages.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "codeqwen",
              "description": "CodeQwen1.5 is a large language model pretrained on a large amount of code data.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "codegeex4",
              "description": "A versatile model for AI software development scenarios, including code completion.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "mistral-large",
              "description": "Mistral Large 2 is Mistral's new flagship model that is significantly more capable in code generation, mathematics, and ...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "stable-code",
              "description": "Stable Code 3B is a coding model with instruct and code completion variants on par with models such as Code Llama 7B tha...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "deepcoder",
              "description": "DeepCoder is a fully open-Source 14B coder model at O3-mini level, with a 1.5B version also available.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "nous-hermes2",
              "description": "The powerful family of models by Nous Research that excels at scientific discussion and coding tasks.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "wizardcoder",
              "description": "State-of-the-art code generation model",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "sqlcoder",
              "description": "SQLCoder is a code completion model fined-tuned on StarCoder for SQL generation tasks",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "granite3-dense",
              "description": "The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrieval augmented genera...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "dolphincoder",
              "description": "A 7B and 15B uncensored variant of the Dolphin model family that excels at coding, based on StarCoder2.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "phind-codellama",
              "description": "Code generation model based on Code Llama.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "yi-coder",
              "description": "Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding performance with fewer th...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "athene-v2",
              "description": "Athene-V2 is a 72B parameter model which excels at code completion, mathematics, and log extraction tasks.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "falcon3",
              "description": "A family of efficient AI models under 10B parameters performant in science, math, and coding through innovative training...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "deepseek-v2.5",
              "description": "An upgraded version of DeekSeek-V2 that integrates the general and coding abilities of both DeepSeek-V2-Chat and DeepSee...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "granite-embedding",
              "description": "The IBM Granite Embedding 30M and 278M models models are text-only dense biencoder embedding models, with 30M available ...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llama-pro",
              "description": "An expansion of Llama 2 that specializes in integrating both general language understanding and domain-specific knowledg...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "opencoder",
              "description": "OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting chat in English and ...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "exaone-deep",
              "description": "EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "codeup",
              "description": "Great code generation model based on Llama2.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "falcon2",
              "description": "Falcon2 is an 11B parameters causal decoder-only model built by TII and trained over 5T tokens.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "magicoder",
              "description": "üé© Magicoder is a family of 7B parameter models trained on 75K synthetic instruction data using OSS-Instruct, a novel app...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "codebooga",
              "description": "A high-performing code instruct model created by merging two existing code models.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tulu3",
              "description": "T√ºlu 3 is a leading instruction following model family, offering fully open-source data, code, and recipes by the The Al...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "open-orca-platypus2",
              "description": "Merge of the Open Orca OpenChat model and the Garage-bAInd Platypus 2 model. Designed for chat and code generation.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            }
          ],
          "rtx3060ti_ready": [
            {
              "name": "devstral",
              "description": "Devstral: the best open source model for coding agents",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llava",
              "description": "üåã LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpo...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "qwen2.5-coder",
              "description": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and co...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "codellama",
              "description": "A large language model that can use text prompts to generate and discuss code.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "dolphin3",
              "description": "Dolphin 3.0 Llama 3.1 8B üê¨ is the next generation of the Dolphin series of instruct-tuned models designed to be the ulti...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "starcoder2",
              "description": "StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes: 3B, 7B and 15B para...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "deepseek-coder-v2",
              "description": "An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specifi...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "deepseek-coder",
              "description": "DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "codegemma",
              "description": "CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks like fill-in-the-mi...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "dolphin-mixtral",
              "description": "Uncensored, 8x7b and 8x22b fine-tuned models based on the Mixtral mixture of experts models that excels at coding tasks....",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "dolphin-mistral",
              "description": "The uncensored Dolphin model based on Mistral that excels at coding tasks. Updated to version 2.8.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "dolphin-llama3",
              "description": "Dolphin 2.9 is a new model with 8B and 70B sizes by Eric Hartford based on Llama 3 that has a variety of instruction, co...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "codestral",
              "description": "Codestral is Mistral AI‚Äôs first-ever code model designed for code generation tasks.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "granite-code",
              "description": "A family of open foundation models by IBM for Code Intelligence",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "starcoder",
              "description": "StarCoder is a code generation model trained on 80+ programming languages.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "codeqwen",
              "description": "CodeQwen1.5 is a large language model pretrained on a large amount of code data.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "codegeex4",
              "description": "A versatile model for AI software development scenarios, including code completion.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "mistral-large",
              "description": "Mistral Large 2 is Mistral's new flagship model that is significantly more capable in code generation, mathematics, and ...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "stable-code",
              "description": "Stable Code 3B is a coding model with instruct and code completion variants on par with models such as Code Llama 7B tha...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "deepcoder",
              "description": "DeepCoder is a fully open-Source 14B coder model at O3-mini level, with a 1.5B version also available.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "nous-hermes2",
              "description": "The powerful family of models by Nous Research that excels at scientific discussion and coding tasks.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "wizardcoder",
              "description": "State-of-the-art code generation model",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "sqlcoder",
              "description": "SQLCoder is a code completion model fined-tuned on StarCoder for SQL generation tasks",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "granite3-dense",
              "description": "The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrieval augmented genera...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "dolphincoder",
              "description": "A 7B and 15B uncensored variant of the Dolphin model family that excels at coding, based on StarCoder2.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "phind-codellama",
              "description": "Code generation model based on Code Llama.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "yi-coder",
              "description": "Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding performance with fewer th...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "athene-v2",
              "description": "Athene-V2 is a 72B parameter model which excels at code completion, mathematics, and log extraction tasks.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "falcon3",
              "description": "A family of efficient AI models under 10B parameters performant in science, math, and coding through innovative training...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "deepseek-v2.5",
              "description": "An upgraded version of DeekSeek-V2 that integrates the general and coding abilities of both DeepSeek-V2-Chat and DeepSee...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "granite-embedding",
              "description": "The IBM Granite Embedding 30M and 278M models models are text-only dense biencoder embedding models, with 30M available ...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llama-pro",
              "description": "An expansion of Llama 2 that specializes in integrating both general language understanding and domain-specific knowledg...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "opencoder",
              "description": "OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting chat in English and ...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "exaone-deep",
              "description": "EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "codeup",
              "description": "Great code generation model based on Llama2.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "falcon2",
              "description": "Falcon2 is an 11B parameters causal decoder-only model built by TII and trained over 5T tokens.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "magicoder",
              "description": "üé© Magicoder is a family of 7B parameter models trained on 75K synthetic instruction data using OSS-Instruct, a novel app...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "codebooga",
              "description": "A high-performing code instruct model created by merging two existing code models.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tulu3",
              "description": "T√ºlu 3 is a leading instruction following model family, offering fully open-source data, code, and recipes by the The Al...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "open-orca-platypus2",
              "description": "Merge of the Open Orca OpenChat model and the Garage-bAInd Platypus 2 model. Designed for chat and code generation.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            }
          ]
        },
        "reasoning_experts": {
          "description": "Models with enhanced reasoning, thinking, and problem-solving capabilities",
          "models": [
            {
              "name": "deepseek-r1",
              "description": "DeepSeek-R1 first-generation of open reasoning models with comparable performance to OpenAI-o3.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "thinking",
              "description": "",
              "capabilities": [],
              "sizes": [
                "1.5b",
                "7b",
                "8b",
                "14b",
                "32b",
                "70b",
                "671b"
              ],
              "pulls": "46.3M",
              "pulls_count": 46300000.0,
              "updated": "22 hours ago",
              "max_size_gb": 671.0
            },
            {
              "name": "gemma3",
              "description": "The current, most capable model that runs on a single GPU.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [
                "thinking"
              ],
              "sizes": [
                "0.6b",
                "1.7b",
                "4b",
                "8b",
                "14b",
                "30b",
                "32b",
                "235b"
              ],
              "pulls": "1.8M",
              "pulls_count": 1800000.0,
              "updated": "yesterday",
              "max_size_gb": 235.0
            },
            {
              "name": "phi4",
              "description": "Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "mistral",
              "description": "The 7B model released by Mistral AI, updated to version 0.3.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "gemma2",
              "description": "Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "gemma",
              "description": "Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "phi3",
              "description": "Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llama3.2-vision",
              "description": "Llama 3.2 Vision is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "mistral-nemo",
              "description": "A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "qwq",
              "description": "QwQ is the reasoning model of the Qwen series.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "mixtral",
              "description": "A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "mistral-small",
              "description": "Mistral Small 3 sets a new benchmark in the ‚Äúsmall‚Äù Large Language Models category below 70B.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "phi",
              "description": "Phi-2: a 2.7B language model by Microsoft Research that demonstrates outstanding reasoning and language understanding ca...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "openthinker",
              "description": "A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-R1.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "wizardlm2",
              "description": "State of the art large language model from Microsoft AI with improved performance on complex chat, multilingual, reasoni...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "phi3.5",
              "description": "A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger sized models.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "zephyr",
              "description": "Zephyr is a series of fine-tuned versions of the Mistral and Mixtral models that are trained to act as helpful assistant...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "wizard-vicuna-uncensored",
              "description": "Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "mistral-openorca",
              "description": "Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "phi4-mini",
              "description": "Phi-4-mini brings significant enhancements in multilingual support, reasoning, and mathematics, and now, the long-awaite...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "openhermes",
              "description": "OpenHermes 2.5 is a 7B model fine-tuned by Teknium on Mistral with fully open datasets.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tinydolphin",
              "description": "An experimental 1.1B parameter model trained on the new Dolphin 2.8 dataset by Eric Hartford and based on TinyLlama.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "bakllava",
              "description": "BakLLaVA is a multimodal model consisting of the Mistral 7B base model augmented with the LLaVA architecture.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "mistral-small3.1",
              "description": "Building upon Mistral Small 3, Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long con...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "neural-chat",
              "description": "A fine-tuned model based on Mistral with good coverage of domain and language.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "reflection",
              "description": "A high-performing model trained with a new technique called Reflection-tuning that teaches a LLM to detect mistakes in i...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "wizard-math",
              "description": "Model focused on math and logic problems",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "granite3.2",
              "description": "Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for thinking capabilities.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "samantha-mistral",
              "description": "A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistral.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "cogito",
              "description": "Cogito v1 Preview is a family of hybrid reasoning models by Deep Cogito that outperform the best available open models o...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llava-phi3",
              "description": "A new small LLaVA model fine-tuned from Phi 3 Mini.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "granite3.2-vision",
              "description": "A compact and efficient vision-language model, specifically designed for visual document understanding, enabling automat...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "deepscaler",
              "description": "A fine-tuned version of Deepseek-R1-Distilled-Qwen-1.5B that surpasses the performance of OpenAI‚Äôs o1-preview with just ...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "internlm2",
              "description": "InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding reasoning capability.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "granite3.3",
              "description": "IBM Granite 2B and 8B models are 128K context length language models that have been fine-tuned for improved reasoning an...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "wizardlm",
              "description": "General use model based on Llama 2.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "dolphin-phi",
              "description": "2.7B uncensored Dolphin model by Eric Hartford, based on the Phi language model by Microsoft Research.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "orca2",
              "description": "Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models. The model is designed to e...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "wizardlm-uncensored",
              "description": "Uncensored version of Wizard LM model",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "phi4-reasoning",
              "description": "Phi 4 reasoning and reasoning plus are 14-billion parameter open-weight reasoning models that rival much larger models o...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "smallthinker",
              "description": "A new small reasoning model fine-tuned from the Qwen 2.5 3B Instruct model.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "yarn-mistral",
              "description": "An extension of Mistral to support context windows of 64K or 128K.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "shieldgemma",
              "description": "ShieldGemma is set of instruction tuned models for evaluating the safety of text prompt input and text output responses ...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "mathstral",
              "description": "MathŒ£tral: a 7B model designed for math reasoning and scientific discovery by Mistral AI.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "r1-1776",
              "description": "A version of the DeepSeek-R1 model that has been post trained to provide unbiased, accurate, and factual information by ...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "marco-o1",
              "description": "An open large reasoning model for real-world solutions by the Alibaba International Digital Commerce Group (AIDC-AI).",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "mistrallite",
              "description": "MistralLite is a fine-tuned model based on Mistral with enhanced capabilities of processing long contexts.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "wizard-vicuna",
              "description": "Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "nuextract",
              "description": "A 3.8B model fine-tuned on a private high-quality synthetic dataset for information extraction, based on Phi-3.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "megadolphin",
              "description": "MegaDolphin-2.2-120b is a transformation of Dolphin-2.2-70b created by interleaving the model with itself.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "phi4-mini-reasoning",
              "description": "Phi 4 mini reasoning is a lightweight open model that balances efficiency with advanced reasoning ability.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            }
          ],
          "rtx3060ti_ready": [
            {
              "name": "deepseek-r1",
              "description": "DeepSeek-R1 first-generation of open reasoning models with comparable performance to OpenAI-o3.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "gemma3",
              "description": "The current, most capable model that runs on a single GPU.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "phi4",
              "description": "Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "mistral",
              "description": "The 7B model released by Mistral AI, updated to version 0.3.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "gemma2",
              "description": "Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "gemma",
              "description": "Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "phi3",
              "description": "Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llama3.2-vision",
              "description": "Llama 3.2 Vision is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "mistral-nemo",
              "description": "A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "qwq",
              "description": "QwQ is the reasoning model of the Qwen series.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "mixtral",
              "description": "A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "mistral-small",
              "description": "Mistral Small 3 sets a new benchmark in the ‚Äúsmall‚Äù Large Language Models category below 70B.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "phi",
              "description": "Phi-2: a 2.7B language model by Microsoft Research that demonstrates outstanding reasoning and language understanding ca...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "openthinker",
              "description": "A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-R1.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "wizardlm2",
              "description": "State of the art large language model from Microsoft AI with improved performance on complex chat, multilingual, reasoni...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "phi3.5",
              "description": "A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger sized models.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "zephyr",
              "description": "Zephyr is a series of fine-tuned versions of the Mistral and Mixtral models that are trained to act as helpful assistant...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "wizard-vicuna-uncensored",
              "description": "Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "mistral-openorca",
              "description": "Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "phi4-mini",
              "description": "Phi-4-mini brings significant enhancements in multilingual support, reasoning, and mathematics, and now, the long-awaite...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "openhermes",
              "description": "OpenHermes 2.5 is a 7B model fine-tuned by Teknium on Mistral with fully open datasets.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tinydolphin",
              "description": "An experimental 1.1B parameter model trained on the new Dolphin 2.8 dataset by Eric Hartford and based on TinyLlama.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "bakllava",
              "description": "BakLLaVA is a multimodal model consisting of the Mistral 7B base model augmented with the LLaVA architecture.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "mistral-small3.1",
              "description": "Building upon Mistral Small 3, Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long con...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "neural-chat",
              "description": "A fine-tuned model based on Mistral with good coverage of domain and language.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "reflection",
              "description": "A high-performing model trained with a new technique called Reflection-tuning that teaches a LLM to detect mistakes in i...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "wizard-math",
              "description": "Model focused on math and logic problems",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "granite3.2",
              "description": "Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for thinking capabilities.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "samantha-mistral",
              "description": "A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistral.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "cogito",
              "description": "Cogito v1 Preview is a family of hybrid reasoning models by Deep Cogito that outperform the best available open models o...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llava-phi3",
              "description": "A new small LLaVA model fine-tuned from Phi 3 Mini.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "granite3.2-vision",
              "description": "A compact and efficient vision-language model, specifically designed for visual document understanding, enabling automat...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "deepscaler",
              "description": "A fine-tuned version of Deepseek-R1-Distilled-Qwen-1.5B that surpasses the performance of OpenAI‚Äôs o1-preview with just ...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "internlm2",
              "description": "InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding reasoning capability.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "granite3.3",
              "description": "IBM Granite 2B and 8B models are 128K context length language models that have been fine-tuned for improved reasoning an...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "wizardlm",
              "description": "General use model based on Llama 2.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "dolphin-phi",
              "description": "2.7B uncensored Dolphin model by Eric Hartford, based on the Phi language model by Microsoft Research.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "orca2",
              "description": "Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models. The model is designed to e...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "wizardlm-uncensored",
              "description": "Uncensored version of Wizard LM model",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "phi4-reasoning",
              "description": "Phi 4 reasoning and reasoning plus are 14-billion parameter open-weight reasoning models that rival much larger models o...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "smallthinker",
              "description": "A new small reasoning model fine-tuned from the Qwen 2.5 3B Instruct model.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "yarn-mistral",
              "description": "An extension of Mistral to support context windows of 64K or 128K.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "shieldgemma",
              "description": "ShieldGemma is set of instruction tuned models for evaluating the safety of text prompt input and text output responses ...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "mathstral",
              "description": "MathŒ£tral: a 7B model designed for math reasoning and scientific discovery by Mistral AI.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "r1-1776",
              "description": "A version of the DeepSeek-R1 model that has been post trained to provide unbiased, accurate, and factual information by ...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "marco-o1",
              "description": "An open large reasoning model for real-world solutions by the Alibaba International Digital Commerce Group (AIDC-AI).",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "mistrallite",
              "description": "MistralLite is a fine-tuned model based on Mistral with enhanced capabilities of processing long contexts.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "wizard-vicuna",
              "description": "Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "nuextract",
              "description": "A 3.8B model fine-tuned on a private high-quality synthetic dataset for information extraction, based on Phi-3.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "megadolphin",
              "description": "MegaDolphin-2.2-120b is a transformation of Dolphin-2.2-70b created by interleaving the model with itself.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "phi4-mini-reasoning",
              "description": "Phi 4 mini reasoning is a lightweight open model that balances efficiency with advanced reasoning ability.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            }
          ]
        },
        "vision_capable": {
          "description": "Multimodal models for diagrams, images, and visual content analysis",
          "models": [
            {
              "name": "vision",
              "description": "",
              "capabilities": [],
              "sizes": [
                "1b",
                "4b",
                "12b",
                "27b"
              ],
              "pulls": "5.4M",
              "pulls_count": 5400000.0,
              "updated": "1 month ago",
              "max_size_gb": 27.0
            },
            {
              "name": "llama4",
              "description": "Meta's latest collection of multimodal models.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "vision",
              "description": "",
              "capabilities": [
                "tools"
              ],
              "sizes": [
                "16x17b",
                "128x17b"
              ],
              "pulls": "367K",
              "pulls_count": 367000.0,
              "updated": "2 days ago",
              "max_size_gb": 17.0
            },
            {
              "name": "qwen2.5vl",
              "description": "Flagship vision-language model of Qwen and also a significant leap from the previous Qwen2-VL.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "vision",
              "description": "",
              "capabilities": [],
              "sizes": [
                "3b",
                "7b",
                "32b",
                "72b"
              ],
              "pulls": "228.2K",
              "pulls_count": 228200.0,
              "updated": "1 week ago",
              "max_size_gb": 72.0
            },
            {
              "name": "vision",
              "description": "",
              "capabilities": [],
              "sizes": [
                "7b",
                "13b",
                "34b"
              ],
              "pulls": "6M",
              "pulls_count": 6000000.0,
              "updated": "1 year ago",
              "max_size_gb": 34.0
            },
            {
              "name": "vision",
              "description": "",
              "capabilities": [],
              "sizes": [
                "11b",
                "90b"
              ],
              "pulls": "2.1M",
              "pulls_count": 2100000.0,
              "updated": "1 week ago",
              "max_size_gb": 90.0
            },
            {
              "name": "minicpm-v",
              "description": "A series of multimodal LLMs (MLLMs) designed for vision-language understanding.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "vision",
              "description": "",
              "capabilities": [],
              "sizes": [
                "8b"
              ],
              "pulls": "1.6M",
              "pulls_count": 1600000.0,
              "updated": "6 months ago",
              "max_size_gb": 8.0
            },
            {
              "name": "llava-llama3",
              "description": "A LLaVA model fine-tuned from Llama 3 Instruct with better scores in several benchmarks.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "vision",
              "description": "",
              "capabilities": [],
              "sizes": [
                "8b"
              ],
              "pulls": "902.8K",
              "pulls_count": 902800.0,
              "updated": "1 year ago",
              "max_size_gb": 8.0
            },
            {
              "name": "moondream",
              "description": "moondream2 is a small vision language model designed to run efficiently on edge devices.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "vision",
              "description": "",
              "capabilities": [],
              "sizes": [
                "1.8b"
              ],
              "pulls": "195.7K",
              "pulls_count": 195700.0,
              "updated": "1 year ago",
              "max_size_gb": 1.8
            },
            {
              "name": "vision",
              "description": "",
              "capabilities": [],
              "sizes": [
                "7b"
              ],
              "pulls": "118.5K",
              "pulls_count": 118500.0,
              "updated": "1 year ago",
              "max_size_gb": 7.0
            },
            {
              "name": "vision",
              "description": "",
              "capabilities": [
                "tools"
              ],
              "sizes": [
                "24b"
              ],
              "pulls": "116.3K",
              "pulls_count": 116300.0,
              "updated": "1 month ago",
              "max_size_gb": 24.0
            },
            {
              "name": "vision",
              "description": "",
              "capabilities": [],
              "sizes": [
                "3.8b"
              ],
              "pulls": "92.2K",
              "pulls_count": 92200.0,
              "updated": "1 year ago",
              "max_size_gb": 3.8
            },
            {
              "name": "vision",
              "description": "",
              "capabilities": [
                "tools"
              ],
              "sizes": [
                "2b"
              ],
              "pulls": "85K",
              "pulls_count": 85000.0,
              "updated": "3 months ago",
              "max_size_gb": 2.0
            }
          ],
          "rtx3060ti_ready": [
            {
              "name": "llama4",
              "description": "Meta's latest collection of multimodal models.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "qwen2.5vl",
              "description": "Flagship vision-language model of Qwen and also a significant leap from the previous Qwen2-VL.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "minicpm-v",
              "description": "A series of multimodal LLMs (MLLMs) designed for vision-language understanding.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "vision",
              "description": "",
              "capabilities": [],
              "sizes": [
                "8b"
              ],
              "pulls": "1.6M",
              "pulls_count": 1600000.0,
              "updated": "6 months ago",
              "max_size_gb": 8.0
            },
            {
              "name": "llava-llama3",
              "description": "A LLaVA model fine-tuned from Llama 3 Instruct with better scores in several benchmarks.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "vision",
              "description": "",
              "capabilities": [],
              "sizes": [
                "8b"
              ],
              "pulls": "902.8K",
              "pulls_count": 902800.0,
              "updated": "1 year ago",
              "max_size_gb": 8.0
            },
            {
              "name": "moondream",
              "description": "moondream2 is a small vision language model designed to run efficiently on edge devices.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "vision",
              "description": "",
              "capabilities": [],
              "sizes": [
                "1.8b"
              ],
              "pulls": "195.7K",
              "pulls_count": 195700.0,
              "updated": "1 year ago",
              "max_size_gb": 1.8
            },
            {
              "name": "vision",
              "description": "",
              "capabilities": [],
              "sizes": [
                "7b"
              ],
              "pulls": "118.5K",
              "pulls_count": 118500.0,
              "updated": "1 year ago",
              "max_size_gb": 7.0
            },
            {
              "name": "vision",
              "description": "",
              "capabilities": [],
              "sizes": [
                "3.8b"
              ],
              "pulls": "92.2K",
              "pulls_count": 92200.0,
              "updated": "1 year ago",
              "max_size_gb": 3.8
            },
            {
              "name": "vision",
              "description": "",
              "capabilities": [
                "tools"
              ],
              "sizes": [
                "2b"
              ],
              "pulls": "85K",
              "pulls_count": 85000.0,
              "updated": "3 months ago",
              "max_size_gb": 2.0
            }
          ]
        },
        "embedding_specialists": {
          "description": "Specialized models for text embeddings, semantic search, and RAG",
          "models": [
            {
              "name": "nomic-embed-text",
              "description": "A high-performing open embedding model with a large token context window.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "embedding",
              "description": "",
              "capabilities": [],
              "sizes": [],
              "pulls": "27M",
              "pulls_count": 27000000.0,
              "updated": "1 year ago",
              "max_size_gb": 0
            },
            {
              "name": "mxbai-embed-large",
              "description": "State-of-the-art large embedding model from mixedbread.ai",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "embedding",
              "description": "335m",
              "capabilities": [],
              "sizes": [],
              "pulls": "3.4M",
              "pulls_count": 3400000.0,
              "updated": "1 year ago",
              "max_size_gb": 0
            },
            {
              "name": "bge-m3",
              "description": "BGE-M3 is a new model from BAAI distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Gr...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "embedding",
              "description": "567m",
              "capabilities": [],
              "sizes": [],
              "pulls": "1.2M",
              "pulls_count": 1200000.0,
              "updated": "9 months ago",
              "max_size_gb": 0
            },
            {
              "name": "snowflake-arctic-embed",
              "description": "A suite of text embedding models by Snowflake, optimized for performance.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "embedding",
              "description": "22m",
              "capabilities": [],
              "sizes": [],
              "pulls": "738.1K",
              "pulls_count": 738100.0,
              "updated": "1 year ago",
              "max_size_gb": 0
            },
            {
              "name": "all-minilm",
              "description": "Embedding models on very large sentence level datasets.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "embedding",
              "description": "22m",
              "capabilities": [],
              "sizes": [],
              "pulls": "529.1K",
              "pulls_count": 529100.0,
              "updated": "1 year ago",
              "max_size_gb": 0
            },
            {
              "name": "bge-large",
              "description": "Embedding model from BAAI mapping texts to vectors.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "embedding",
              "description": "335m",
              "capabilities": [],
              "sizes": [],
              "pulls": "104.5K",
              "pulls_count": 104500.0,
              "updated": "9 months ago",
              "max_size_gb": 0
            },
            {
              "name": "snowflake-arctic-embed2",
              "description": "Snowflake's frontier embedding model. Arctic Embed 2.0 adds multilingual support without sacrificing English performance...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "embedding",
              "description": "568m",
              "capabilities": [],
              "sizes": [],
              "pulls": "84.7K",
              "pulls_count": 84700.0,
              "updated": "5 months ago",
              "max_size_gb": 0
            },
            {
              "name": "paraphrase-multilingual",
              "description": "Sentence-transformers model that can be used for tasks like clustering or semantic search.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "embedding",
              "description": "278m",
              "capabilities": [],
              "sizes": [],
              "pulls": "61.6K",
              "pulls_count": 61600.0,
              "updated": "9 months ago",
              "max_size_gb": 0
            },
            {
              "name": "embedding",
              "description": "30m",
              "capabilities": [],
              "sizes": [],
              "pulls": "51.2K",
              "pulls_count": 51200.0,
              "updated": "5 months ago",
              "max_size_gb": 0
            }
          ],
          "rtx3060ti_ready": [
            {
              "name": "nomic-embed-text",
              "description": "A high-performing open embedding model with a large token context window.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "embedding",
              "description": "",
              "capabilities": [],
              "sizes": [],
              "pulls": "27M",
              "pulls_count": 27000000.0,
              "updated": "1 year ago",
              "max_size_gb": 0
            },
            {
              "name": "mxbai-embed-large",
              "description": "State-of-the-art large embedding model from mixedbread.ai",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "embedding",
              "description": "335m",
              "capabilities": [],
              "sizes": [],
              "pulls": "3.4M",
              "pulls_count": 3400000.0,
              "updated": "1 year ago",
              "max_size_gb": 0
            },
            {
              "name": "bge-m3",
              "description": "BGE-M3 is a new model from BAAI distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Gr...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "embedding",
              "description": "567m",
              "capabilities": [],
              "sizes": [],
              "pulls": "1.2M",
              "pulls_count": 1200000.0,
              "updated": "9 months ago",
              "max_size_gb": 0
            },
            {
              "name": "snowflake-arctic-embed",
              "description": "A suite of text embedding models by Snowflake, optimized for performance.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "embedding",
              "description": "22m",
              "capabilities": [],
              "sizes": [],
              "pulls": "738.1K",
              "pulls_count": 738100.0,
              "updated": "1 year ago",
              "max_size_gb": 0
            },
            {
              "name": "all-minilm",
              "description": "Embedding models on very large sentence level datasets.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "embedding",
              "description": "22m",
              "capabilities": [],
              "sizes": [],
              "pulls": "529.1K",
              "pulls_count": 529100.0,
              "updated": "1 year ago",
              "max_size_gb": 0
            },
            {
              "name": "bge-large",
              "description": "Embedding model from BAAI mapping texts to vectors.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "embedding",
              "description": "335m",
              "capabilities": [],
              "sizes": [],
              "pulls": "104.5K",
              "pulls_count": 104500.0,
              "updated": "9 months ago",
              "max_size_gb": 0
            },
            {
              "name": "snowflake-arctic-embed2",
              "description": "Snowflake's frontier embedding model. Arctic Embed 2.0 adds multilingual support without sacrificing English performance...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "embedding",
              "description": "568m",
              "capabilities": [],
              "sizes": [],
              "pulls": "84.7K",
              "pulls_count": 84700.0,
              "updated": "5 months ago",
              "max_size_gb": 0
            },
            {
              "name": "paraphrase-multilingual",
              "description": "Sentence-transformers model that can be used for tasks like clustering or semantic search.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "embedding",
              "description": "278m",
              "capabilities": [],
              "sizes": [],
              "pulls": "61.6K",
              "pulls_count": 61600.0,
              "updated": "9 months ago",
              "max_size_gb": 0
            },
            {
              "name": "embedding",
              "description": "30m",
              "capabilities": [],
              "sizes": [],
              "pulls": "51.2K",
              "pulls_count": 51200.0,
              "updated": "5 months ago",
              "max_size_gb": 0
            }
          ]
        },
        "general_purpose": {
          "description": "Versatile models for general tasks and conversations",
          "models": [
            {
              "name": "qwen3",
              "description": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixt...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "24b"
              ],
              "pulls": "86.6K",
              "pulls_count": 86600.0,
              "updated": "1 week ago",
              "max_size_gb": 24.0
            },
            {
              "name": "llama3.3",
              "description": "New state of the art 70B model. Llama 3.3 70B offers similar performance compared to the Llama 3.1 405B model.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "70b"
              ],
              "pulls": "1.9M",
              "pulls_count": 1900000.0,
              "updated": "5 months ago",
              "max_size_gb": 70.0
            },
            {
              "name": "llama3.2",
              "description": "Meta's Llama 3.2 goes small with 1B and 3B models.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "1b",
                "3b"
              ],
              "pulls": "18M",
              "pulls_count": 18000000.0,
              "updated": "8 months ago",
              "max_size_gb": 3.0
            },
            {
              "name": "llama3.1",
              "description": "Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "8b",
                "70b",
                "405b"
              ],
              "pulls": "93.9M",
              "pulls_count": 93900000.0,
              "updated": "6 months ago",
              "max_size_gb": 405.0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "7b"
              ],
              "pulls": "14.4M",
              "pulls_count": 14400000.0,
              "updated": "10 months ago",
              "max_size_gb": 7.0
            },
            {
              "name": "qwen2.5",
              "description": "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model ...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "0.5b",
                "1.5b",
                "3b",
                "7b",
                "14b",
                "32b",
                "72b"
              ],
              "pulls": "9.1M",
              "pulls_count": 9100000.0,
              "updated": "8 months ago",
              "max_size_gb": 72.0
            },
            {
              "name": "llama3",
              "description": "Meta Llama 3: The most capable openly available LLM to date",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "0.5b",
                "1.5b",
                "3b",
                "7b",
                "14b",
                "32b"
              ],
              "pulls": "5.3M",
              "pulls_count": 5300000.0,
              "updated": "2 days ago",
              "max_size_gb": 32.0
            },
            {
              "name": "qwen",
              "description": "Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "qwen2",
              "description": "Qwen2 is a new series of large language models from Alibaba group",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "0.5b",
                "1.5b",
                "7b",
                "72b"
              ],
              "pulls": "4.2M",
              "pulls_count": 4200000.0,
              "updated": "8 months ago",
              "max_size_gb": 72.0
            },
            {
              "name": "llama2",
              "description": "Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tinyllama",
              "description": "The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "12b"
              ],
              "pulls": "1.8M",
              "pulls_count": 1800000.0,
              "updated": "9 months ago",
              "max_size_gb": 12.0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "32b"
              ],
              "pulls": "1.5M",
              "pulls_count": 1500000.0,
              "updated": "2 months ago",
              "max_size_gb": 32.0
            },
            {
              "name": "deepseek-v3",
              "description": "A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "olmo2",
              "description": "OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equi...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llama2-uncensored",
              "description": "Uncensored Llama 2 model by George Sung and Jarrad Hope.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "8x7b",
                "8x22b"
              ],
              "pulls": "1M",
              "pulls_count": 1000000.0,
              "updated": "5 months ago",
              "max_size_gb": 22.0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "22b",
                "24b"
              ],
              "pulls": "856.2K",
              "pulls_count": 856200.0,
              "updated": "4 months ago",
              "max_size_gb": 24.0
            },
            {
              "name": "smollm2",
              "description": "SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "135m",
              "capabilities": [],
              "sizes": [
                "1.7b"
              ],
              "pulls": "837.6K",
              "pulls_count": 837600.0,
              "updated": "7 months ago",
              "max_size_gb": 1.7
            },
            {
              "name": "orca-mini",
              "description": "A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "command-r",
              "description": "Command R is a Large Language Model optimized for conversational interaction and long context tasks.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "35b"
              ],
              "pulls": "296.1K",
              "pulls_count": 296100.0,
              "updated": "9 months ago",
              "max_size_gb": 35.0
            },
            {
              "name": "hermes3",
              "description": "Hermes 3 is the latest version of the flagship Hermes series of LLMs by Nous Research",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "3b",
                "8b",
                "70b",
                "405b"
              ],
              "pulls": "285K",
              "pulls_count": 285000.0,
              "updated": "5 months ago",
              "max_size_gb": 405.0
            },
            {
              "name": "yi",
              "description": "Yi 1.5 is a high-performing, bilingual language model.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "smollm",
              "description": "ü™ê A family of small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "135m",
              "description": "360m",
              "capabilities": [],
              "sizes": [
                "1.7b"
              ],
              "pulls": "269.9K",
              "pulls_count": 269900.0,
              "updated": "9 months ago",
              "max_size_gb": 1.7
            },
            {
              "name": "vicuna",
              "description": "General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "openchat",
              "description": "A family of open-source models trained on a wide variety of data, surpassing ChatGPT on various benchmarks. Updated to v...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "3.8b"
              ],
              "pulls": "160.8K",
              "pulls_count": 160800.0,
              "updated": "3 months ago",
              "max_size_gb": 3.8
            },
            {
              "name": "deepseek-v2",
              "description": "A strong, economical, and efficient Mixture-of-Experts language model.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llama2-chinese",
              "description": "Llama 2 based model fine tuned to improve Chinese dialogue ability.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "151K",
              "description": "Pulls",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "1 year ago",
              "max_size_gb": 0
            },
            {
              "name": "deepseek-llm",
              "description": "An advanced language model crafted with 2 trillion bilingual tokens.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "aya",
              "description": "Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "123b"
              ],
              "pulls": "136.7K",
              "pulls_count": 136700.0,
              "updated": "6 months ago",
              "max_size_gb": 123.0
            },
            {
              "name": "glm4",
              "description": "A strong multi-lingual general language model with competitive performance to Llama 3.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "qwen2-math",
              "description": "Qwen2 Math is a series of specialized math language models built upon the Qwen2 LLMs, which significantly outperforms th...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "command-r-plus",
              "description": "Command R+ is a powerful, scalable large language model purpose-built to excel at real-world enterprise use cases.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "104b"
              ],
              "pulls": "126.8K",
              "pulls_count": 126800.0,
              "updated": "9 months ago",
              "max_size_gb": 104.0
            },
            {
              "name": "stablelm2",
              "description": "Stable LM 2 is a state-of-the-art 1.6B and 12B parameter language model trained on multilingual data in English, Spanish...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llama3-chatqa",
              "description": "A model from NVIDIA based on Llama 3 that excels at conversational question answering (QA) and retrieval-augmented gener...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "2b",
                "8b"
              ],
              "pulls": "104K",
              "pulls_count": 104000.0,
              "updated": "3 months ago",
              "max_size_gb": 8.0
            },
            {
              "name": "llama3-gradient",
              "description": "This model extends LLama-3 8B's context length from 8k to over 1m tokens.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "2b",
                "8b"
              ],
              "pulls": "98.7K",
              "pulls_count": 98700.0,
              "updated": "6 months ago",
              "max_size_gb": 8.0
            },
            {
              "name": "granite3.1-dense",
              "description": "The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated signi...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "2b",
                "8b"
              ],
              "pulls": "96.2K",
              "pulls_count": 96200.0,
              "updated": "4 months ago",
              "max_size_gb": 8.0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "3b",
                "8b",
                "14b",
                "32b",
                "70b"
              ],
              "pulls": "93.7K",
              "pulls_count": 93700.0,
              "updated": "1 month ago",
              "max_size_gb": 70.0
            },
            {
              "name": "nous-hermes",
              "description": "General use models based on Llama and Llama 2 from Nous Research.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "xwinlm",
              "description": "Conversational model based on Llama 2 that performs competitively on various benchmarks.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "starling-lm",
              "description": "Starling is a large language model trained by reinforcement learning from AI feedback focused on improving chatbot helpf...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "nemotron-mini",
              "description": "A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "4b"
              ],
              "pulls": "82.4K",
              "pulls_count": 82400.0,
              "updated": "8 months ago",
              "max_size_gb": 4.0
            },
            {
              "name": "solar",
              "description": "A compact, yet powerful 10.7B large language model designed for single-turn conversation.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "72b"
              ],
              "pulls": "82.1K",
              "pulls_count": 82100.0,
              "updated": "6 months ago",
              "max_size_gb": 72.0
            },
            {
              "name": "yarn-llama2",
              "description": "An extension of Llama 2 that supports a context of up to 128k tokens.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "1m",
              "description": "",
              "capabilities": [],
              "sizes": [
                "1.8b",
                "7b",
                "20b"
              ],
              "pulls": "80.3K",
              "pulls_count": 80300.0,
              "updated": "9 months ago",
              "max_size_gb": 20.0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "2b",
                "8b"
              ],
              "pulls": "77.4K",
              "pulls_count": 77400.0,
              "updated": "1 month ago",
              "max_size_gb": 8.0
            },
            {
              "name": "77.1K",
              "description": "Pulls",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "1 year ago",
              "max_size_gb": 0
            },
            {
              "name": "exaone3.5",
              "description": "EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "falcon",
              "description": "A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "nemotron",
              "description": "Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generat...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "70b"
              ],
              "pulls": "73.3K",
              "pulls_count": 73300.0,
              "updated": "7 months ago",
              "max_size_gb": 70.0
            },
            {
              "name": "llama3-groq-tool-use",
              "description": "A series of models from Groq that represent a significant advancement in open-source AI capabilities for tool use/functi...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "8b",
                "70b"
              ],
              "pulls": "67.4K",
              "pulls_count": 67400.0,
              "updated": "10 months ago",
              "max_size_gb": 70.0
            },
            {
              "name": "aya-expanse",
              "description": "Cohere For AI's language models trained to perform well across 23 different languages.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "8b",
                "32b"
              ],
              "pulls": "63.5K",
              "pulls_count": 63500.0,
              "updated": "7 months ago",
              "max_size_gb": 32.0
            },
            {
              "name": "stable-beluga",
              "description": "Llama 2 based model fine tuned on an Orca-style dataset. Originally called Free Willy.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "nous-hermes2-mixtral",
              "description": "The Nous Hermes 2 model from Nous Research, now trained over Mixtral.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "meditron",
              "description": "Open-source medical large language model adapted from Llama 2 to the medical domain.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "medllama2",
              "description": "Fine-tuned Llama 2 model to answer medical questions based on an open source medical dataset.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "granite3-moe",
              "description": "The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "1b",
                "3b"
              ],
              "pulls": "50.1K",
              "pulls_count": 50100.0,
              "updated": "6 months ago",
              "max_size_gb": 3.0
            },
            {
              "name": "47.7K",
              "description": "Pulls",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "1 year ago",
              "max_size_gb": 0
            },
            {
              "name": "granite3.1-moe",
              "description": "The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM designed for low late...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "1b",
                "3b"
              ],
              "pulls": "44.6K",
              "pulls_count": 44600.0,
              "updated": "4 months ago",
              "max_size_gb": 3.0
            },
            {
              "name": "nexusraven",
              "description": "Nexus Raven is a 13B instruction tuned model for function calling tasks.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "everythinglm",
              "description": "Uncensored Llama2 based model with support for a 16K context window.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llama-guard3",
              "description": "Llama Guard 3 is a series of models fine-tuned for content safety classification of LLM inputs and responses.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "reader-lm",
              "description": "A series of models that convert HTML content to Markdown content, which is useful for content conversion tasks.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "stablelm-zephyr",
              "description": "A lightweight chat model allowing accurate, and responsive output without requiring high-end hardware.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "solar-pro",
              "description": "Solar Pro Preview: an advanced large language model (LLM) with 22 billion parameters designed to fit into a single GPU",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "duckdb-nsql",
              "description": "7B parameter text-to-SQL model made by MotherDuck and Numbers Station.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "7b"
              ],
              "pulls": "34.9K",
              "pulls_count": 34900.0,
              "updated": "4 months ago",
              "max_size_gb": 7.0
            },
            {
              "name": "bespoke-minicheck",
              "description": "A state-of-the-art fact-checking model developed by Bespoke Labs.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "notux",
              "description": "A top-performing mixture of experts model, fine-tuned with high-quality data.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "notus",
              "description": "A 7B chat model fine-tuned with high-quality data and based on Zephyr.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "goliath",
              "description": "A language model created by combining two fine-tuned Llama 2 70B models into one.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "23.7K",
              "description": "Pulls",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "1 year ago",
              "max_size_gb": 0
            },
            {
              "name": "firefunction-v2",
              "description": "An open weights function calling model based on Llama 3, competitive with GPT-4o function calling capabilities.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "70b"
              ],
              "pulls": "21.4K",
              "pulls_count": 21400.0,
              "updated": "10 months ago",
              "max_size_gb": 70.0
            },
            {
              "name": "dbrx",
              "description": "DBRX is an open, general-purpose LLM created by Databricks.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "granite3-guardian",
              "description": "The IBM Granite Guardian 3.0 2B and 8B models are designed to detect risks in prompts and/or responses.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "alfred",
              "description": "A robust conversational model designed to be used for both chat and instruct use cases.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "command-a",
              "description": "111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "111b"
              ],
              "pulls": "15.4K",
              "pulls_count": 15400.0,
              "updated": "2 months ago",
              "max_size_gb": 111.0
            },
            {
              "name": "sailor2",
              "description": "Sailor2 are multilingual language models made for South-East Asia. Available in 1B, 8B, and 20B parameter sizes.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "command-r7b-arabic",
              "description": "A new state-of-the-art version of the lightweight Command R7B model that excels in advanced Arabic language capabilities...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "7b"
              ],
              "pulls": "",
              "pulls_count": 0,
              "updated": "3 months ago",
              "max_size_gb": 7.0
            }
          ],
          "rtx3060ti_ready": [
            {
              "name": "qwen3",
              "description": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixt...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llama3.3",
              "description": "New state of the art 70B model. Llama 3.3 70B offers similar performance compared to the Llama 3.1 405B model.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llama3.2",
              "description": "Meta's Llama 3.2 goes small with 1B and 3B models.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "1b",
                "3b"
              ],
              "pulls": "18M",
              "pulls_count": 18000000.0,
              "updated": "8 months ago",
              "max_size_gb": 3.0
            },
            {
              "name": "llama3.1",
              "description": "Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "7b"
              ],
              "pulls": "14.4M",
              "pulls_count": 14400000.0,
              "updated": "10 months ago",
              "max_size_gb": 7.0
            },
            {
              "name": "qwen2.5",
              "description": "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model ...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llama3",
              "description": "Meta Llama 3: The most capable openly available LLM to date",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "qwen",
              "description": "Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "qwen2",
              "description": "Qwen2 is a new series of large language models from Alibaba group",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llama2",
              "description": "Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tinyllama",
              "description": "The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "12b"
              ],
              "pulls": "1.8M",
              "pulls_count": 1800000.0,
              "updated": "9 months ago",
              "max_size_gb": 12.0
            },
            {
              "name": "deepseek-v3",
              "description": "A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "olmo2",
              "description": "OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equi...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llama2-uncensored",
              "description": "Uncensored Llama 2 model by George Sung and Jarrad Hope.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "smollm2",
              "description": "SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "135m",
              "capabilities": [],
              "sizes": [
                "1.7b"
              ],
              "pulls": "837.6K",
              "pulls_count": 837600.0,
              "updated": "7 months ago",
              "max_size_gb": 1.7
            },
            {
              "name": "orca-mini",
              "description": "A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "command-r",
              "description": "Command R is a Large Language Model optimized for conversational interaction and long context tasks.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "hermes3",
              "description": "Hermes 3 is the latest version of the flagship Hermes series of LLMs by Nous Research",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "yi",
              "description": "Yi 1.5 is a high-performing, bilingual language model.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "smollm",
              "description": "ü™ê A family of small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "135m",
              "description": "360m",
              "capabilities": [],
              "sizes": [
                "1.7b"
              ],
              "pulls": "269.9K",
              "pulls_count": 269900.0,
              "updated": "9 months ago",
              "max_size_gb": 1.7
            },
            {
              "name": "vicuna",
              "description": "General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "openchat",
              "description": "A family of open-source models trained on a wide variety of data, surpassing ChatGPT on various benchmarks. Updated to v...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "3.8b"
              ],
              "pulls": "160.8K",
              "pulls_count": 160800.0,
              "updated": "3 months ago",
              "max_size_gb": 3.8
            },
            {
              "name": "deepseek-v2",
              "description": "A strong, economical, and efficient Mixture-of-Experts language model.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llama2-chinese",
              "description": "Llama 2 based model fine tuned to improve Chinese dialogue ability.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "151K",
              "description": "Pulls",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "1 year ago",
              "max_size_gb": 0
            },
            {
              "name": "deepseek-llm",
              "description": "An advanced language model crafted with 2 trillion bilingual tokens.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "aya",
              "description": "Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "glm4",
              "description": "A strong multi-lingual general language model with competitive performance to Llama 3.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "qwen2-math",
              "description": "Qwen2 Math is a series of specialized math language models built upon the Qwen2 LLMs, which significantly outperforms th...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "command-r-plus",
              "description": "Command R+ is a powerful, scalable large language model purpose-built to excel at real-world enterprise use cases.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "stablelm2",
              "description": "Stable LM 2 is a state-of-the-art 1.6B and 12B parameter language model trained on multilingual data in English, Spanish...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llama3-chatqa",
              "description": "A model from NVIDIA based on Llama 3 that excels at conversational question answering (QA) and retrieval-augmented gener...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "2b",
                "8b"
              ],
              "pulls": "104K",
              "pulls_count": 104000.0,
              "updated": "3 months ago",
              "max_size_gb": 8.0
            },
            {
              "name": "llama3-gradient",
              "description": "This model extends LLama-3 8B's context length from 8k to over 1m tokens.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "2b",
                "8b"
              ],
              "pulls": "98.7K",
              "pulls_count": 98700.0,
              "updated": "6 months ago",
              "max_size_gb": 8.0
            },
            {
              "name": "granite3.1-dense",
              "description": "The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated signi...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "2b",
                "8b"
              ],
              "pulls": "96.2K",
              "pulls_count": 96200.0,
              "updated": "4 months ago",
              "max_size_gb": 8.0
            },
            {
              "name": "nous-hermes",
              "description": "General use models based on Llama and Llama 2 from Nous Research.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "xwinlm",
              "description": "Conversational model based on Llama 2 that performs competitively on various benchmarks.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "starling-lm",
              "description": "Starling is a large language model trained by reinforcement learning from AI feedback focused on improving chatbot helpf...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "nemotron-mini",
              "description": "A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "4b"
              ],
              "pulls": "82.4K",
              "pulls_count": 82400.0,
              "updated": "8 months ago",
              "max_size_gb": 4.0
            },
            {
              "name": "solar",
              "description": "A compact, yet powerful 10.7B large language model designed for single-turn conversation.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "yarn-llama2",
              "description": "An extension of Llama 2 that supports a context of up to 128k tokens.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "2b",
                "8b"
              ],
              "pulls": "77.4K",
              "pulls_count": 77400.0,
              "updated": "1 month ago",
              "max_size_gb": 8.0
            },
            {
              "name": "77.1K",
              "description": "Pulls",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "1 year ago",
              "max_size_gb": 0
            },
            {
              "name": "exaone3.5",
              "description": "EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "falcon",
              "description": "A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "nemotron",
              "description": "Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generat...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llama3-groq-tool-use",
              "description": "A series of models from Groq that represent a significant advancement in open-source AI capabilities for tool use/functi...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "aya-expanse",
              "description": "Cohere For AI's language models trained to perform well across 23 different languages.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "stable-beluga",
              "description": "Llama 2 based model fine tuned on an Orca-style dataset. Originally called Free Willy.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "nous-hermes2-mixtral",
              "description": "The Nous Hermes 2 model from Nous Research, now trained over Mixtral.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "meditron",
              "description": "Open-source medical large language model adapted from Llama 2 to the medical domain.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "medllama2",
              "description": "Fine-tuned Llama 2 model to answer medical questions based on an open source medical dataset.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "granite3-moe",
              "description": "The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "1b",
                "3b"
              ],
              "pulls": "50.1K",
              "pulls_count": 50100.0,
              "updated": "6 months ago",
              "max_size_gb": 3.0
            },
            {
              "name": "47.7K",
              "description": "Pulls",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "1 year ago",
              "max_size_gb": 0
            },
            {
              "name": "granite3.1-moe",
              "description": "The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM designed for low late...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "1b",
                "3b"
              ],
              "pulls": "44.6K",
              "pulls_count": 44600.0,
              "updated": "4 months ago",
              "max_size_gb": 3.0
            },
            {
              "name": "nexusraven",
              "description": "Nexus Raven is a 13B instruction tuned model for function calling tasks.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "everythinglm",
              "description": "Uncensored Llama2 based model with support for a 16K context window.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "llama-guard3",
              "description": "Llama Guard 3 is a series of models fine-tuned for content safety classification of LLM inputs and responses.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "reader-lm",
              "description": "A series of models that convert HTML content to Markdown content, which is useful for content conversion tasks.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "stablelm-zephyr",
              "description": "A lightweight chat model allowing accurate, and responsive output without requiring high-end hardware.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "solar-pro",
              "description": "Solar Pro Preview: an advanced large language model (LLM) with 22 billion parameters designed to fit into a single GPU",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "duckdb-nsql",
              "description": "7B parameter text-to-SQL model made by MotherDuck and Numbers Station.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "7b"
              ],
              "pulls": "34.9K",
              "pulls_count": 34900.0,
              "updated": "4 months ago",
              "max_size_gb": 7.0
            },
            {
              "name": "bespoke-minicheck",
              "description": "A state-of-the-art fact-checking model developed by Bespoke Labs.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "notux",
              "description": "A top-performing mixture of experts model, fine-tuned with high-quality data.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "notus",
              "description": "A 7B chat model fine-tuned with high-quality data and based on Zephyr.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "goliath",
              "description": "A language model created by combining two fine-tuned Llama 2 70B models into one.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "23.7K",
              "description": "Pulls",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "1 year ago",
              "max_size_gb": 0
            },
            {
              "name": "firefunction-v2",
              "description": "An open weights function calling model based on Llama 3, competitive with GPT-4o function calling capabilities.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "dbrx",
              "description": "DBRX is an open, general-purpose LLM created by Databricks.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "granite3-guardian",
              "description": "The IBM Granite Guardian 3.0 2B and 8B models are designed to detect risks in prompts and/or responses.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "alfred",
              "description": "A robust conversational model designed to be used for both chat and instruct use cases.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "command-a",
              "description": "111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "sailor2",
              "description": "Sailor2 are multilingual language models made for South-East Asia. Available in 1B, 8B, and 20B parameter sizes.",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "command-r7b-arabic",
              "description": "A new state-of-the-art version of the lightweight Command R7B model that excels in advanced Arabic language capabilities...",
              "capabilities": [],
              "sizes": [],
              "pulls": "",
              "pulls_count": 0,
              "updated": "",
              "max_size_gb": 0
            },
            {
              "name": "tools",
              "description": "",
              "capabilities": [],
              "sizes": [
                "7b"
              ],
              "pulls": "",
              "pulls_count": 0,
              "updated": "3 months ago",
              "max_size_gb": 7.0
            }
          ]
        }
      },
      "hardware_compatibility": {
        "rtx3060ti_native_8gb": [
          {
            "name": "embedding",
            "description": "",
            "capabilities": [],
            "sizes": [],
            "pulls": "27M",
            "pulls_count": 27000000.0,
            "updated": "1 year ago",
            "max_size_gb": 0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "1b",
              "3b"
            ],
            "pulls": "18M",
            "pulls_count": 18000000.0,
            "updated": "8 months ago",
            "max_size_gb": 3.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "7b"
            ],
            "pulls": "14.4M",
            "pulls_count": 14400000.0,
            "updated": "10 months ago",
            "max_size_gb": 7.0
          },
          {
            "name": "embedding",
            "description": "335m",
            "capabilities": [],
            "sizes": [],
            "pulls": "3.4M",
            "pulls_count": 3400000.0,
            "updated": "1 year ago",
            "max_size_gb": 0
          },
          {
            "name": "embedding",
            "description": "567m",
            "capabilities": [],
            "sizes": [],
            "pulls": "1.2M",
            "pulls_count": 1200000.0,
            "updated": "9 months ago",
            "max_size_gb": 0
          },
          {
            "name": "tools",
            "description": "135m",
            "capabilities": [],
            "sizes": [
              "1.7b"
            ],
            "pulls": "837.6K",
            "pulls_count": 837600.0,
            "updated": "7 months ago",
            "max_size_gb": 1.7
          },
          {
            "name": "embedding",
            "description": "22m",
            "capabilities": [],
            "sizes": [],
            "pulls": "738.1K",
            "pulls_count": 738100.0,
            "updated": "1 year ago",
            "max_size_gb": 0
          },
          {
            "name": "embedding",
            "description": "22m",
            "capabilities": [],
            "sizes": [],
            "pulls": "529.1K",
            "pulls_count": 529100.0,
            "updated": "1 year ago",
            "max_size_gb": 0
          },
          {
            "name": "135m",
            "description": "360m",
            "capabilities": [],
            "sizes": [
              "1.7b"
            ],
            "pulls": "269.9K",
            "pulls_count": 269900.0,
            "updated": "9 months ago",
            "max_size_gb": 1.7
          },
          {
            "name": "vision",
            "description": "",
            "capabilities": [],
            "sizes": [
              "1.8b"
            ],
            "pulls": "195.7K",
            "pulls_count": 195700.0,
            "updated": "1 year ago",
            "max_size_gb": 1.8
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "3.8b"
            ],
            "pulls": "160.8K",
            "pulls_count": 160800.0,
            "updated": "3 months ago",
            "max_size_gb": 3.8
          },
          {
            "name": "vision",
            "description": "",
            "capabilities": [],
            "sizes": [
              "7b"
            ],
            "pulls": "118.5K",
            "pulls_count": 118500.0,
            "updated": "1 year ago",
            "max_size_gb": 7.0
          },
          {
            "name": "embedding",
            "description": "335m",
            "capabilities": [],
            "sizes": [],
            "pulls": "104.5K",
            "pulls_count": 104500.0,
            "updated": "9 months ago",
            "max_size_gb": 0
          },
          {
            "name": "vision",
            "description": "",
            "capabilities": [],
            "sizes": [
              "3.8b"
            ],
            "pulls": "92.2K",
            "pulls_count": 92200.0,
            "updated": "1 year ago",
            "max_size_gb": 3.8
          },
          {
            "name": "vision",
            "description": "",
            "capabilities": [
              "tools"
            ],
            "sizes": [
              "2b"
            ],
            "pulls": "85K",
            "pulls_count": 85000.0,
            "updated": "3 months ago",
            "max_size_gb": 2.0
          },
          {
            "name": "embedding",
            "description": "568m",
            "capabilities": [],
            "sizes": [],
            "pulls": "84.7K",
            "pulls_count": 84700.0,
            "updated": "5 months ago",
            "max_size_gb": 0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "4b"
            ],
            "pulls": "82.4K",
            "pulls_count": 82400.0,
            "updated": "8 months ago",
            "max_size_gb": 4.0
          },
          {
            "name": "embedding",
            "description": "278m",
            "capabilities": [],
            "sizes": [],
            "pulls": "61.6K",
            "pulls_count": 61600.0,
            "updated": "9 months ago",
            "max_size_gb": 0
          },
          {
            "name": "embedding",
            "description": "30m",
            "capabilities": [],
            "sizes": [],
            "pulls": "51.2K",
            "pulls_count": 51200.0,
            "updated": "5 months ago",
            "max_size_gb": 0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "1b",
              "3b"
            ],
            "pulls": "50.1K",
            "pulls_count": 50100.0,
            "updated": "6 months ago",
            "max_size_gb": 3.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "1b",
              "3b"
            ],
            "pulls": "44.6K",
            "pulls_count": 44600.0,
            "updated": "4 months ago",
            "max_size_gb": 3.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "7b"
            ],
            "pulls": "34.9K",
            "pulls_count": 34900.0,
            "updated": "4 months ago",
            "max_size_gb": 7.0
          },
          {
            "name": "deepseek-r1",
            "description": "DeepSeek-R1 first-generation of open reasoning models with comparable performance to OpenAI-o3.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "gemma3",
            "description": "The current, most capable model that runs on a single GPU.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "qwen3",
            "description": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixt...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "devstral",
            "description": "Devstral: the best open source model for coding agents",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "llama4",
            "description": "Meta's latest collection of multimodal models.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "qwen2.5vl",
            "description": "Flagship vision-language model of Qwen and also a significant leap from the previous Qwen2-VL.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "llama3.3",
            "description": "New state of the art 70B model. Llama 3.3 70B offers similar performance compared to the Llama 3.1 405B model.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "phi4",
            "description": "Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "llama3.2",
            "description": "Meta's Llama 3.2 goes small with 1B and 3B models.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "llama3.1",
            "description": "Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "nomic-embed-text",
            "description": "A high-performing open embedding model with a large token context window.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "mistral",
            "description": "The 7B model released by Mistral AI, updated to version 0.3.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "qwen2.5",
            "description": "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model ...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "llama3",
            "description": "Meta Llama 3: The most capable openly available LLM to date",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "llava",
            "description": "üåã LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpo...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "qwen2.5-coder",
            "description": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and co...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "gemma2",
            "description": "Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "qwen",
            "description": "Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "gemma",
            "description": "Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "qwen2",
            "description": "Qwen2 is a new series of large language models from Alibaba group",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "llama2",
            "description": "Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "mxbai-embed-large",
            "description": "State-of-the-art large embedding model from mixedbread.ai",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "phi3",
            "description": "Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "llama3.2-vision",
            "description": "Llama 3.2 Vision is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "codellama",
            "description": "A large language model that can use text prompts to generate and discuss code.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "tinyllama",
            "description": "The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "mistral-nemo",
            "description": "A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "minicpm-v",
            "description": "A series of multimodal LLMs (MLLMs) designed for vision-language understanding.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "qwq",
            "description": "QwQ is the reasoning model of the Qwen series.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "deepseek-v3",
            "description": "A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "dolphin3",
            "description": "Dolphin 3.0 Llama 3.1 8B üê¨ is the next generation of the Dolphin series of instruct-tuned models designed to be the ulti...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "olmo2",
            "description": "OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equi...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "bge-m3",
            "description": "BGE-M3 is a new model from BAAI distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Gr...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "llama2-uncensored",
            "description": "Uncensored Llama 2 model by George Sung and Jarrad Hope.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "mixtral",
            "description": "A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "starcoder2",
            "description": "StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes: 3B, 7B and 15B para...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "llava-llama3",
            "description": "A LLaVA model fine-tuned from Llama 3 Instruct with better scores in several benchmarks.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "mistral-small",
            "description": "Mistral Small 3 sets a new benchmark in the ‚Äúsmall‚Äù Large Language Models category below 70B.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "smollm2",
            "description": "SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "deepseek-coder-v2",
            "description": "An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specifi...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "snowflake-arctic-embed",
            "description": "A suite of text embedding models by Snowflake, optimized for performance.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "deepseek-coder",
            "description": "DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "codegemma",
            "description": "CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks like fill-in-the-mi...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "dolphin-mixtral",
            "description": "Uncensored, 8x7b and 8x22b fine-tuned models based on the Mixtral mixture of experts models that excels at coding tasks....",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "phi",
            "description": "Phi-2: a 2.7B language model by Microsoft Research that demonstrates outstanding reasoning and language understanding ca...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "all-minilm",
            "description": "Embedding models on very large sentence level datasets.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "openthinker",
            "description": "A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-R1.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "wizardlm2",
            "description": "State of the art large language model from Microsoft AI with improved performance on complex chat, multilingual, reasoni...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "dolphin-mistral",
            "description": "The uncensored Dolphin model based on Mistral that excels at coding tasks. Updated to version 2.8.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "orca-mini",
            "description": "A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "dolphin-llama3",
            "description": "Dolphin 2.9 is a new model with 8B and 70B sizes by Eric Hartford based on Llama 3 that has a variety of instruction, co...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "command-r",
            "description": "Command R is a Large Language Model optimized for conversational interaction and long context tasks.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "codestral",
            "description": "Codestral is Mistral AI‚Äôs first-ever code model designed for code generation tasks.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "hermes3",
            "description": "Hermes 3 is the latest version of the flagship Hermes series of LLMs by Nous Research",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "phi3.5",
            "description": "A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger sized models.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "yi",
            "description": "Yi 1.5 is a high-performing, bilingual language model.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "smollm",
            "description": "ü™ê A family of small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "zephyr",
            "description": "Zephyr is a series of fine-tuned versions of the Mistral and Mixtral models that are trained to act as helpful assistant...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "granite-code",
            "description": "A family of open foundation models by IBM for Code Intelligence",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "wizard-vicuna-uncensored",
            "description": "Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "starcoder",
            "description": "StarCoder is a code generation model trained on 80+ programming languages.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "moondream",
            "description": "moondream2 is a small vision language model designed to run efficiently on edge devices.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "vicuna",
            "description": "General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "mistral-openorca",
            "description": "Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "openchat",
            "description": "A family of open-source models trained on a wide variety of data, surpassing ChatGPT on various benchmarks. Updated to v...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "phi4-mini",
            "description": "Phi-4-mini brings significant enhancements in multilingual support, reasoning, and mathematics, and now, the long-awaite...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "deepseek-v2",
            "description": "A strong, economical, and efficient Mixture-of-Experts language model.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "llama2-chinese",
            "description": "Llama 2 based model fine tuned to improve Chinese dialogue ability.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "openhermes",
            "description": "OpenHermes 2.5 is a 7B model fine-tuned by Teknium on Mistral with fully open datasets.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "151K",
            "description": "Pulls",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "1 year ago",
            "max_size_gb": 0
          },
          {
            "name": "deepseek-llm",
            "description": "An advanced language model crafted with 2 trillion bilingual tokens.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "codeqwen",
            "description": "CodeQwen1.5 is a large language model pretrained on a large amount of code data.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "codegeex4",
            "description": "A versatile model for AI software development scenarios, including code completion.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "aya",
            "description": "Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "mistral-large",
            "description": "Mistral Large 2 is Mistral's new flagship model that is significantly more capable in code generation, mathematics, and ...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "glm4",
            "description": "A strong multi-lingual general language model with competitive performance to Llama 3.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "stable-code",
            "description": "Stable Code 3B is a coding model with instruct and code completion variants on par with models such as Code Llama 7B tha...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "tinydolphin",
            "description": "An experimental 1.1B parameter model trained on the new Dolphin 2.8 dataset by Eric Hartford and based on TinyLlama.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "deepcoder",
            "description": "DeepCoder is a fully open-Source 14B coder model at O3-mini level, with a 1.5B version also available.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "nous-hermes2",
            "description": "The powerful family of models by Nous Research that excels at scientific discussion and coding tasks.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "qwen2-math",
            "description": "Qwen2 Math is a series of specialized math language models built upon the Qwen2 LLMs, which significantly outperforms th...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "command-r-plus",
            "description": "Command R+ is a powerful, scalable large language model purpose-built to excel at real-world enterprise use cases.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "wizardcoder",
            "description": "State-of-the-art code generation model",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "bakllava",
            "description": "BakLLaVA is a multimodal model consisting of the Mistral 7B base model augmented with the LLaVA architecture.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "mistral-small3.1",
            "description": "Building upon Mistral Small 3, Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long con...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "stablelm2",
            "description": "Stable LM 2 is a state-of-the-art 1.6B and 12B parameter language model trained on multilingual data in English, Spanish...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "neural-chat",
            "description": "A fine-tuned model based on Mistral with good coverage of domain and language.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "sqlcoder",
            "description": "SQLCoder is a code completion model fined-tuned on StarCoder for SQL generation tasks",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "llama3-chatqa",
            "description": "A model from NVIDIA based on Llama 3 that excels at conversational question answering (QA) and retrieval-augmented gener...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "reflection",
            "description": "A high-performing model trained with a new technique called Reflection-tuning that teaches a LLM to detect mistakes in i...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "wizard-math",
            "description": "Model focused on math and logic problems",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "bge-large",
            "description": "Embedding model from BAAI mapping texts to vectors.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "granite3.2",
            "description": "Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for thinking capabilities.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "llama3-gradient",
            "description": "This model extends LLama-3 8B's context length from 8k to over 1m tokens.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "granite3-dense",
            "description": "The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrieval augmented genera...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "granite3.1-dense",
            "description": "The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated signi...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "samantha-mistral",
            "description": "A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistral.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "cogito",
            "description": "Cogito v1 Preview is a family of hybrid reasoning models by Deep Cogito that outperform the best available open models o...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "llava-phi3",
            "description": "A new small LLaVA model fine-tuned from Phi 3 Mini.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "dolphincoder",
            "description": "A 7B and 15B uncensored variant of the Dolphin model family that excels at coding, based on StarCoder2.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "nous-hermes",
            "description": "General use models based on Llama and Llama 2 from Nous Research.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "xwinlm",
            "description": "Conversational model based on Llama 2 that performs competitively on various benchmarks.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "starling-lm",
            "description": "Starling is a large language model trained by reinforcement learning from AI feedback focused on improving chatbot helpf...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "phind-codellama",
            "description": "Code generation model based on Code Llama.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "granite3.2-vision",
            "description": "A compact and efficient vision-language model, specifically designed for visual document understanding, enabling automat...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "snowflake-arctic-embed2",
            "description": "Snowflake's frontier embedding model. Arctic Embed 2.0 adds multilingual support without sacrificing English performance...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "yi-coder",
            "description": "Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding performance with fewer th...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "nemotron-mini",
            "description": "A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "solar",
            "description": "A compact, yet powerful 10.7B large language model designed for single-turn conversation.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "athene-v2",
            "description": "Athene-V2 is a 72B parameter model which excels at code completion, mathematics, and log extraction tasks.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "yarn-llama2",
            "description": "An extension of Llama 2 that supports a context of up to 128k tokens.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "deepscaler",
            "description": "A fine-tuned version of Deepseek-R1-Distilled-Qwen-1.5B that surpasses the performance of OpenAI‚Äôs o1-preview with just ...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "internlm2",
            "description": "InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding reasoning capability.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "granite3.3",
            "description": "IBM Granite 2B and 8B models are 128K context length language models that have been fine-tuned for improved reasoning an...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "wizardlm",
            "description": "General use model based on Llama 2.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "77.1K",
            "description": "Pulls",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "1 year ago",
            "max_size_gb": 0
          },
          {
            "name": "exaone3.5",
            "description": "EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "dolphin-phi",
            "description": "2.7B uncensored Dolphin model by Eric Hartford, based on the Phi language model by Microsoft Research.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "falcon",
            "description": "A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "nemotron",
            "description": "Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generat...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "llama3-groq-tool-use",
            "description": "A series of models from Groq that represent a significant advancement in open-source AI capabilities for tool use/functi...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "orca2",
            "description": "Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models. The model is designed to e...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "wizardlm-uncensored",
            "description": "Uncensored version of Wizard LM model",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "aya-expanse",
            "description": "Cohere For AI's language models trained to perform well across 23 different languages.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "paraphrase-multilingual",
            "description": "Sentence-transformers model that can be used for tasks like clustering or semantic search.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "stable-beluga",
            "description": "Llama 2 based model fine tuned on an Orca-style dataset. Originally called Free Willy.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "phi4-reasoning",
            "description": "Phi 4 reasoning and reasoning plus are 14-billion parameter open-weight reasoning models that rival much larger models o...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "nous-hermes2-mixtral",
            "description": "The Nous Hermes 2 model from Nous Research, now trained over Mixtral.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "smallthinker",
            "description": "A new small reasoning model fine-tuned from the Qwen 2.5 3B Instruct model.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "falcon3",
            "description": "A family of efficient AI models under 10B parameters performant in science, math, and coding through innovative training...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "meditron",
            "description": "Open-source medical large language model adapted from Llama 2 to the medical domain.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "deepseek-v2.5",
            "description": "An upgraded version of DeekSeek-V2 that integrates the general and coding abilities of both DeepSeek-V2-Chat and DeepSee...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "medllama2",
            "description": "Fine-tuned Llama 2 model to answer medical questions based on an open source medical dataset.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "granite-embedding",
            "description": "The IBM Granite Embedding 30M and 278M models models are text-only dense biencoder embedding models, with 30M available ...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "granite3-moe",
            "description": "The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "llama-pro",
            "description": "An expansion of Llama 2 that specializes in integrating both general language understanding and domain-specific knowledg...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "47.7K",
            "description": "Pulls",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "1 year ago",
            "max_size_gb": 0
          },
          {
            "name": "opencoder",
            "description": "OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting chat in English and ...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "yarn-mistral",
            "description": "An extension of Mistral to support context windows of 64K or 128K.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "granite3.1-moe",
            "description": "The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM designed for low late...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "exaone-deep",
            "description": "EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "nexusraven",
            "description": "Nexus Raven is a 13B instruction tuned model for function calling tasks.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "shieldgemma",
            "description": "ShieldGemma is set of instruction tuned models for evaluating the safety of text prompt input and text output responses ...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "codeup",
            "description": "Great code generation model based on Llama2.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "everythinglm",
            "description": "Uncensored Llama2 based model with support for a 16K context window.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "llama-guard3",
            "description": "Llama Guard 3 is a series of models fine-tuned for content safety classification of LLM inputs and responses.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "reader-lm",
            "description": "A series of models that convert HTML content to Markdown content, which is useful for content conversion tasks.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "stablelm-zephyr",
            "description": "A lightweight chat model allowing accurate, and responsive output without requiring high-end hardware.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "mathstral",
            "description": "MathŒ£tral: a 7B model designed for math reasoning and scientific discovery by Mistral AI.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "solar-pro",
            "description": "Solar Pro Preview: an advanced large language model (LLM) with 22 billion parameters designed to fit into a single GPU",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "r1-1776",
            "description": "A version of the DeepSeek-R1 model that has been post trained to provide unbiased, accurate, and factual information by ...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "marco-o1",
            "description": "An open large reasoning model for real-world solutions by the Alibaba International Digital Commerce Group (AIDC-AI).",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "duckdb-nsql",
            "description": "7B parameter text-to-SQL model made by MotherDuck and Numbers Station.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "falcon2",
            "description": "Falcon2 is an 11B parameters causal decoder-only model built by TII and trained over 5T tokens.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "magicoder",
            "description": "üé© Magicoder is a family of 7B parameter models trained on 75K synthetic instruction data using OSS-Instruct, a novel app...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "mistrallite",
            "description": "MistralLite is a fine-tuned model based on Mistral with enhanced capabilities of processing long contexts.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "codebooga",
            "description": "A high-performing code instruct model created by merging two existing code models.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "wizard-vicuna",
            "description": "Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "nuextract",
            "description": "A 3.8B model fine-tuned on a private high-quality synthetic dataset for information extraction, based on Phi-3.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "bespoke-minicheck",
            "description": "A state-of-the-art fact-checking model developed by Bespoke Labs.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "tulu3",
            "description": "T√ºlu 3 is a leading instruction following model family, offering fully open-source data, code, and recipes by the The Al...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "megadolphin",
            "description": "MegaDolphin-2.2-120b is a transformation of Dolphin-2.2-70b created by interleaving the model with itself.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "notux",
            "description": "A top-performing mixture of experts model, fine-tuned with high-quality data.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "open-orca-platypus2",
            "description": "Merge of the Open Orca OpenChat model and the Garage-bAInd Platypus 2 model. Designed for chat and code generation.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "notus",
            "description": "A 7B chat model fine-tuned with high-quality data and based on Zephyr.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "goliath",
            "description": "A language model created by combining two fine-tuned Llama 2 70B models into one.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "23.7K",
            "description": "Pulls",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "1 year ago",
            "max_size_gb": 0
          },
          {
            "name": "firefunction-v2",
            "description": "An open weights function calling model based on Llama 3, competitive with GPT-4o function calling capabilities.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "dbrx",
            "description": "DBRX is an open, general-purpose LLM created by Databricks.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "granite3-guardian",
            "description": "The IBM Granite Guardian 3.0 2B and 8B models are designed to detect risks in prompts and/or responses.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "phi4-mini-reasoning",
            "description": "Phi 4 mini reasoning is a lightweight open model that balances efficiency with advanced reasoning ability.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "alfred",
            "description": "A robust conversational model designed to be used for both chat and instruct use cases.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "command-a",
            "description": "111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "sailor2",
            "description": "Sailor2 are multilingual language models made for South-East Asia. Available in 1B, 8B, and 20B parameter sizes.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "command-r7b-arabic",
            "description": "A new state-of-the-art version of the lightweight Command R7B model that excels in advanced Arabic language capabilities...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "7b"
            ],
            "pulls": "",
            "pulls_count": 0,
            "updated": "3 months ago",
            "max_size_gb": 7.0
          }
        ],
        "rtx3060ti_quantized_8gb": [
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "12b"
            ],
            "pulls": "1.8M",
            "pulls_count": 1800000.0,
            "updated": "9 months ago",
            "max_size_gb": 12.0
          },
          {
            "name": "vision",
            "description": "",
            "capabilities": [],
            "sizes": [
              "8b"
            ],
            "pulls": "1.6M",
            "pulls_count": 1600000.0,
            "updated": "6 months ago",
            "max_size_gb": 8.0
          },
          {
            "name": "vision",
            "description": "",
            "capabilities": [],
            "sizes": [
              "8b"
            ],
            "pulls": "902.8K",
            "pulls_count": 902800.0,
            "updated": "1 year ago",
            "max_size_gb": 8.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "2b",
              "8b"
            ],
            "pulls": "104K",
            "pulls_count": 104000.0,
            "updated": "3 months ago",
            "max_size_gb": 8.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "2b",
              "8b"
            ],
            "pulls": "98.7K",
            "pulls_count": 98700.0,
            "updated": "6 months ago",
            "max_size_gb": 8.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "2b",
              "8b"
            ],
            "pulls": "96.2K",
            "pulls_count": 96200.0,
            "updated": "4 months ago",
            "max_size_gb": 8.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "2b",
              "8b"
            ],
            "pulls": "77.4K",
            "pulls_count": 77400.0,
            "updated": "1 month ago",
            "max_size_gb": 8.0
          }
        ],
        "needs_more_vram": [
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "8b",
              "70b",
              "405b"
            ],
            "pulls": "93.9M",
            "pulls_count": 93900000.0,
            "updated": "6 months ago",
            "max_size_gb": 405.0
          },
          {
            "name": "thinking",
            "description": "",
            "capabilities": [],
            "sizes": [
              "1.5b",
              "7b",
              "8b",
              "14b",
              "32b",
              "70b",
              "671b"
            ],
            "pulls": "46.3M",
            "pulls_count": 46300000.0,
            "updated": "22 hours ago",
            "max_size_gb": 671.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "0.5b",
              "1.5b",
              "3b",
              "7b",
              "14b",
              "32b",
              "72b"
            ],
            "pulls": "9.1M",
            "pulls_count": 9100000.0,
            "updated": "8 months ago",
            "max_size_gb": 72.0
          },
          {
            "name": "vision",
            "description": "",
            "capabilities": [],
            "sizes": [
              "7b",
              "13b",
              "34b"
            ],
            "pulls": "6M",
            "pulls_count": 6000000.0,
            "updated": "1 year ago",
            "max_size_gb": 34.0
          },
          {
            "name": "vision",
            "description": "",
            "capabilities": [],
            "sizes": [
              "1b",
              "4b",
              "12b",
              "27b"
            ],
            "pulls": "5.4M",
            "pulls_count": 5400000.0,
            "updated": "1 month ago",
            "max_size_gb": 27.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "0.5b",
              "1.5b",
              "3b",
              "7b",
              "14b",
              "32b"
            ],
            "pulls": "5.3M",
            "pulls_count": 5300000.0,
            "updated": "2 days ago",
            "max_size_gb": 32.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "0.5b",
              "1.5b",
              "7b",
              "72b"
            ],
            "pulls": "4.2M",
            "pulls_count": 4200000.0,
            "updated": "8 months ago",
            "max_size_gb": 72.0
          },
          {
            "name": "vision",
            "description": "",
            "capabilities": [],
            "sizes": [
              "11b",
              "90b"
            ],
            "pulls": "2.1M",
            "pulls_count": 2100000.0,
            "updated": "1 week ago",
            "max_size_gb": 90.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "70b"
            ],
            "pulls": "1.9M",
            "pulls_count": 1900000.0,
            "updated": "5 months ago",
            "max_size_gb": 70.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [
              "thinking"
            ],
            "sizes": [
              "0.6b",
              "1.7b",
              "4b",
              "8b",
              "14b",
              "30b",
              "32b",
              "235b"
            ],
            "pulls": "1.8M",
            "pulls_count": 1800000.0,
            "updated": "yesterday",
            "max_size_gb": 235.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "32b"
            ],
            "pulls": "1.5M",
            "pulls_count": 1500000.0,
            "updated": "2 months ago",
            "max_size_gb": 32.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "8x7b",
              "8x22b"
            ],
            "pulls": "1M",
            "pulls_count": 1000000.0,
            "updated": "5 months ago",
            "max_size_gb": 22.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "22b",
              "24b"
            ],
            "pulls": "856.2K",
            "pulls_count": 856200.0,
            "updated": "4 months ago",
            "max_size_gb": 24.0
          },
          {
            "name": "vision",
            "description": "",
            "capabilities": [
              "tools"
            ],
            "sizes": [
              "16x17b",
              "128x17b"
            ],
            "pulls": "367K",
            "pulls_count": 367000.0,
            "updated": "2 days ago",
            "max_size_gb": 17.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "35b"
            ],
            "pulls": "296.1K",
            "pulls_count": 296100.0,
            "updated": "9 months ago",
            "max_size_gb": 35.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "3b",
              "8b",
              "70b",
              "405b"
            ],
            "pulls": "285K",
            "pulls_count": 285000.0,
            "updated": "5 months ago",
            "max_size_gb": 405.0
          },
          {
            "name": "vision",
            "description": "",
            "capabilities": [],
            "sizes": [
              "3b",
              "7b",
              "32b",
              "72b"
            ],
            "pulls": "228.2K",
            "pulls_count": 228200.0,
            "updated": "1 week ago",
            "max_size_gb": 72.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "123b"
            ],
            "pulls": "136.7K",
            "pulls_count": 136700.0,
            "updated": "6 months ago",
            "max_size_gb": 123.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "104b"
            ],
            "pulls": "126.8K",
            "pulls_count": 126800.0,
            "updated": "9 months ago",
            "max_size_gb": 104.0
          },
          {
            "name": "vision",
            "description": "",
            "capabilities": [
              "tools"
            ],
            "sizes": [
              "24b"
            ],
            "pulls": "116.3K",
            "pulls_count": 116300.0,
            "updated": "1 month ago",
            "max_size_gb": 24.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "3b",
              "8b",
              "14b",
              "32b",
              "70b"
            ],
            "pulls": "93.7K",
            "pulls_count": 93700.0,
            "updated": "1 month ago",
            "max_size_gb": 70.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "24b"
            ],
            "pulls": "86.6K",
            "pulls_count": 86600.0,
            "updated": "1 week ago",
            "max_size_gb": 24.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "72b"
            ],
            "pulls": "82.1K",
            "pulls_count": 82100.0,
            "updated": "6 months ago",
            "max_size_gb": 72.0
          },
          {
            "name": "1m",
            "description": "",
            "capabilities": [],
            "sizes": [
              "1.8b",
              "7b",
              "20b"
            ],
            "pulls": "80.3K",
            "pulls_count": 80300.0,
            "updated": "9 months ago",
            "max_size_gb": 20.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "70b"
            ],
            "pulls": "73.3K",
            "pulls_count": 73300.0,
            "updated": "7 months ago",
            "max_size_gb": 70.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "8b",
              "70b"
            ],
            "pulls": "67.4K",
            "pulls_count": 67400.0,
            "updated": "10 months ago",
            "max_size_gb": 70.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "8b",
              "32b"
            ],
            "pulls": "63.5K",
            "pulls_count": 63500.0,
            "updated": "7 months ago",
            "max_size_gb": 32.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "70b"
            ],
            "pulls": "21.4K",
            "pulls_count": 21400.0,
            "updated": "10 months ago",
            "max_size_gb": 70.0
          },
          {
            "name": "tools",
            "description": "",
            "capabilities": [],
            "sizes": [
              "111b"
            ],
            "pulls": "15.4K",
            "pulls_count": 15400.0,
            "updated": "2 months ago",
            "max_size_gb": 111.0
          }
        ]
      },
      "top_recommendations": {
        "primary_code_analysis": [
          {
            "name": "devstral",
            "description": "Devstral: the best open source model for coding agents",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "llava",
            "description": "üåã LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpo...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "qwen2.5-coder",
            "description": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and co...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "codellama",
            "description": "A large language model that can use text prompts to generate and discuss code.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "dolphin3",
            "description": "Dolphin 3.0 Llama 3.1 8B üê¨ is the next generation of the Dolphin series of instruct-tuned models designed to be the ulti...",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          }
        ],
        "architectural_reasoning": [
          {
            "name": "deepseek-r1",
            "description": "DeepSeek-R1 first-generation of open reasoning models with comparable performance to OpenAI-o3.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "gemma3",
            "description": "The current, most capable model that runs on a single GPU.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "phi4",
            "description": "Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "mistral",
            "description": "The 7B model released by Mistral AI, updated to version 0.3.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          },
          {
            "name": "gemma2",
            "description": "Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.",
            "capabilities": [],
            "sizes": [],
            "pulls": "",
            "pulls_count": 0,
            "updated": "",
            "max_size_gb": 0
          }
        ],
        "documentation_vision": [
          {
            "name": "vision",
            "description": "",
            "capabilities": [],
            "sizes": [
              "8b"
            ],
            "pulls": "1.6M",
            "pulls_count": 1600000.0,
            "updated": "6 months ago",
            "max_size_gb": 8.0
          },
          {
            "name": "vision",
            "description": "",
            "capabilities": [],
            "sizes": [
              "8b"
            ],
            "pulls": "902.8K",
            "pulls_count": 902800.0,
            "updated": "1 year ago",
            "max_size_gb": 8.0
          },
          {
            "name": "vision",
            "description": "",
            "capabilities": [],
            "sizes": [
              "1.8b"
            ],
            "pulls": "195.7K",
            "pulls_count": 195700.0,
            "updated": "1 year ago",
            "max_size_gb": 1.8
          },
          {
            "name": "vision",
            "description": "",
            "capabilities": [],
            "sizes": [
              "7b"
            ],
            "pulls": "118.5K",
            "pulls_count": 118500.0,
            "updated": "1 year ago",
            "max_size_gb": 7.0
          },
          {
            "name": "vision",
            "description": "",
            "capabilities": [],
            "sizes": [
              "3.8b"
            ],
            "pulls": "92.2K",
            "pulls_count": 92200.0,
            "updated": "1 year ago",
            "max_size_gb": 3.8
          }
        ],
        "semantic_search": [
          {
            "name": "embedding",
            "description": "",
            "capabilities": [],
            "sizes": [],
            "pulls": "27M",
            "pulls_count": 27000000.0,
            "updated": "1 year ago",
            "max_size_gb": 0
          },
          {
            "name": "embedding",
            "description": "335m",
            "capabilities": [],
            "sizes": [],
            "pulls": "3.4M",
            "pulls_count": 3400000.0,
            "updated": "1 year ago",
            "max_size_gb": 0
          },
          {
            "name": "embedding",
            "description": "567m",
            "capabilities": [],
            "sizes": [],
            "pulls": "1.2M",
            "pulls_count": 1200000.0,
            "updated": "9 months ago",
            "max_size_gb": 0
          },
          {
            "name": "embedding",
            "description": "22m",
            "capabilities": [],
            "sizes": [],
            "pulls": "738.1K",
            "pulls_count": 738100.0,
            "updated": "1 year ago",
            "max_size_gb": 0
          },
          {
            "name": "embedding",
            "description": "22m",
            "capabilities": [],
            "sizes": [],
            "pulls": "529.1K",
            "pulls_count": 529100.0,
            "updated": "1 year ago",
            "max_size_gb": 0
          }
        ]
      }
    },
    "quality_metrics": {
      "rtx3060ti_native": 198,
      "rtx3060ti_quantized": 7,
      "total_rtx_compatible": 205,
      "code_specialists": 40,
      "reasoning_experts": 53,
      "vision_capable": 17,
      "embedding_specialists": 17
    }
  },
  "project_context": {
    "name": "LLMStruct",
    "scale": "272 modules, 1857 functions, 183 classes",
    "hardware": "RTX 3060 Ti (8GB VRAM)",
    "budget_remaining": "$21.685",
    "goal": "Architectural analysis with optimal Ollama model recommendations"
  },
  "grok_consultation_request": {
    "primary_question": "Which specific Ollama models are optimal for LLMStruct architectural analysis on RTX 3060 Ti 8GB?",
    "strategic_options": [
      "Original GPT-4.1 plan ($22, 1M context) - single high-end model approach",
      "Continued Grok series ($21.685, proven effective) - expert consultation approach",
      "Hybrid: Strategic Grok + Local Ollama pipeline - best of both worlds",
      "Self-analysis enhanced: LLMStruct AI + Ollama specialists - leverage existing capabilities"
    ],
    "specific_requirements": [
      "Analyze 272 Python modules automatically",
      "Generate architectural documentation with diagrams",
      "Consolidate 8 different bot versions into coherent system",
      "Create implementation roadmap with concrete next steps",
      "Leverage existing struct.json knowledge base (1.2MB)",
      "Use existing AI self-awareness and context orchestration systems"
    ],
    "constraints_and_assets": [
      "Hardware: RTX 3060 Ti 8GB VRAM limitation",
      "Budget: $21.685 remaining from original $22",
      "Time: Need production-ready outputs, not experiments",
      "Assets: LLMStruct already has SystemCapabilityDiscovery, Context Orchestrator, Metrics tracking",
      "Goal: Maximum architectural insight with optimal resource utilization"
    ]
  }
}