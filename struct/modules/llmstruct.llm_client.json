{
  "module_info": {
    "uid": "llmstruct.llm_client",
    "file_path": "",
    "tags": [
      "module",
      "public"
    ],
    "summary": "Module llmstruct.llm_client with 6 functions and 1 classes",
    "hash": "sha256:de8abc9d34ea0852",
    "dependencies": [
      "aiohttp",
      "asyncio",
      "json",
      "logging",
      "os",
      "response",
      "result",
      "self",
      "session"
    ],
    "exports": [
      "LLMClient",
      "query"
    ]
  },
  "functions": [
    {
      "name": "__init__",
      "docstring": "Initialize LLMClient with optional Ollama host.",
      "line_range": [
        28,
        36
      ],
      "parameters": [
        "self",
        "ollama_host"
      ],
      "decorators": [],
      "uid": "llmstruct.llm_client.__init__:28#function",
      "uid_components": [
        "llmstruct",
        "llmstruct.llm_client",
        "llmstruct.llm_client.__init__:28"
      ],
      "hash": "02606981cd67408d4c8eb5a29d4b4b97d2cc30191e44ba23a8d2160619dfe84e",
      "hash_source": "code_ast_v1",
      "hash_version": "2.1.0",
      "markdown_anchor": "#llmstruct-llm-client---init--",
      "summary": "Initialize LLMClient with optional Ollama host.",
      "summary_source": "docstring",
      "tags": [
        "function",
        "private"
      ],
      "calls": [
        "logging.info",
        "int",
        "os.getenv"
      ],
      "called_by": []
    },
    {
      "name": "query",
      "docstring": "Query LLMs with prompt, context, and optional model.",
      "line_range": [
        38,
        86
      ],
      "parameters": [
        "self",
        "prompt",
        "context_path",
        "mode",
        "model",
        "artifact_ids"
      ],
      "decorators": [],
      "uid": "llmstruct.llm_client.query:38#function",
      "uid_components": [
        "llmstruct",
        "llmstruct.llm_client",
        "llmstruct.llm_client.query:38"
      ],
      "hash": "c2ea7863606eb20f1b06243c89423cd38947f76dbdd8fd5fb90cc551d926a18d",
      "hash_source": "code_ast_v1",
      "hash_version": "2.1.0",
      "markdown_anchor": "#llmstruct-llm-client-query",
      "summary": "Query LLMs with prompt, context, and optional model.",
      "summary_source": "docstring",
      "tags": [
        "async",
        "function",
        "public"
      ],
      "calls": [
        "logging.info",
        "self._query_anthropic",
        "asyncio.sleep",
        "self._query_grok",
        "json.dumps",
        "logging.error",
        "self._query_hybrid",
        "logging.warning",
        "range",
        "json.load",
        "self._query_ollama",
        "Path"
      ],
      "called_by": []
    },
    {
      "name": "_query_grok",
      "docstring": "Query Grok API.",
      "line_range": [
        88,
        115
      ],
      "parameters": [
        "self",
        "prompt"
      ],
      "decorators": [],
      "uid": "llmstruct.llm_client._query_grok:88#function",
      "uid_components": [
        "llmstruct",
        "llmstruct.llm_client",
        "llmstruct.llm_client._query_grok:88"
      ],
      "hash": "76b7223a6dd2dc24d112f20eb1514dacaadf3ba4cc1f83c0278bdbc5a1059b06",
      "hash_source": "code_ast_v1",
      "hash_version": "2.1.0",
      "markdown_anchor": "#llmstruct-llm-client--query-grok",
      "summary": "Query Grok API.",
      "summary_source": "docstring",
      "tags": [
        "async",
        "function",
        "private",
        "public"
      ],
      "calls": [
        "logging.info",
        "result.get",
        "aiohttp.ClientSession",
        "response.json",
        "session.post",
        "logging.error"
      ],
      "called_by": []
    },
    {
      "name": "_query_anthropic",
      "docstring": "Query Anthropic API.",
      "line_range": [
        117,
        141
      ],
      "parameters": [
        "self",
        "prompt"
      ],
      "decorators": [],
      "uid": "llmstruct.llm_client._query_anthropic:117#function",
      "uid_components": [
        "llmstruct",
        "llmstruct.llm_client",
        "llmstruct.llm_client._query_anthropic:117"
      ],
      "hash": "16eadde7d70922918e1bfb932e1fd7bf0bd0c67030e0abf4161db309a0b14210",
      "hash_source": "code_ast_v1",
      "hash_version": "2.1.0",
      "markdown_anchor": "#llmstruct-llm-client--query-anthropic",
      "summary": "Query Anthropic API.",
      "summary_source": "docstring",
      "tags": [
        "async",
        "function",
        "private",
        "public"
      ],
      "calls": [
        "logging.info",
        "result.get",
        "aiohttp.ClientSession",
        "response.json",
        "session.post",
        "logging.error"
      ],
      "called_by": []
    },
    {
      "name": "_query_ollama",
      "docstring": "Query Ollama API with specified model.",
      "line_range": [
        143,
        156
      ],
      "parameters": [
        "self",
        "prompt",
        "model"
      ],
      "decorators": [],
      "uid": "llmstruct.llm_client._query_ollama:143#function",
      "uid_components": [
        "llmstruct",
        "llmstruct.llm_client",
        "llmstruct.llm_client._query_ollama:143"
      ],
      "hash": "45698ea6501118d0775201986fe8db251b8a71fa3965e2a57f2a0d0e93ee99d9",
      "hash_source": "code_ast_v1",
      "hash_version": "2.1.0",
      "markdown_anchor": "#llmstruct-llm-client--query-ollama",
      "summary": "Query Ollama API with specified model.",
      "summary_source": "docstring",
      "tags": [
        "async",
        "function",
        "private",
        "public"
      ],
      "calls": [
        "logging.debug",
        "logging.info",
        "result.get",
        "aiohttp.ClientSession",
        "response.json",
        "session.post",
        "logging.error"
      ],
      "called_by": []
    },
    {
      "name": "_query_hybrid",
      "docstring": "Query multiple LLMs and combine results.",
      "line_range": [
        158,
        172
      ],
      "parameters": [
        "self",
        "prompt",
        "model"
      ],
      "decorators": [],
      "uid": "llmstruct.llm_client._query_hybrid:158#function",
      "uid_components": [
        "llmstruct",
        "llmstruct.llm_client",
        "llmstruct.llm_client._query_hybrid:158"
      ],
      "hash": "517244a6131f6cf295033daa2cfd4751164b35fac4cc42672841bf1ff84b2f35",
      "hash_source": "code_ast_v1",
      "hash_version": "2.1.0",
      "markdown_anchor": "#llmstruct-llm-client--query-hybrid",
      "summary": "Query multiple LLMs and combine results.",
      "summary_source": "docstring",
      "tags": [
        "async",
        "function",
        "private"
      ],
      "calls": [
        "logging.info",
        "self._query_anthropic",
        "isinstance",
        "self._query_grok",
        "asyncio.gather",
        "len",
        "self._query_ollama"
      ],
      "called_by": []
    }
  ],
  "classes": [
    {
      "name": "LLMClient",
      "docstring": "",
      "line_range": [
        27,
        172
      ],
      "methods": [
        {
          "name": "__init__",
          "docstring": "Initialize LLMClient with optional Ollama host.",
          "line_range": [
            28,
            36
          ],
          "parameters": [
            "self",
            "ollama_host"
          ],
          "uid": "llmstruct.llm_client.LLMClient.__init__:28#method",
          "uid_components": [
            "llmstruct",
            "llmstruct.llm_client",
            "llmstruct.llm_client.LLMClient",
            "llmstruct.llm_client.LLMClient.__init__:28"
          ],
          "hash": "61ba865d366bc4852d8bf176ea18c09b00c4fe4b31a3001b66042353abf1d152",
          "hash_source": "code_ast_v1",
          "hash_version": "2.1.0",
          "markdown_anchor": "#llmstruct-llm-client-llmclient---init--",
          "summary": "Initialize LLMClient with optional Ollama host.",
          "summary_source": "docstring",
          "tags": [
            "method",
            "private"
          ],
          "calls": [
            "logging.info",
            "int",
            "os.getenv"
          ],
          "called_by": []
        },
        {
          "name": "query",
          "docstring": "Query LLMs with prompt, context, and optional model.",
          "line_range": [
            38,
            86
          ],
          "parameters": [
            "self",
            "prompt",
            "context_path",
            "mode",
            "model",
            "artifact_ids"
          ],
          "uid": "llmstruct.llm_client.LLMClient.query:38#method",
          "uid_components": [
            "llmstruct",
            "llmstruct.llm_client",
            "llmstruct.llm_client.LLMClient",
            "llmstruct.llm_client.LLMClient.query:38"
          ],
          "hash": "2a21cb56dea8d6db198d62354c1b5bfe4a097857f08eaa790293b1589f001470",
          "hash_source": "code_ast_v1",
          "hash_version": "2.1.0",
          "markdown_anchor": "#llmstruct-llm-client-llmclient-query",
          "summary": "Query LLMs with prompt, context, and optional model.",
          "summary_source": "docstring",
          "tags": [
            "async",
            "method",
            "public"
          ],
          "calls": [
            "logging.info",
            "self._query_anthropic",
            "asyncio.sleep",
            "self._query_grok",
            "json.dumps",
            "logging.error",
            "self._query_hybrid",
            "logging.warning",
            "range",
            "json.load",
            "self._query_ollama",
            "Path"
          ],
          "called_by": []
        },
        {
          "name": "_query_grok",
          "docstring": "Query Grok API.",
          "line_range": [
            88,
            115
          ],
          "parameters": [
            "self",
            "prompt"
          ],
          "uid": "llmstruct.llm_client.LLMClient._query_grok:88#method",
          "uid_components": [
            "llmstruct",
            "llmstruct.llm_client",
            "llmstruct.llm_client.LLMClient",
            "llmstruct.llm_client.LLMClient._query_grok:88"
          ],
          "hash": "3681309fcbdee164bee5831258983106a1d87257a9988ada4897a3664a0a7e94",
          "hash_source": "code_ast_v1",
          "hash_version": "2.1.0",
          "markdown_anchor": "#llmstruct-llm-client-llmclient--query-grok",
          "summary": "Query Grok API.",
          "summary_source": "docstring",
          "tags": [
            "async",
            "method",
            "private",
            "public"
          ],
          "calls": [
            "logging.info",
            "result.get",
            "aiohttp.ClientSession",
            "response.json",
            "session.post",
            "logging.error"
          ],
          "called_by": []
        },
        {
          "name": "_query_anthropic",
          "docstring": "Query Anthropic API.",
          "line_range": [
            117,
            141
          ],
          "parameters": [
            "self",
            "prompt"
          ],
          "uid": "llmstruct.llm_client.LLMClient._query_anthropic:117#method",
          "uid_components": [
            "llmstruct",
            "llmstruct.llm_client",
            "llmstruct.llm_client.LLMClient",
            "llmstruct.llm_client.LLMClient._query_anthropic:117"
          ],
          "hash": "d645f5c291a36483bd0b7f49b28e01415e20b31c739f2f5cb04b3420490d87ba",
          "hash_source": "code_ast_v1",
          "hash_version": "2.1.0",
          "markdown_anchor": "#llmstruct-llm-client-llmclient--query-anthropic",
          "summary": "Query Anthropic API.",
          "summary_source": "docstring",
          "tags": [
            "async",
            "method",
            "private",
            "public"
          ],
          "calls": [
            "logging.info",
            "result.get",
            "aiohttp.ClientSession",
            "response.json",
            "session.post",
            "logging.error"
          ],
          "called_by": []
        },
        {
          "name": "_query_ollama",
          "docstring": "Query Ollama API with specified model.",
          "line_range": [
            143,
            156
          ],
          "parameters": [
            "self",
            "prompt",
            "model"
          ],
          "uid": "llmstruct.llm_client.LLMClient._query_ollama:143#method",
          "uid_components": [
            "llmstruct",
            "llmstruct.llm_client",
            "llmstruct.llm_client.LLMClient",
            "llmstruct.llm_client.LLMClient._query_ollama:143"
          ],
          "hash": "96753a786a454d4fc233c85aed62478769733d6b0f70fdd4fb52f8bbc637674e",
          "hash_source": "code_ast_v1",
          "hash_version": "2.1.0",
          "markdown_anchor": "#llmstruct-llm-client-llmclient--query-ollama",
          "summary": "Query Ollama API with specified model.",
          "summary_source": "docstring",
          "tags": [
            "async",
            "method",
            "private",
            "public"
          ],
          "calls": [
            "logging.debug",
            "logging.info",
            "result.get",
            "aiohttp.ClientSession",
            "response.json",
            "session.post",
            "logging.error"
          ],
          "called_by": []
        },
        {
          "name": "_query_hybrid",
          "docstring": "Query multiple LLMs and combine results.",
          "line_range": [
            158,
            172
          ],
          "parameters": [
            "self",
            "prompt",
            "model"
          ],
          "uid": "llmstruct.llm_client.LLMClient._query_hybrid:158#method",
          "uid_components": [
            "llmstruct",
            "llmstruct.llm_client",
            "llmstruct.llm_client.LLMClient",
            "llmstruct.llm_client.LLMClient._query_hybrid:158"
          ],
          "hash": "f52cded3aca8d21b448f82b677ee384d9ed9fc5984d8c48a8f02945edf95d0a4",
          "hash_source": "code_ast_v1",
          "hash_version": "2.1.0",
          "markdown_anchor": "#llmstruct-llm-client-llmclient--query-hybrid",
          "summary": "Query multiple LLMs and combine results.",
          "summary_source": "docstring",
          "tags": [
            "async",
            "method",
            "private"
          ],
          "calls": [
            "logging.info",
            "self._query_anthropic",
            "isinstance",
            "self._query_grok",
            "asyncio.gather",
            "len",
            "self._query_ollama"
          ],
          "called_by": []
        }
      ],
      "bases": [],
      "uid": "llmstruct.llm_client.LLMClient:27#class",
      "uid_components": [
        "llmstruct",
        "llmstruct.llm_client",
        "llmstruct.llm_client.LLMClient:27"
      ],
      "hash": "b5b433f11c8052da86480e5539979391ccca08daae757a5c3797f8c50394e1a6",
      "hash_source": "code_ast_v1",
      "hash_version": "2.1.0",
      "markdown_anchor": "#llmstruct-llm-client-llmclient",
      "summary": "Client for llm",
      "summary_source": "heuristic",
      "tags": [
        "async",
        "class",
        "public"
      ]
    }
  ],
  "imports": [],
  "calls": [
    "logging.info",
    "int",
    "os.getenv",
    "logging.info",
    "self._query_anthropic",
    "asyncio.sleep",
    "self._query_grok",
    "json.dumps",
    "logging.error",
    "self._query_hybrid",
    "logging.warning",
    "range",
    "json.load",
    "self._query_ollama",
    "Path",
    "logging.info",
    "result.get",
    "aiohttp.ClientSession",
    "response.json",
    "session.post",
    "logging.error",
    "logging.info",
    "result.get",
    "aiohttp.ClientSession",
    "response.json",
    "session.post",
    "logging.error",
    "logging.debug",
    "logging.info",
    "result.get",
    "aiohttp.ClientSession",
    "response.json",
    "session.post",
    "logging.error",
    "logging.info",
    "self._query_anthropic",
    "isinstance",
    "self._query_grok",
    "asyncio.gather",
    "len",
    "self._query_ollama"
  ],
  "metadata": {
    "generated_at": "2025-06-26T11:45:18.765844",
    "generator_version": "2.1.0",
    "source_hash": "sha256:de8abc9d34ea0852",
    "lines_of_code": 7
  }
}